[Script Info]
; Script generated by Aegisub r8942
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: TV.601
PlayResX: 1280
PlayResY: 720

[Aegisub Project Garbage]
Last Style Storage: Default
Audio File: ../../../../Desktop/csapp/Lecture 23  Concurrent Programming.mp4
Video File: ../../../../Desktop/csapp/Lecture 23  Concurrent Programming.mp4
Video AR Mode: 4
Video AR Value: 1.777778
Video Zoom Percent: 1.000000
Scroll Position: 743
Active Line: 751
Video Position: 128324

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: English,Source Han Sans CN,30,&H00FFFFFF,&H00412A2C,&H00412A2C,&H00412A2C,0,0,0,0,100,100,0,0,1,2.2,1,2,10,10,10,1
Style: Chinese,Source Han Sans CN,34,&H00FBFD00,&H00FFFFFF,&H00362A28,&H00FFFFFF,0,0,0,0,100,100,0,0,1,2,0.2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.31,0:00:02.94,English,,0,0,0,,Well good afternoon everybody welcome
Dialogue: 0,0:00:04.44,0:00:05.34,English,,0,0,0,,Good to see you
Dialogue: 0,0:00:08.98,0:00:13.74,English,,0,0,0,,So this week we're going to study how to incorporate concurrency into programs
Dialogue: 0,0:00:13.80,0:00:22.46,English,,0,0,0,,Now we've seen concurrency before in the form of processes exception handlers
Dialogue: 0,0:00:23.24,0:00:25.50,English,,0,0,0,,And in the case of processes it was a mac
Dialogue: 0,0:00:25.56,0:00:32.06,English,,0,0,0,,We use processes as a mechanism to run multiple independent application programs
Dialogue: 0,0:00:34.46,0:00:40.12,English,,0,0,0,,Now but you could concurrency also exists in application programs
Dialogue: 0,0:00:40.64,0:00:44.40,English,,0,0,0,,Now we've seen a little bit of this when we studied signal handlers
Dialogue: 0,0:00:44.84,0:00:52.02,English,,0,0,0,,Okay so a signal handler is a concurrent flow that runs concurrently with your main application program
Dialogue: 0,0:00:53.06,0:00:54.16,English,,0,0,0,,Okay and we've seen
Dialogue: 0,0:00:54.58,0:00:58.64,English,,0,0,0,,We've seen how some of the difficulties that can arise
Dialogue: 0,0:01:00.60,0:01:03.14,English,,0,0,0,,When we introduce concurrency in our programs
Dialogue: 0,0:01:08.04,0:01:11.32,English,,0,0,0,,So even with something like a signal handler which isn't doing very much
Dialogue: 0,0:01:12.88,0:01:19.88,English,,0,0,0,,It's very hard for us to reason about this kind of thing we have to two concurrent flows running at the same time
Dialogue: 0,0:01:20.50,0:01:26.34,English,,0,0,0,,Like there's our brains just tend to be kind of sequential we want to think about things happening one after the other
Dialogue: 0,0:01:26.94,0:01:29.84,English,,0,0,0,,You know it is and it's much easier for us to reason about that
Dialogue: 0,0:01:30.62,0:01:36.12,English,,0,0,0,,Reasoning about multiple things happening at the same time really causes problems
Dialogue: 0,0:01:36.16,0:01:46.44,English,,0,0,0,,And the fundamental reason is that to really reason about say (two independent) two concurrent flows
Dialogue: 0,0:01:47.24,0:01:50.50,English,,0,0,0,,We have to account for all of the possible inter leavings of those flows
Dialogue: 0,0:01:50.98,0:01:56.44,English,,0,0,0,,Okay and that's where and that's that grows exponentially with the number of flows
Dialogue: 0,0:01:57.62,0:02:01.04,English,,0,0,0,,Okay so you had you saw this with your signal handlers when you did shell lab
Dialogue: 0,0:02:01.78,0:02:08.36,English,,0,0,0,,You had two concurrent flows the main program and your signal handler
Dialogue: 0,0:02:08.86,0:02:13.32,English,,0,0,0,,Both accessing a shared resource in the form of the jobs list
Dialogue: 0,0:02:14.02,0:02:18.86,English,,0,0,0,,Right and you had to be very careful to prevent an interleaving where
Dialogue: 0,0:02:19.46,0:02:24.48,English,,0,0,0,,Where that data structure was being referenced in an inconsistent state
Dialogue: 0,0:02:26.46,0:02:33.44,English,,0,0,0,,So what we're going to do this week,and into next week is we're going to look at that kind of application level concurrency
Dialogue: 0,0:02:33.44,0:02:39.58,English,,0,0,0,,But in a more principled in a more principled way than we encountered with signal handlers
Dialogue: 0,0:02:45.70,0:02:54.62,English,,0,0,0,,So as soon as you have multiple flows accessing shared resources all kinds of bad things can happen in your program
Dialogue: 0,0:02:54.64,0:03:04.68,English,,0,0,0,,And these have been...these bad things,these problems that occur been objects of study in computer science for decades
Dialogue: 0,0:03:05.42,0:03:08.32,English,,0,0,0,,But the kinds of things that can happen are our races
Dialogue: 0,0:03:08.92,0:03:11.60,English,,0,0,0,,Which we've seen in when we did the shell lab
Dialogue: 0,0:03:12.12,0:03:19.46,English,,0,0,0,,Where the outcome whether good or bad outcome depends on some some arbitrary scheduling decision
Dialogue: 0,0:03:19.70,0:03:22.58,English,,0,0,0,,Right in the case of you know one of the races we saw
Dialogue: 0,0:03:23.16,0:03:28.70,English,,0,0,0,,In the case of a shell was the case where that a child just because of a scheduling decision by the kernel
Dialogue: 0,0:03:29.20,0:03:34.64,English,,0,0,0,,Runs and finishes before the parent has a chance to add that child to the job list
Dialogue: 0,0:03:34.68,0:03:38.44,English,,0,0,0,,Okay so that's a classic example of a race
Dialogue: 0,0:03:39.26,0:03:48.30,English,,0,0,0,,And similarly if you have two people that are trying to making accessing a reservation system on a bird for an airline
Dialogue: 0,0:03:48.64,0:03:54.00,English,,0,0,0,,Who gets the if they both access at the same time,who actually gets a seat
Dialogue: 0,0:03:54.44,0:04:00.00,English,,0,0,0,,Just depends on those various scheduling decisions that are going on in the reservation system
Dialogue: 0,0:04:01.78,0:04:05.80,English,,0,0,0,,Another kind of a problem that occurs is deadlock
Dialogue: 0,0:04:06.10,0:04:07.54,English,,0,0,0,,So a deadlock is a condition
Dialogue: 0,0:04:08.62,0:04:13.94,English,,0,0,0,,That exists where you have multiple flows waiting for an event that will never occur
Dialogue: 0,0:04:15.52,0:04:21.86,English,,0,0,0,,Okay so using printf in a signal handler is an example of this kind of problem
Dialogue: 0,0:04:22.04,0:04:24.68,English,,0,0,0,,Introduces the potential for that kind of problem
Dialogue: 0,0:04:25.74,0:04:30.54,English,,0,0,0,,Okay so in your main routine you execute a printf
Dialogue: 0,0:04:32.50,0:04:40.08,English,,0,0,0,,And that printf acquires a lock on some system resource,i think it's a terminal lock
Dialogue: 0,0:04:42.02,0:04:44.40,English,,0,0,0,,And then after that main printf
Dialogue: 0,0:04:45.14,0:04:48.16,English,,0,0,0,,Acquires that lock it gets interrupted by a signal handler
Dialogue: 0,0:04:49.76,0:04:52.00,English,,0,0,0,,And now the signal handler if it does a printf
Dialogue: 0,0:04:53.12,0:04:56.46,English,,0,0,0,,That printf will try to acquire that lock
Dialogue: 0,0:04:56.96,0:05:00.54,English,,0,0,0,,But it won't be able to get it because the printf and the main routine has it
Dialogue: 0,0:05:01.80,0:05:06.94,English,,0,0,0,,So now your signal handler the printf and the signal handler
Dialogue: 0,0:05:06.94,0:05:08.76,English,,0,0,0,,It's waiting for that lock to be released is waiting for an event that will never occur
Dialogue: 0,0:05:09.18,0:05:15.74,English,,0,0,0,,And it will never occur because the printf in the main routine can't release the lock
Dialogue: 0,0:05:16.98,0:05:18.66,English,,0,0,0,,Until the signal handler returns
Dialogue: 0,0:05:19.28,0:05:24.80,English,,0,0,0,,And the signal handler can't acquire the lock until the printf in the main routine terminates
Dialogue: 0,0:05:24.86,0:05:27.54,English,,0,0,0,,So that's a classic example of deadlock
Dialogue: 0,0:05:28.06,0:05:37.08,English,,0,0,0,,Another more from real life imagine that you're all drivers follow the rules very precisely
Dialogue: 0,0:05:37.56,0:05:45.56,English,,0,0,0,,And the rule for a four-way stop is that whoever gets there first gets to go first okay
Dialogue: 0,0:05:46.62,0:05:51.36,English,,0,0,0,,So if four cars arrive at the intersection exactly the same time
Dialogue: 0,0:05:51.98,0:05:55.82,English,,0,0,0,,Then you have a deadlock,you have none of the drivers was first
Dialogue: 0,0:05:56.18,0:05:58.04,English,,0,0,0,,So none of the drivers goes
Dialogue: 0,0:05:58.32,0:06:02.50,English,,0,0,0,,And so they're all waiting for a condition that will never occur
Dialogue: 0,0:06:03.48,0:06:09.16,English,,0,0,0,,And then other classical things that can go wrong or things like liveness  starvation fairness of this
Dialogue: 0,0:06:11.22,0:06:13.96,English,,0,0,0,,Starvation occurs when you fail
Dialogue: 0,0:06:14.24,0:06:19.44,English,,0,0,0,,You're trying to do something but you fail to make progress because somebody else keeps getting all the work
Dialogue: 0,0:06:20.22,0:06:27.24,English,,0,0,0,,Right so if you were...if you had two processes and the kernel always scheduled process a instead of process b
Dialogue: 0,0:06:28.22,0:06:34.90,English,,0,0,0,,Process b that would be an example of process b being starved out because of a improper scheduling decision
Dialogue: 0,0:06:35.30,0:06:40.18,English,,0,0,0,,And then we would say that that that scheduling policy always schedule b is unfair
Dialogue: 0,0:06:40.28,0:06:51.84,English,,0,0,0,,Right so it doesn't have this property of fairness where every entity in the system gets sort of a reasonable chunk of the processor
Dialogue: 0,0:06:54.38,0:06:55.70,English,,0,0,0,,So like i said i mean this
Dialogue: 0,0:06:56.20,0:07:01.16,English,,0,0,0,,Concurrency has been studied for years it's a very difficult topic
Dialogue: 0,0:07:01.22,0:07:06.16,English,,0,0,0,,Because of this because of this sort of exponential explosion in the number of inter leavings
Dialogue: 0,0:07:07.48,0:07:10.64,English,,0,0,0,,So we can't,we're not going to cover all of them
Dialogue: 0,0:07:10.86,0:07:13.78,English,,0,0,0,,But we will cover some so as you get a reasonable idea
Dialogue: 0,0:07:16.14,0:07:18.46,English,,0,0,0,,Of how to incorporate concurrency in your programs
Dialogue: 0,0:07:19.54,0:07:23.38,English,,0,0,0,,Now for our study of application level concurrency
Dialogue: 0,0:07:23.68,0:07:26.38,English,,0,0,0,,We're going to use servers as a motivating example
Dialogue: 0,0:07:26.92,0:07:32.71,English,,0,0,0,,And the reason is that you cannot write a correct server without using concurrency
Dialogue: 0,0:07:34.60,0:07:38.06,English,,0,0,0,,Okay so it's a good,it's a really good,good motivation
Dialogue: 0,0:07:39.98,0:07:41.02,English,,0,0,0,,And here's the reason
Dialogue: 0,0:07:41.82,0:07:44.68,English,,0,0,0,,So far we've looked at servers that are iterative
Dialogue: 0,0:07:45.22,0:07:49.26,English,,0,0,0,,Okay so the only process requests from one client at a time
Dialogue: 0,0:07:50.04,0:07:55.62,English,,0,0,0,,And once they finish processing a request from a client then they go on to the next client
Dialogue: 0,0:07:56.94,0:08:01.46,English,,0,0,0,,So they so like with our iterative echo server
Dialogue: 0,0:08:02.30,0:08:07.78,English,,0,0,0,,You can see the each of these clients makes a connection request
Dialogue: 0,0:08:09.74,0:08:13.68,English,,0,0,0,,Then it writes a line of text to the server
Dialogue: 0,0:08:14.08,0:08:16.62,English,,0,0,0,,And then it waits for the server to echo that back
Dialogue: 0,0:08:17.12,0:08:20.72,English,,0,0,0,,And in this case this simple case then it just closes
Dialogue: 0,0:08:21.92,0:08:25.34,English,,0,0,0,,Okay and the server waits for a connection requests and accept
Dialogue: 0,0:08:27.84,0:08:33.30,English,,0,0,0,,And then waits for...and then once it accepts that connection requests
Dialogue: 0,0:08:33.98,0:08:40.68,English,,0,0,0,,It reads and when waits for what waits for that client to write something to the connection
Dialogue: 0,0:08:41.36,0:08:42.74,English,,0,0,0,,And then it echoes it back
Dialogue: 0,0:08:44.78,0:08:48.86,English,,0,0,0,,And then it waits for the next line
Dialogue: 0,0:08:49.32,0:08:54.14,English,,0,0,0,,Until the client closes that connection
Dialogue: 0,0:08:54.58,0:08:56.44,English,,0,0,0,,And then the server closes this connection
Dialogue: 0,0:08:56.44,0:09:01.12,English,,0,0,0,,And then only them does it does it do another except to wait for the next connection request
Dialogue: 0,0:09:02.60,0:09:07.82,English,,0,0,0,,Okay so in this example client two is also making a connection request
Dialogue: 0,0:09:09.44,0:09:18.34,English,,0,0,0,,But it never runs it has to wait until the server actually echoes back the response
Dialogue: 0,0:09:19.56,0:09:27.02,English,,0,0,0,,Now there's a little subtlety here that  in where exactly that this client waits
Dialogue: 0,0:09:28.96,0:09:30.54,English,,0,0,0,,So the semantics of connect
Dialogue: 0,0:09:31.08,0:09:34.94,English,,0,0,0,,You would think that connect would block until the connection was established
Dialogue: 0,0:09:35.66,0:09:37.38,English,,0,0,0,,But actually if you tried this out
Dialogue: 0,0:09:37.92,0:09:44.90,English,,0,0,0,,It turns out that connect actually initiates the connection process inside the kernel
Dialogue: 0,0:09:44.90,0:09:45.86,English,,0,0,0,,But then it returns
Dialogue: 0,0:09:46.28,0:09:50.16,English,,0,0,0,,Okay before the connection has been established
Dialogue: 0,0:09:51.98,0:09:53.06,English,,0,0,0,,And then it does a right
Dialogue: 0,0:09:53.76,0:09:57.88,English,,0,0,0,,And that right also returns immediately
Dialogue: 0,0:09:57.96,0:10:06.44,English,,0,0,0,,So right it doesn't wait until the server reads that the string that was written
Dialogue: 0,0:10:07.32,0:10:16.44,English,,0,0,0,,And it doesn't block until it calls the read function waiting for the echoed response from the server
Dialogue: 0,0:10:18.05,0:10:22.96,English,,0,0,0,,So it actually it doesn't block until it hits this read and then it spends all
Dialogue: 0,0:10:23.20,0:10:24.66,English,,0,0,0,,It waits waits waits waits
Dialogue: 0,0:10:24.66,0:10:33.74,English,,0,0,0,,And finally the server accepts the connection requests and then writes echoes the string back to the client
Dialogue: 0,0:10:41.24,0:10:45.36,English,,0,0,0,,So the call to connect actually returns immediately
Dialogue: 0,0:10:47.06,0:10:50.92,English,,0,0,0,,And it exploits this feature in the kernel that can queue up these connection requests
Dialogue: 0,0:10:51.46,0:10:56.66,English,,0,0,0,,Okay so that the kernel now is going through all the process of setting up the connection
Dialogue: 0,0:10:56.98,0:10:58.96,English,,0,0,0,,But the application program continues
Dialogue: 0,0:10:59.86,0:11:04.60,English,,0,0,0,,And then the right,the right inside the client doesn't block
Dialogue: 0,0:11:04.66,0:11:09.36,English,,0,0,0,,Because the kernel can also queue up the data that's written
Dialogue: 0,0:11:09.50,0:11:13.80,English,,0,0,0,,So it'll little queue it up remember that it was written when the connection actually gets created
Dialogue: 0,0:11:14.18,0:11:16.12,English,,0,0,0,,Then it'll send that data along
Dialogue: 0,0:11:17.24,0:11:17.70,English,,0,0,0,,And
Dialogue: 0,0:11:18.44,0:11:25.06,English,,0,0,0,,But there's no way to avoid the read from blocking,write a read can't return until it gets some data
Dialogue: 0,0:11:25.76,0:11:26.90,English,,0,0,0,,Okay so read has to block
Dialogue: 0,0:11:29.78,0:11:32.48,English,,0,0,0,,Okay now here's the fundamental flaw of an iterative server
Dialogue: 0,0:11:32.58,0:11:38.58,English,,0,0,0,,And the reason why we have to write them with the concurrency
Dialogue: 0,0:11:39.48,0:11:41.82,English,,0,0,0,,Okay so let's say in a echo server example
Dialogue: 0,0:11:42.68,0:11:49.80,English,,0,0,0,,We have a client that creates a connection
Dialogue: 0,0:11:52.82,0:11:55.48,English,,0,0,0,,Or request a connection it's accepted in the server
Dialogue: 0,0:11:56.36,0:11:57.14,English,,0,0,0,,Does the write
Dialogue: 0,0:11:58.62,0:12:02.96,English,,0,0,0,,The server echoes back one you know one string
Dialogue: 0,0:12:04.54,0:12:11.10,English,,0,0,0,,And then the client blocks again or instead of doing the next write or closing the connection
Dialogue: 0,0:12:11.98,0:12:13.14,English,,0,0,0,,The person
Dialogue: 0,0:12:17.04,0:12:21.76,English,,0,0,0,,The user goes out to lunch and never types in a string to the the echo client
Dialogue: 0,0:12:22.86,0:12:26.54,English,,0,0,0,,Okay so at this point the server calls read
Dialogue: 0,0:12:26.96,0:12:32.22,English,,0,0,0,,And then it blocks waiting for this user to type in something
Dialogue: 0,0:12:32.92,0:12:35.62,English,,0,0,0,,And so that the client can send it to the server to be echoed
Dialogue: 0,0:12:36.50,0:12:39.26,English,,0,0,0,,But the user is gone gets hit by a truck
Dialogue: 0,0:12:39.68,0:12:40.20,English,,0,0,0,,Who knows
Dialogue: 0,0:12:42.16,0:12:45.34,English,,0,0,0,,Anyway so this never this read then blocks
Dialogue: 0,0:12:46.24,0:12:48.16,English,,0,0,0,,For an indeterminate amount of time right
Dialogue: 0,0:12:49.88,0:12:54.16,English,,0,0,0,,And while it's blocking client to which also wants service
Dialogue: 0,0:12:56.78,0:12:57.60,English,,0,0,0,,It has to block
Dialogue: 0,0:12:59.32,0:13:02.12,English,,0,0,0,,Okay so now you're in an untenable situation
Dialogue: 0,0:13:02.68,0:13:05.60,English,,0,0,0,,Where one client has sort of totally affected
Dialogue: 0,0:13:06.02,0:13:09.54,English,,0,0,0,,All of the other clients in the system and none of the other clients can get service
Dialogue: 0,0:13:10.06,0:13:11.74,English,,0,0,0,,It's so if this were a web server
Dialogue: 0,0:13:12.50,0:13:15.50,English,,0,0,0,,If one client for some reason
Dialogue: 0,0:13:16.34,0:13:17.34,English,,0,0,0,,Blocked
Dialogue: 0,0:13:17.92,0:13:24.04,English,,0,0,0,,No other users would be able to use that web service or look at pages on that's site
Dialogue: 0,0:13:24.78,0:13:26.94,English,,0,0,0,,So obviously this we can't have this
Dialogue: 0,0:13:29.20,0:13:35.54,English,,0,0,0,,Okay so the solution is to use is to write a concurrent server instead of an iterative server
Dialogue: 0,0:13:36.42,0:13:45.20,English,,0,0,0,,Where we'll have a separate concurrent flow handle each clients request and interact with each client
Dialogue: 0,0:13:46.90,0:13:52.34,English,,0,0,0,,So now if one client for some reason is slow or misbehaves or blocks the system
Dialogue: 0,0:13:52.90,0:13:57.18,English,,0,0,0,,Other clients won't be affected because those clients will be handled way by concurrent flows
Dialogue: 0,0:14:00.94,0:14:04.86,English,,0,0,0,,So there are several ways a number of ways to create these.yes
Dialogue: 0,0:14:04.94,0:14:19.68,English,,0,0,0,,[student speaking]
Dialogue: 0,0:14:19.68,0:14:22.86,English,,0,0,0,,It actually could and in fact that's that's a form of concurrency
Dialogue: 0,0:14:24.24,0:14:31.38,English,,0,0,0,,So the question is the could the server queue up requests from the from clients
Dialogue: 0,0:14:32.06,0:14:33.22,English,,0,0,0,,It could but it would...
Dialogue: 0,0:14:34.54,0:14:39.30,English,,0,0,0,,You know i guess actually it would have to queue up it would somehow have to accept those connections right
Dialogue: 0,0:14:41.38,0:14:41.92,English,,0,0,0,,So that
Dialogue: 0,0:14:43.28,0:14:45.06,English,,0,0,0,,Now so that wouldn't work so somehow you have
Dialogue: 0,0:14:49.56,0:14:53.74,English,,0,0,0,,Since the accept calls are iterative sequential
Dialogue: 0,0:14:54.40,0:14:57.04,English,,0,0,0,,There's no way to get data from those other clients
Dialogue: 0,0:14:59.28,0:15:03.72,English,,0,0,0,,Okay but actually what you're suggesting is very similar to something called an event-based server
Dialogue: 0,0:15:04.90,0:15:09.36,English,,0,0,0,,That will which is one of the ways we can create concurrent flows
Dialogue: 0,0:15:10.68,0:15:16.34,English,,0,0,0,,So there's three ways to create these concurrent flows
Dialogue: 0,0:15:18.42,0:15:21.18,English,,0,0,0,,One is to use processes okay like we've already seen
Dialogue: 0,0:15:22.70,0:15:29.00,English,,0,0,0,,Okay so the kernel...so in this case the kernel handles all the scheduling for us and interleaves
Dialogue: 0,0:15:30.20,0:15:34.88,English,,0,0,0,,It interleaves the process execution automatically for us
Dialogue: 0,0:15:35.46,0:15:39.44,English,,0,0,0,,And then as we saw before each flow has its own private address space
Dialogue: 0,0:15:39.50,0:15:43.76,English,,0,0,0,,So that each flow is independent and scheduled by the kernel
Dialogue: 0,0:15:45.80,0:15:49.74,English,,0,0,0,,Okay now there's a another approach called event based
Dialogue: 0,0:15:50.56,0:15:53.86,English,,0,0,0,,Where the programmer manually interleaves the flows
Dialogue: 0,0:15:54.62,0:15:58.70,English,,0,0,0,,Okay so instead of relying on the kernel to interleave these different flows
Dialogue: 0,0:15:58.70,0:16:06.94,English,,0,0,0,,The user the programmer creates this flows and then manually interleaves them okay
Dialogue: 0,0:16:09.62,0:16:11.06,English,,0,0,0,,And since it's one program
Dialogue: 0,0:16:11.46,0:16:14.14,English,,0,0,0,,All of the flows share the same address space right
Dialogue: 0,0:16:14.20,0:16:16.74,English,,0,0,0,,So they have access to all the same global data structures
Dialogue: 0,0:16:19.46,0:16:25.52,English,,0,0,0,,And they do they do this inter interleaving using a technique called i/o multiplexing
Dialogue: 0,0:16:25.81,0:16:27.84,English,,0,0,0,,You know i'll talk briefly about that but
Dialogue: 0,0:16:28.44,0:16:32.56,English,,0,0,0,,It's addressed much more detail in your book
Dialogue: 0,0:16:34.40,0:16:40.86,English,,0,0,0,,The third approach which is kind of a hybrid of process based and event based is thread base
Dialogue: 0,0:16:40.96,0:16:47.84,English,,0,0,0,,So used each of these flows is implemented using something called a thread
Dialogue: 0,0:16:51.32,0:16:54.90,English,,0,0,0,,The kernel like processes
Dialogue: 0,0:16:55.26,0:17:01.76,English,,0,0,0,,The kernel automatically interleaves these different threads
Dialogue: 0,0:17:03.10,0:17:07.08,English,,0,0,0,,But unlike a process each thread shares the same address space
Dialogue: 0,0:17:08.24,0:17:13.74,English,,0,0,0,,Okay so each thread has access to all the global variables declared in the program
Dialogue: 0,0:17:15.06,0:17:20.44,English,,0,0,0,,So in so it's like process based and that the kernel automatically schedules it for us
Dialogue: 0,0:17:20.92,0:17:22.82,English,,0,0,0,,But it's like event based in the sense that
Dialogue: 0,0:17:23.46,0:17:26.08,English,,0,0,0,,Every flow shares the same address space
Dialogue: 0,0:17:27.68,0:17:31.58,English,,0,0,0,,Okay so we'll look,let's look at all three of these approaches in more detail
Dialogue: 0,0:17:33.62,0:17:37.34,English,,0,0,0,,So the first approach is to create these flows using processes
Dialogue: 0,0:17:39.22,0:17:40.10,English,,0,0,0,,So in this case
Dialogue: 0,0:17:41.50,0:17:43.62,English,,0,0,0,,This is our echo server example
Dialogue: 0,0:17:47.06,0:17:49.90,English,,0,0,0,,The the client requests a connection
Dialogue: 0,0:17:51.88,0:17:58.04,English,,0,0,0,,And then calls fgets to wait for the user to type something at the keyboard
Dialogue: 0,0:17:58.94,0:18:05.32,English,,0,0,0,,But the user is gone and so fgets this client just blocks in the call to fgets
Dialogue: 0,0:18:07.24,0:18:15.16,English,,0,0,0,,So the server when it gets a request,it accepts the connection requests and returns from the accept call
Dialogue: 0,0:18:18.36,0:18:20.90,English,,0,0,0,,And after it returns from the accept call
Dialogue: 0,0:18:21.54,0:18:22.76,English,,0,0,0,,It forks a child
Dialogue: 0,0:18:23.98,0:18:25.30,English,,0,0,0,,And then that child interacts
Dialogue: 0,0:18:25.86,0:18:31.08,English,,0,0,0,,That child process now will be responsible for interacting with client number one
Dialogue: 0,0:18:31.76,0:18:34.70,English,,0,0,0,,So the child blocks waiting for data from client one
Dialogue: 0,0:18:35.76,0:18:40.10,English,,0,0,0,,Which is never going to show up because the the user left
Dialogue: 0,0:18:42.20,0:18:45.26,English,,0,0,0,,Okay but it that's okay because it doesn't stop the server
Dialogue: 0,0:18:46.44,0:18:51.32,English,,0,0,0,,The server after it forks the child goes right back and calls accept
Dialogue: 0,0:18:52.80,0:18:57.18,English,,0,0,0,,And now accept can accept the connection request from client 2
Dialogue: 0,0:18:58.70,0:19:04.70,English,,0,0,0,,And fork off another a different child that can interact with client 2
Dialogue: 0,0:19:05.26,0:19:11.24,English,,0,0,0,,So that child will read waits for data to show up from the client  and then it echoes it back
Dialogue: 0,0:19:12.02,0:19:15.72,English,,0,0,0,,And at some point then closes the this connection
Dialogue: 0,0:19:16.86,0:19:19.68,English,,0,0,0,,Okay so you see that this misbehaving client number one
Dialogue: 0,0:19:19.68,0:19:25.04,English,,0,0,0,,Now because we have concurrent flows interacting with all the clients
Dialogue: 0,0:19:25.88,0:19:29.24,English,,0,0,0,,This misbehaving client can't adversely affect other clients
Dialogue: 0,0:19:29.84,0:19:30.58,English,,0,0,0,,Okay so now that's
Dialogue: 0,0:19:31.64,0:19:39.18,English,,0,0,0,,And this idea of creating concurrent flows to interact with clients is fundamental
Dialogue: 0,0:19:39.18,0:19:45.46,English,,0,0,0,,There you have to do this in order to have a sort of a working server implementation
Dialogue: 0,0:19:47.02,0:19:51.80,English,,0,0,0,,Alright so how would we actually program this process based concurrent server
Dialogue: 0,0:19:53.78,0:19:56.62,English,,0,0,0,,It's actually surprisingly compact
Dialogue: 0,0:19:59.10,0:19:59.54,English,,0,0,0,,So
Dialogue: 0,0:20:00.54,0:20:07.24,English,,0,0,0,,We're going to pass in argv,we're going to pass in a port number that we want this server to listen on
Dialogue: 0,0:20:09.06,0:20:12.16,English,,0,0,0,,We've got a listening descriptor and a connected descriptor
Dialogue: 0,0:20:13.50,0:20:20.30,English,,0,0,0,,We've got and then we've got a length and a an address address field
Dialogue: 0,0:20:21.44,0:20:28.36,English,,0,0,0,,And the address is declared in a protocol independent way using this sockaddr_storage type
Dialogue: 0,0:20:28.76,0:20:31.54,English,,0,0,0,,Which is guaranteed to be big enough as you saw last time
Dialogue: 0,0:20:31.64,0:20:37.08,English,,0,0,0,,It's guaranteed to be big enough to handle any type of address either ipv4 or ipv6
Dialogue: 0,0:20:39.90,0:20:41.86,English,,0,0,0,,Okay so we install a sigchld_handler
Dialogue: 0,0:20:43.62,0:20:50.02,English,,0,0,0,,And then we use the open_listenfd call from your textbook
Dialogue: 0,0:20:50.50,0:20:57.88,English,,0,0,0,,To create a listening descriptor on port that we pass in as the as the argument to this program
Dialogue: 0,0:21:00.46,0:21:03.32,English,,0,0,0,,And then the server goes into an into a loop
Dialogue: 0,0:21:04.60,0:21:05.90,English,,0,0,0,,And in each iteration
Dialogue: 0,0:21:10.16,0:21:16.70,English,,0,0,0,,It gets the size of the socketaddr_storage type and puts it into clientlen
Dialogue: 0,0:21:18.96,0:21:20.98,English,,0,0,0,,And then it calls it except
Dialogue: 0,0:21:23.38,0:21:28.20,English,,0,0,0,,With pointers to the clients address and clientlen
Dialogue: 0,0:21:32.36,0:21:36.78,English,,0,0,0,,Using the listening descriptor that was returned by open listenfd
Dialogue: 0,0:21:37.52,0:21:44.68,English,,0,0,0,,The accept call after it gets a connection request it returns with the address of the client that made the that
Dialogue: 0,0:21:45.20,0:21:55.22,English,,0,0,0,,At the other end of the connection along with the true length of that  address
Dialogue: 0,0:21:55.36,0:21:59.22,English,,0,0,0,,So the case of ipv4 before 4 bytes
Dialogue: 0,0:22:01.06,0:22:04.20,English,,0,0,0,,And then the acceptor turns this this connected file descriptor
Dialogue: 0,0:22:07.24,0:22:11.56,English,,0,0,0,,That the then that that the the child
Dialogue: 0,0:22:12.26,0:22:16.20,English,,0,0,0,,That that it can use to to read and write and interact with that client
Dialogue: 0,0:22:17.46,0:22:19.56,English,,0,0,0,,So it creates say it forks the child
Dialogue: 0,0:22:20.80,0:22:23.96,English,,0,0,0,,And then the child closes it's listening descriptor
Dialogue: 0,0:22:25.24,0:22:30.94,English,,0,0,0,,And then it calls the echo routine to interact with the client
Dialogue: 0,0:22:31.86,0:22:39.18,English,,0,0,0,,And when the echo routine returns the client closes this connected descriptor and then exits
Dialogue: 0,0:22:39.42,0:22:45.84,English,,0,0,0,,And so this close isn't absolutely necessary but we just did it to be careful
Dialogue: 0,0:22:47.28,0:22:48.38,English,,0,0,0,,Okay now the parent
Dialogue: 0,0:22:49.06,0:22:52.90,English,,0,0,0,,And this is important closes the connected descriptor
Dialogue: 0,0:22:52.90,0:22:57.78,English,,0,0,0,,Because it's not going to use that connected scripture only the child's will use that connected descriptor
Dialogue: 0,0:22:58.62,0:23:03.92,English,,0,0,0,,So in order to avoid this memory leak it's very important for the child to to close this descriptor
Dialogue: 0,0:23:04.92,0:23:08.76,English,,0,0,0,,Okay because remember this the server's running in an infinite loop on it
Dialogue: 0,0:23:09.58,0:23:12.92,English,,0,0,0,,In theory it would never terminate
Dialogue: 0,0:23:14.46,0:23:23.26,English,,0,0,0,,Okay and then to avoid a memory leak
Dialogue: 0,0:23:23.90,0:23:32.36,English,,0,0,0,,We have to...in a handler we have to have a sigchld_handler that will reap all of the children that have terminated
Dialogue: 0,0:23:34.46,0:23:39.50,English,,0,0,0,,Okay so let's look a little more detail how this except works
Dialogue: 0,0:23:41.36,0:23:44.54,English,,0,0,0,,So you have a client with a client file descriptor
Dialogue: 0,0:23:45.16,0:23:47.60,English,,0,0,0,,And then you have a server that creates a listening descriptor
Dialogue: 0,0:23:47.64,0:23:52.02,English,,0,0,0,,So let's say that's you know descriptors are indexed by small integers
Dialogue: 0,0:23:52.06,0:23:58.34,English,,0,0,0,,So let's say that index is three the description number is three
Dialogue: 0,0:23:59.76,0:24:02.92,English,,0,0,0,,So the server blocks and accept waiting for this connection request
Dialogue: 0,0:24:04.66,0:24:07.58,English,,0,0,0,,The client makes a connection request using the connect call
Dialogue: 0,0:24:10.48,0:24:15.70,English,,0,0,0,,Okay the server accepts the connect call and then it creates a child
Dialogue: 0,0:24:18.96,0:24:25.00,English,,0,0,0,,And then the child interacts with the client using the connected file descriptor that was returned from the accept
Dialogue: 0,0:24:25.46,0:24:30.12,English,,0,0,0,,So that would be say descriptive number four just be some different number
Dialogue: 0,0:24:33.72,0:24:37.42,English,,0,0,0,,Okay so are the execution model we have for these process based servers
Dialogue: 0,0:24:38.34,0:24:45.50,English,,0,0,0,,Is that we have this the server processor listening for connection requests one after the other from clients
Dialogue: 0,0:24:46.82,0:24:56.86,English,,0,0,0,,And then we have multiple clients interacting concurrently with multiple children interacting concurrently with multiple clients
Dialogue: 0,0:24:58.80,0:25:06.36,English,,0,0,0,,Okay since each of these children are processes there's no shared state between them
Dialogue: 0,0:25:07.18,0:25:14.86,English,,0,0,0,,And both parent and child inherit the have,they inherit the descriptor table
Dialogue: 0,0:25:14.88,0:25:22.78,English,,0,0,0,,So they have they both have copies of listenfd and listening descriptor in the connected descriptor
Dialogue: 0,0:25:24.40,0:25:27.26,English,,0,0,0,,Okay and as we saw before the parent must close
Dialogue: 0,0:25:27.90,0:25:33.76,English,,0,0,0,,It's copy of the connected file descriptor,the child should close the listening descriptor but
Dialogue: 0,0:25:34.94,0:25:39.24,English,,0,0,0,,It's you know just to be just because it's not needed
Dialogue: 0,0:25:41.22,0:25:46.86,English,,0,0,0,,All right when you,so these are actually pretty simple to create and there's just a couple of things you have to keep in mind
Dialogue: 0,0:25:47.42,0:25:49.94,English,,0,0,0,,When you build a process based server
Dialogue: 0,0:25:51.70,0:25:56.30,English,,0,0,0,,So firstly as we...as with any process that creates children
Dialogue: 0,0:25:57.86,0:26:03.98,English,,0,0,0,,It has to reap these children that have terminated to avoid this memory leak
Dialogue: 0,0:26:04.70,0:26:09.40,English,,0,0,0,,The parent process has to close its copy of the connected file descriptor
Dialogue: 0,0:26:10.98,0:26:12.12,English,,0,0,0,,And there's a couple reasons
Dialogue: 0,0:26:13.88,0:26:17.30,English,,0,0,0,,It fact if it doesn't it will not only create a memory leak but that
Dialogue: 0,0:26:19.32,0:26:25.14,English,,0,0,0,,The state associated with that descriptor will actually stay around forever
Dialogue: 0,0:26:25.14,0:26:28.74,English,,0,0,0,,Because the kernel won't close that connection
Dialogue: 0,0:26:29.28,0:26:33.04,English,,0,0,0,,So it as we saw when we looked at file i/o
Dialogue: 0,0:26:33.60,0:26:37.42,English,,0,0,0,,This is just enough,this is the same kind of file i/o we looked at before
Dialogue: 0,0:26:39.00,0:26:43.28,English,,0,0,0,,So the kernel keeps a reference count for each socket that's open
Dialogue: 0,0:26:44.18,0:26:49.12,English,,0,0,0,,So after the fork now there's two,there's a parent and the child which are accessing
Dialogue: 0,0:26:49.72,0:26:53.48,English,,0,0,0,,The file table associated with the connected file descriptor
Dialogue: 0,0:26:55.60,0:26:56.08,English,,0,0,0,,Okay so that
Dialogue: 0,0:26:56.46,0:27:01.78,English,,0,0,0,,And the connection won't be closed until the reference count for that connected file description is zero right
Dialogue: 0,0:27:02.84,0:27:06.20,English,,0,0,0,,That file table entry won't be removed from the kernel
Dialogue: 0,0:27:06.60,0:27:11.58,English,,0,0,0,,Until until there's only there until there's zero references to it
Dialogue: 0,0:27:13.74,0:27:19.34,English,,0,0,0,,Okay so both the parent and the child have to close that descriptor
Dialogue: 0,0:27:21.44,0:27:26.48,English,,0,0,0,,Okay now the good thing about process based servers is that they do the job for us that we asked them to do
Dialogue: 0,0:27:26.48,0:27:32.76,English,,0,0,0,,Right we wanted them to handle to interact with multiple clients concurrently or have that ability
Dialogue: 0,0:27:33.48,0:27:35.32,English,,0,0,0,,There's a very clean sharing model
Dialogue: 0,0:27:36.26,0:27:42.30,English,,0,0,0,,Right so there's private address spaces between the all of the children and the parent
Dialogue: 0,0:27:43.06,0:27:50.94,English,,0,0,0,,They shared,they have separate descriptors,but they share they have separate copies of the descriptor table
Dialogue: 0,0:27:50.96,0:27:52.94,English,,0,0,0,,But they share the same open file table
Dialogue: 0,0:27:54.88,0:27:55.20,English,,0,0,0,,Okay
Dialogue: 0,0:27:56.58,0:28:03.36,English,,0,0,0,,And there's so in some sense this is a simplest possible way to create a concurrent servers
Dialogue: 0,0:28:03.96,0:28:05.00,English,,0,0,0,,And if you can get
Dialogue: 0,0:28:05.12,0:28:12.18,English,,0,0,0,,If you can get by with not sharing any global variables or sharing address basis then this is the way to go
Dialogue: 0,0:28:15.64,0:28:19.18,English,,0,0,0,,The disadvantage is that there's additional overhead for
Dialogue: 0,0:28:19.54,0:28:24.52,English,,0,0,0,,Even with this processes copy-on-write trick that we saw for sharing the
Dialogue: 0,0:28:25.68,0:28:28.34,English,,0,0,0,,Sharing the address space between the parent and the child still
Dialogue: 0,0:28:30.26,0:28:32.40,English,,0,0,0,,It's still non-trivial overhead
Dialogue: 0,0:28:33.36,0:28:38.02,English,,0,0,0,,And it's you have to actually do a lot of work if you want to share data between processes
Dialogue: 0,0:28:38.14,0:28:42.76,English,,0,0,0,,So like let's say you want to have some kind of a shared cache between multiple processes
Dialogue: 0,0:28:43.58,0:28:47.96,English,,0,0,0,,Either have to use files okay on disk
Dialogue: 0,0:28:48.76,0:28:55.08,English,,0,0,0,,Or if you want to share memory you have to use  some kind of you have to use some kind of memory mapping
Dialogue: 0,0:28:55.64,0:29:01.46,English,,0,0,0,,Or you have to use what's these inter process communication mechanisms which
Dialogue: 0,0:29:02.10,0:29:07.94,English,,0,0,0,,We haven't talked about but there's ways pipes are probably the ones you're most familiar with
Dialogue: 0,0:29:08.54,0:29:13.76,English,,0,0,0,,So a pipe allows one process to send data to another process
Dialogue: 0,0:29:16.08,0:29:19.68,English,,0,0,0,,And there's ways to share memory between processes
Dialogue: 0,0:29:19.68,0:29:26.46,English,,0,0,0,,But they're cumbersome and require i have to be implemented with care
Dialogue: 0,0:29:28.46,0:29:32.20,English,,0,0,0,,Okay now the second approach is we call an event-based server
Dialogue: 0,0:29:33.24,0:29:39.88,English,,0,0,0,,So the idea here is that the server maintains a set of active connections
Dialogue: 0,0:29:40.58,0:29:48.85,English,,0,0,0,,So it  has an array of of connected file descriptors from different clients okay
Dialogue: 0,0:29:49.86,0:29:53.08,English,,0,0,0,,And then it determines which of those and it also has a listening descriptor
Dialogue: 0,0:29:54.58,0:29:59.70,English,,0,0,0,,And then it determines which of those descriptors have pending input
Dialogue: 0,0:30:00.90,0:30:08.30,English,,0,0,0,,And it determines this using a system call called select or epoll there's several ways to determine this
Dialogue: 0,0:30:09.26,0:30:11.66,English,,0,0,0,,But basically using select or a epoll
Dialogue: 0,0:30:13.54,0:30:17.76,English,,0,0,0,,You can determine which of a set of descriptors has pending input right and
Dialogue: 0,0:30:21.40,0:30:28.68,English,,0,0,0,,Then this and so the arrival of input at a descriptors is called an event because it changes the state of the descriptor
Dialogue: 0,0:30:30.54,0:30:35.16,English,,0,0,0,,So an event is always event in general is always some kind of state change
Dialogue: 0,0:30:38.34,0:30:43.78,English,,0,0,0,,So in this case when data arrives on a socket  that's a change in the state
Dialogue: 0,0:30:43.84,0:30:51.62,English,,0,0,0,,So there was no data before the event after the event now there's data that the server can read
Dialogue: 0,0:30:54.78,0:31:01.34,English,,0,0,0,,So if the listening descriptor has input then the server calls except to accept the connection
Dialogue: 0,0:31:02.08,0:31:07.12,English,,0,0,0,,And for all and then all connected descriptors that have pending inputs
Dialogue: 0,0:31:07.72,0:31:11.48,English,,0,0,0,,It services those it reads from those in some order
Dialogue: 0,0:31:15.76,0:31:18.48,English,,0,0,0,,Okay now that the details for how to do this are described in the book
Dialogue: 0,0:31:20.64,0:31:21.82,English,,0,0,0,,But basically
Dialogue: 0,0:31:23.90,0:31:27.48,English,,0,0,0,,I mean the conceptually is pretty simple it's actually tricky to implement
Dialogue: 0,0:31:29.24,0:31:32.06,English,,0,0,0,,But the idea is that there's some set of active descriptors
Dialogue: 0,0:31:32.54,0:31:36.74,English,,0,0,0,,Right there's some set of descriptors connected descriptors that you're using
Dialogue: 0,0:31:38.30,0:31:41.98,English,,0,0,0,,That are being used right now to to interact with a client
Dialogue: 0,0:31:44.74,0:31:49.66,English,,0,0,0,,There's some that are inactive so if descriptors was closed
Dialogue: 0,0:31:49.70,0:31:52.24,English,,0,0,0,,Then it's no longer active right
Dialogue: 0,0:31:53.64,0:31:56.50,English,,0,0,0,,And then there's other descriptors that have never been used
Dialogue: 0,0:31:56.56,0:31:58.52,English,,0,0,0,,So we just have this array of descriptors
Dialogue: 0,0:32:01.00,0:32:12.14,English,,0,0,0,,And then we record there you know the the descriptor number for each of those connected for each of those descriptors
Dialogue: 0,0:32:17.50,0:32:20.38,English,,0,0,0,,And then using select or a epoll or some other mechanism
Dialogue: 0,0:32:20.66,0:32:25.06,English,,0,0,0,,We somehow determine which of those active descriptors have input
Dialogue: 0,0:32:26.02,0:32:27.54,English,,0,0,0,,And then we service each of those
Dialogue: 0,0:32:28.26,0:32:30.68,English,,0,0,0,,In the case of listenfd by calling except
Dialogue: 0,0:32:31.30,0:32:35.58,English,,0,0,0,,In the case of these connected descriptors actually either
Dialogue: 0,0:32:36.66,0:32:39.00,English,,0,0,0,,This should be connectfd not clientfd
Dialogue: 0,0:32:41.16,0:32:45.08,English,,0,0,0,,But in the case of these connected descriptors we read the data from them
Dialogue: 0,0:32:49.08,0:32:54.68,English,,0,0,0,,And the when we read the data from each descriptor we do some work
Dialogue: 0,0:32:57.66,0:33:01.00,English,,0,0,0,,Okay so data arrives that a descriptor
Dialogue: 0,0:33:01.60,0:33:04.12,English,,0,0,0,,And then we read that data and then we do some kind of work
Dialogue: 0,0:33:07.02,0:33:11.18,English,,0,0,0,,Maybe in the case of an echo server we echo it right back okay
Dialogue: 0,0:33:12.44,0:33:14.40,English,,0,0,0,,In the case of a web server we may
Dialogue: 0,0:33:15.54,0:33:22.18,English,,0,0,0,,If that data was http request we might go and fetch a file from disk and return it
Dialogue: 0,0:33:22.66,0:33:28.70,English,,0,0,0,,Okay but in any case we notice that the descriptor has some data
Dialogue: 0,0:33:29.50,0:33:33.16,English,,0,0,0,,We read that data and then we respond to it in some way
Dialogue: 0,0:33:34.54,0:33:39.64,English,,0,0,0,,Okay so that response those multiple responses are concurrent flows
Dialogue: 0,0:33:41.58,0:33:44.90,English,,0,0,0,,Okay the we're interacting with that client
Dialogue: 0,0:33:46.06,0:33:51.84,English,,0,0,0,,We're interacting we're creating concurrent flows while concurrent flow for each client
Dialogue: 0,0:33:52.74,0:33:55.40,English,,0,0,0,,And we're servicing those clients concurrently
Dialogue: 0,0:33:55.96,0:34:00.24,English,,0,0,0,,Okay so even though it's a sequential program right we're not using fork or anything
Dialogue: 0,0:34:00.32,0:34:03.96,English,,0,0,0,,It's just a c program straightforward c program
Dialogue: 0,0:34:05.66,0:34:09.52,English,,0,0,0,,We're writing in such a way that we're creating our own concurrent flows
Dialogue: 0,0:34:16.34,0:34:20.48,English,,0,0,0,,So there's...as with any approach there's advantages and disadvantages
Dialogue: 0,0:34:20.58,0:34:24.20,English,,0,0,0,,So the a big advantage of event based servers is that it's just
Dialogue: 0,0:34:24.60,0:34:29.16,English,,0,0,0,,A sequoia it's one process with one address space
Dialogue: 0,0:34:29.48,0:34:33.62,English,,0,0,0,,Right so it's very easy you can use conventional debugger gdb to step through
Dialogue: 0,0:34:33.62,0:34:36.32,English,,0,0,0,,You can see everything you have access to everything
Dialogue: 0,0:34:36.88,0:34:40.64,English,,0,0,0,,So in that sense they're much simpler to debug understand
Dialogue: 0,0:34:42.90,0:34:45.48,English,,0,0,0,,And then there's no process or thread control overhead
Dialogue: 0,0:34:46.14,0:34:53.90,English,,0,0,0,,So when we service a particular descriptor it's very...there's very little overhead right
Dialogue: 0,0:34:54.18,0:34:58.98,English,,0,0,0,,We just the only overhead is sort of determining that descriptor has input available
Dialogue: 0,0:35:03.04,0:35:05.10,English,,0,0,0,,Okay and so because of that this is the
Dialogue: 0,0:35:06.88,0:35:13.54,English,,0,0,0,,High performance web servers like nodejs,nginx,tornado they all use this event based approach gates
Dialogue: 0,0:35:15.40,0:35:21.68,English,,0,0,0,,If you want to get over 10,000 requests per second you have to go with something like this
Dialogue: 0,0:35:23.46,0:35:27.62,English,,0,0,0,,Okay the disadvantage is it's much harder to more complexed
Dialogue: 0,0:35:28.16,0:35:32.70,English,,0,0,0,,It to code up then the other processor thread based designs
Dialogue: 0,0:35:35.84,0:35:37.22,English,,0,0,0,,And it's very difficult
Dialogue: 0,0:35:38.82,0:35:42.98,English,,0,0,0,,So one of the hardest aspects of the writing and event based server
Dialogue: 0,0:35:43.60,0:35:47.26,English,,0,0,0,,Is that you have to figure out how much work you're going to do in response to an event
Dialogue: 0,0:35:49.46,0:35:52.18,English,,0,0,0,,Okay so let me give you...let's say that this server is a web server
Dialogue: 0,0:35:53.50,0:35:58.10,English,,0,0,0,,And you get input on one of your connected file descriptor
Dialogue: 0,0:35:59.72,0:36:07.50,English,,0,0,0,,The simplest thing to do would be to then assume to read the entire http request
Dialogue: 0,0:36:08.50,0:36:12.02,English,,0,0,0,,And not return until you've read the entire request
Dialogue: 0,0:36:13.48,0:36:21.00,English,,0,0,0,,Okay so in that case the amount of work that you do in response to an event is very coarse-grained
Dialogue: 0,0:36:21.00,0:36:27.92,English,,0,0,0,,There's a lot of instructions because we're going to read every single line in that http request header
Dialogue: 0,0:36:28.68,0:36:31.88,English,,0,0,0,,But it's...so that's course that's an example of coarse grain
Dialogue: 0,0:36:33.74,0:36:39.04,English,,0,0,0,,It's very simple because every time you get a request on a connected descriptor you just read the whole
Dialogue: 0,0:36:39.56,0:36:43.52,English,,0,0,0,,You just read the whole http request  and then send a response
Dialogue: 0,0:36:45.00,0:36:45.66,English,,0,0,0,,Okay so there's
Dialogue: 0,0:36:46.72,0:36:49.92,English,,0,0,0,,On the other hand it's vulnerable
Dialogue: 0,0:36:50.94,0:36:55.68,English,,0,0,0,,Because what if a client misbehaves and doesn't send the entire http request
Dialogue: 0,0:36:55.80,0:36:57.52,English,,0,0,0,,What if it sends half of the requests
Dialogue: 0,0:36:58.84,0:37:03.60,English,,0,0,0,,So if you were doing to design an event based web server you probably wouldn't want to do that
Dialogue: 0,0:37:04.42,0:37:10.08,English,,0,0,0,,Right because that a single client we would be back in the situation we were before
Dialogue: 0,0:37:10.32,0:37:13.52,English,,0,0,0,,Where a single misbehaving client could sort of shut down the whole server
Dialogue: 0,0:37:14.64,0:37:15.72,English,,0,0,0,,So you might say well
Dialogue: 0,0:37:16.38,0:37:27.46,English,,0,0,0,,I'm going to my unit of work that i do in response to a request will be to read a single line from the request
Dialogue: 0,0:37:28.88,0:37:32.18,English,,0,0,0,,Okay so i'll read a single line and then i'll return
Dialogue: 0,0:37:33.70,0:37:38.34,English,,0,0,0,,Okay so every so now we're interleaving reading single lines
Dialogue: 0,0:37:39.06,0:37:43.36,English,,0,0,0,,And once i've read the entire request then out then i'll send the response
Dialogue: 0,0:37:44.88,0:37:48.40,English,,0,0,0,,So that's better right so a misbehaving client
Dialogue: 0,0:37:49.48,0:37:54.04,English,,0,0,0,,If sending like whole text lines at a time
Dialogue: 0,0:37:54.64,0:37:59.86,English,,0,0,0,,Even if it stops halfway through we'll still be able to make progress in service other other clients
Dialogue: 0,0:38:01.14,0:38:03.40,English,,0,0,0,,So that's a finer grained approach it's better
Dialogue: 0,0:38:03.82,0:38:08.28,English,,0,0,0,,It's probably more robust than waiting for the whole request
Dialogue: 0,0:38:08.38,0:38:09.48,English,,0,0,0,,But it's still vulnerable
Dialogue: 0,0:38:10.52,0:38:12.64,English,,0,0,0,,Because the client could just send a partial line
Dialogue: 0,0:38:14.22,0:38:14.94,English,,0,0,0,,So now we're back
Dialogue: 0,0:38:15.96,0:38:25.00,English,,0,0,0,,So really the only way to write a robust event based web server is to be able to handle partial lines
Dialogue: 0,0:38:25.16,0:38:29.62,English,,0,0,0,,Just read when there's data available on a descriptor
Dialogue: 0,0:38:29.62,0:38:31.52,English,,0,0,0,,You just read whatever data is available
Dialogue: 0,0:38:32.16,0:38:33.52,English,,0,0,0,,You remember how much you read
Dialogue: 0,0:38:33.52,0:38:38.04,English,,0,0,0,,If it's not a whole line you somehow have to remember that you have to buffer it remember it
Dialogue: 0,0:38:38.40,0:38:40.18,English,,0,0,0,,So now it's getting really complicated right
Dialogue: 0,0:38:41.16,0:38:49.42,English,,0,0,0,,But that's what you that's the price you pay for this low overhead sort of easier to debug kind of kind of model
Dialogue: 0,0:38:50.52,0:38:56.02,English,,0,0,0,,And then another disadvantage is that you can't since it's really a sequential program right it's one c program
Dialogue: 0,0:38:56.44,0:38:58.42,English,,0,0,0,,You can't take advantage of multiple cores
Dialogue: 0,0:38:59.28,0:39:02.20,English,,0,0,0,,So the only way to get sort of more performance out of something
Dialogue: 0,0:39:02.74,0:39:06.42,English,,0,0,0,,An event-based servers just to replicate copies of that server
Dialogue: 0,0:39:07.66,0:39:12.68,English,,0,0,0,,But you can't make an individual server go faster by using multiple cores
Dialogue: 0,0:39:15.76,0:39:21.54,English,,0,0,0,,Okay the third approach is to use threads to create these concurrent flows
Dialogue: 0,0:39:22.52,0:39:27.62,English,,0,0,0,,It's very similar to processes but there are some important differences
Dialogue: 0,0:39:29.02,0:39:33.44,English,,0,0,0,,So let's look first at what we mean by a thread
Dialogue: 0,0:39:35.40,0:39:41.76,English,,0,0,0,,So let's go back i'm going to...let's go back in and look at the traditional view of a process
Dialogue: 0,0:39:41.78,0:39:47.30,English,,0,0,0,,So we think of a process as some context that's data structures in the kernel
Dialogue: 0,0:39:47.86,0:39:50.30,English,,0,0,0,,Okay data that the kernel keeps about that process
Dialogue: 0,0:39:50.98,0:39:58.76,English,,0,0,0,,As well as this private address space which contains a stack  code and data and then the stack
Dialogue: 0,0:40:00.26,0:40:03.62,English,,0,0,0,,Okay and then as part of the and then the
Dialogue: 0,0:40:04.24,0:40:12.62,English,,0,0,0,,The process context consists of context that's associated with the program like the registers,condition codes,program counter,stack pointer
Dialogue: 0,0:40:14.10,0:40:19.50,English,,0,0,0,,And then it contains kernel context which is information in the kernel
Dialogue: 0,0:40:20.02,0:40:23.76,English,,0,0,0,,That the kernel needs to implement this idea of a process
Dialogue: 0,0:40:25.58,0:40:27.80,English,,0,0,0,,Okay so all of this data is actually stored in the kernel
Dialogue: 0,0:40:27.82,0:40:37.86,English,,0,0,0,,But some of the data is directly associated with the program and other data is sort of
Dialogue: 0,0:40:38.00,0:40:42.16,English,,0,0,0,,Support data that the kernel needs to implement processes
Dialogue: 0,0:40:43.76,0:40:45.32,English,,0,0,0,,Okay so let's just take this picture
Dialogue: 0,0:40:46.90,0:40:49.74,English,,0,0,0,,And we're just going to just move things around a little bit
Dialogue: 0,0:40:54.76,0:41:00.78,English,,0,0,0,,So what i've done is i've taken the stack off of the this virtual address space
Dialogue: 0,0:41:03.02,0:41:06.96,English,,0,0,0,,And sort of pulled it out along with its stack pointer
Dialogue: 0,0:41:07.66,0:41:13.02,English,,0,0,0,,And the contacts that's associated with the program
Dialogue: 0,0:41:14.02,0:41:18.64,English,,0,0,0,,So the data registers the condition codes the stack pointer the program counter
Dialogue: 0,0:41:19.40,0:41:23.64,English,,0,0,0,,Okay and i've just renamed a thread context instead of program context
Dialogue: 0,0:41:23.76,0:41:26.94,English,,0,0,0,,But it's the same it's the same thing
Dialogue: 0,0:41:28.02,0:41:31.78,English,,0,0,0,,And then i'm going to call this whole thing the combination of a stack
Dialogue: 0,0:41:33.14,0:41:35.68,English,,0,0,0,,And this thread context i'm gonna call that a thread
Dialogue: 0,0:41:40.50,0:41:44.40,English,,0,0,0,,And then everything else remains the same
Dialogue: 0,0:41:44.42,0:41:47.12,English,,0,0,0,,It's we have...we still have a code and data
Dialogue: 0,0:41:48.02,0:41:49.58,English,,0,0,0,,And we have the kernel context
Dialogue: 0,0:41:53.20,0:41:57.86,English,,0,0,0,,Okay now by doing this sort of refactoring and just moving things around
Dialogue: 0,0:41:59.02,0:42:02.56,English,,0,0,0,,I can now imagine so this stack isn't very much right
Dialogue: 0,0:42:04.58,0:42:08.03,English,,0,0,0,,It's so there's that,there's this,there's the stack space
Dialogue: 0,0:42:08.32,0:42:12.22,English,,0,0,0,,And then there's a little bit of data here in the form of some registers and some things to be stored
Dialogue: 0,0:42:12.22,0:42:16.06,English,,0,0,0,,But in this is a fairly small amount of data
Dialogue: 0,0:42:17.46,0:42:19.10,English,,0,0,0,,Now when i really like this
Dialogue: 0,0:42:20.18,0:42:21.00,English,,0,0,0,,Now i can
Dialogue: 0,0:42:22.36,0:42:26.54,English,,0,0,0,,I can think about there being multiple threads associated with the same process
Dialogue: 0,0:42:28.12,0:42:32.92,English,,0,0,0,,If i just keep different set of data thread context for each thread
Dialogue: 0,0:42:33.64,0:42:38.82,English,,0,0,0,,And then a private a separate portion of the stack that's associated with that thread
Dialogue: 0,0:42:40.72,0:42:45.73,English,,0,0,0,,So now each thread shares the same code and data
Dialogue: 0,0:42:46.42,0:42:50.54,English,,0,0,0,,Same virtual address space has the same kernel context
Dialogue: 0,0:42:51.44,0:42:52.92,English,,0,0,0,,Okay the same i/o structures
Dialogue: 0,0:42:55.80,0:43:01.28,English,,0,0,0,,But now it has its own separate individual stack
Dialogue: 0,0:43:02.84,0:43:09.90,English,,0,0,0,,Okay so local variables things that you would store on a stack now would be would be private sort of independent
Dialogue: 0,0:43:10.46,0:43:14.90,English,,0,0,0,,And it has its own set of its own program counter its own stack pointer
Dialogue: 0,0:43:15.58,0:43:17.64,English,,0,0,0,,Its own set of registers and condition codes
Dialogue: 0,0:43:23.14,0:43:26.06,English,,0,0,0,,And then the one difference is now is that each thread
Dialogue: 0,0:43:26.62,0:43:31.80,English,,0,0,0,,Instead of there's still a process id for this process that's pointed part of the kernel context
Dialogue: 0,0:43:32.44,0:43:36.42,English,,0,0,0,,But each thread now has its own thread id as part of its thread context
Dialogue: 0,0:43:39.62,0:43:40.38,English,,0,0,0,,Okay so now
Dialogue: 0,0:43:43.58,0:43:46.30,English,,0,0,0,,The kernel can just treat each of these as separate flows
Dialogue: 0,0:43:46.80,0:43:49.34,English,,0,0,0,,Right there's a separate so it's just like a process now
Dialogue: 0,0:43:49.84,0:43:54.42,English,,0,0,0,,The kernel can remember where the each thread has its own program counter
Dialogue: 0,0:43:55.72,0:43:58.28,English,,0,0,0,,And but they're running code out of the same
Dialogue: 0,0:43:58.88,0:44:02.64,English,,0,0,0,,That code the same code section in the virtual address space
Dialogue: 0,0:44:02.68,0:44:06.04,English,,0,0,0,,So the sharing code sharing data
Dialogue: 0,0:44:06.42,0:44:07.88,English,,0,0,0,,But they have their own program counter
Dialogue: 0,0:44:07.98,0:44:10.34,English,,0,0,0,,So they can the kernel can provide
Dialogue: 0,0:44:10.98,0:44:15.10,English,,0,0,0,,Can create each of these threads as a separate flow of control
Dialogue: 0,0:44:15.48,0:44:17.96,English,,0,0,0,,That it then schedules just like it does of a process
Dialogue: 0,0:44:18.48,0:44:21.02,English,,0,0,0,,Or in a similar way that does is a process
Dialogue: 0,0:44:21.94,0:44:32.80,English,,0,0,0,,But the difference is the reason that for threads in the first place is that when the colonel wants to contact switch from one thread to another
Dialogue: 0,0:44:33.78,0:44:39.18,English,,0,0,0,,There's not that much information that has to be saved and restored
Dialogue: 0,0:44:39.24,0:44:40.76,English,,0,0,0,,It's just a small amount of data
Dialogue: 0,0:44:41.54,0:44:44.76,English,,0,0,0,,So the kernel has to save this the data thread one context
Dialogue: 0,0:44:45.58,0:44:49.72,English,,0,0,0,,In some data structure some way and then restore the context for thread 2
Dialogue: 0,0:44:50.62,0:44:53.52,English,,0,0,0,,But we're talking about a very low overhead kind of operation
Dialogue: 0,0:44:53.90,0:45:00.36,English,,0,0,0,,Right it doesn't have to mess around with page tables virtual address space or any other of the process context
Dialogue: 0,0:45:06.46,0:45:09.90,English,,0,0,0,,So threads are luck kind of like processes
Dialogue: 0,0:45:10.78,0:45:14.14,English,,0,0,0,,But they're different in the sense that they share the same virtual address space
Dialogue: 0,0:45:16.40,0:45:20.58,English,,0,0,0,,And unlike processes which are created by fork
Dialogue: 0,0:45:21.84,0:45:26.60,English,,0,0,0,,Which creates a strict process hierarchy threads are just pools
Dialogue: 0,0:45:27.26,0:45:32.94,English,,0,0,0,,You can think of threads as pools of concurrent flows that access the same code and data
Dialogue: 0,0:45:36.64,0:45:39.98,English,,0,0,0,,And then the kernel is responsible for scheduling those flows
Dialogue: 0,0:45:39.98,0:45:43.60,English,,0,0,0,,And if in a way that so each flow gets time on the processor
Dialogue: 0,0:45:46.72,0:45:51.74,English,,0,0,0,,So much like processes,much like concurrent processes
Dialogue: 0,0:45:52.26,0:45:54.62,English,,0,0,0,,We say the two threads are concurrent
Dialogue: 0,0:45:55.66,0:45:59.78,English,,0,0,0,,If their flows overlap in time otherwise they're sequential so this is the exact same
Dialogue: 0,0:46:00.32,0:46:02.74,English,,0,0,0,,Example that i showed you when we looked at processes
Dialogue: 0,0:46:03.22,0:46:08.50,English,,0,0,0,,So here you have instead of three processes we have three threads running it within the same process
Dialogue: 0,0:46:11.22,0:46:12.62,English,,0,0,0,,Thread a runs for a little bit
Dialogue: 0,0:46:13.02,0:46:18.78,English,,0,0,0,,And then the kernel just decides to walk it out and run thread b
Dialogue: 0,0:46:19.46,0:46:24.80,English,,0,0,0,,So then thread b runs for a little bit and then the kernel decides to give thread c some time
Dialogue: 0,0:46:24.84,0:46:30.02,English,,0,0,0,,So it saves thread b's context restores thread c's context
Dialogue: 0,0:46:30.32,0:46:36.58,English,,0,0,0,,And sets the pc to...the pc value
Dialogue: 0,0:46:36.60,0:46:42.48,English,,0,0,0,,And thread c's context and so c runs and then the kernel decides to give a some time again
Dialogue: 0,0:46:42.50,0:46:44.18,English,,0,0,0,,So then a runs some more
Dialogue: 0,0:46:45.42,0:46:51.88,English,,0,0,0,,So because thread a and b overlap in time they're running concurrently
Dialogue: 0,0:46:54.10,0:46:57.98,English,,0,0,0,,B and c don't overlap in time so they're not running concurrently
Dialogue: 0,0:46:58.50,0:47:02.72,English,,0,0,0,,But a and c are concurrent because they overlap in time
Dialogue: 0,0:47:04.80,0:47:10.78,English,,0,0,0,,And so you can...you also have the option if there's multiple cores than multiple threat
Dialogue: 0,0:47:10.84,0:47:14.62,English,,0,0,0,,A thread can run on each core so then you can have true parallelism
Dialogue: 0,0:47:14.94,0:47:15.48,English,,0,0,0,,Okay
Dialogue: 0,0:47:21.50,0:47:28.02,English,,0,0,0,,Okay so like i said threads and processes are similar ideas
Dialogue: 0,0:47:28.02,0:47:37.56,English,,0,0,0,,But in the sense that they each stage kinase and process corresponds to some kind of logical flow
Dialogue: 0,0:47:40.00,0:47:42.42,English,,0,0,0,,And they can run concurrently with other flows
Dialogue: 0,0:47:43.42,0:47:46.68,English,,0,0,0,,And each is scheduled and contact switched by the kernel
Dialogue: 0,0:47:48.98,0:47:54.30,English,,0,0,0,,Okay but they're different because threads share all code and data except their local stacks
Dialogue: 0,0:47:56.94,0:48:05.98,English,,0,0,0,,And in fact those local stacks although they're each thread has its own it's really just sharing the same stack
Dialogue: 0,0:48:08.14,0:48:10.80,English,,0,0,0,,And so it's really the same portion of the virtual address space
Dialogue: 0,0:48:10.94,0:48:15.52,English,,0,0,0,,It's just that each thread is assigned its own part of that stack
Dialogue: 0,0:48:15.70,0:48:19.04,English,,0,0,0,,Okay so even though threads have their own individual stacks
Dialogue: 0,0:48:20.86,0:48:22.96,English,,0,0,0,,Since it's all part of the same virtual address space
Dialogue: 0,0:48:23.46,0:48:26.70,English,,0,0,0,,A thread can access the stack of any other thread
Dialogue: 0,0:48:27.92,0:48:29.98,English,,0,0,0,,If it's and that's not a good thing to do but it's possible
Dialogue: 0,0:48:33.10,0:48:35.26,English,,0,0,0,,Okay so processes don't share any of the state
Dialogue: 0,0:48:35.42,0:48:38.42,English,,0,0,0,,Right they have their own private address spaces
Dialogue: 0,0:48:39.00,0:48:42.96,English,,0,0,0,,And threads are less expensive than processes
Dialogue: 0,0:48:46.26,0:48:47.80,English,,0,0,0,,It's cheaper to create them
Dialogue: 0,0:48:47.80,0:48:52.94,English,,0,0,0,,And the main reason is that there's just less context associated with the thread than there is as a process
Dialogue: 0,0:48:53.70,0:48:55.76,English,,0,0,0,,And so on our system when we measure
Dialogue: 0,0:48:57.56,0:49:05.94,English,,0,0,0,,A bunch of we just create in and wait for a bunch of creative process wait for word process over and over again and measure the time
Dialogue: 0,0:49:06.40,0:49:10.38,English,,0,0,0,,Turns out to be about twenty thousand cycles to create and
Dialogue: 0,0:49:10.78,0:49:15.90,English,,0,0,0,,Reap a process about ten thousand cycles to create an reapeth thread
Dialogue: 0,0:49:17.54,0:49:24.66,English,,0,0,0,,So that the kernel provides threads to us using an interface called the pthreads  posix threads
Dialogue: 0,0:49:25.52,0:49:36.48,English,,0,0,0,,So this is a fairly recent standard that all linux,unix systems and windows macintosh
Dialogue: 0,0:49:36.56,0:49:46.70,English,,0,0,0,,So every this is a sort of standard posix standard for manipulating threads
Dialogue: 0,0:49:47.64,0:49:50.86,English,,0,0,0,,And so you can do things like create and reap threads
Dialogue: 0,0:49:50.86,0:49:53.60,English,,0,0,0,,So there's this is sort of sort of like fork
Dialogue: 0,0:49:54.68,0:49:59.36,English,,0,0,0,,And this is sort of like weight but not quite because it doesn't create
Dialogue: 0,0:50:01.92,0:50:04.22,English,,0,0,0,,There's no hierarchy associated with these
Dialogue: 0,0:50:04.80,0:50:12.66,English,,0,0,0,,You can you could just like get pit you can retain there's a function to get your thread id
Dialogue: 0,0:50:13.48,0:50:16.84,English,,0,0,0,,You can kill threads so one thread can kill another thread
Dialogue: 0,0:50:18.84,0:50:21.60,English,,0,0,0,,A thread there's a there's a function to exit a thread
Dialogue: 0,0:50:23.30,0:50:26.92,English,,0,0,0,,The normal exit system call terminates all the threads
Dialogue: 0,0:50:27.72,0:50:35.10,English,,0,0,0,,And return is similar to pthread_exit in the sense that just terminates the current thread that calls it
Dialogue: 0,0:50:36.26,0:50:40.50,English,,0,0,0,,And then there's ways to access synchronize access to shared variables
Dialogue: 0,0:50:41.04,0:50:43.92,English,,0,0,0,,Which we'll look at on thursday more detail
Dialogue: 0,0:50:47.22,0:50:50.50,English,,0,0,0,,Okay so here's the pthreads hello world program
Dialogue: 0,0:50:52.66,0:50:56.38,English,,0,0,0,,You know your k&r book the the c reference manual
Dialogue: 0,0:50:57.12,0:51:03.34,English,,0,0,0,,The very first thing that it does is it shows you how to write the simplest possible c program called
Dialogue: 0,0:51:03.66,0:51:06.24,English,,0,0,0,,So the famous hello world program
Dialogue: 0,0:51:06.68,0:51:11.02,English,,0,0,0,,So that goes all the way back to like 1978 when k&r book was first written
Dialogue: 0,0:51:11.76,0:51:14.36,English,,0,0,0,,But that's caught on
Dialogue: 0,0:51:14.86,0:51:19.60,English,,0,0,0,,And whenever we like learn a new language or we learn a new concept
Dialogue: 0,0:51:19.92,0:51:22.48,English,,0,0,0,,We always write the hello world program for that concept
Dialogue: 0,0:51:23.12,0:51:29.62,English,,0,0,0,,So the hello world program for threads this is a simplest threads program that i can think of
Dialogue: 0,0:51:29.64,0:51:32.30,English,,0,0,0,,So i call it the hello world program for threads
Dialogue: 0,0:51:33.56,0:51:38.16,English,,0,0,0,,So this program defines a function
Dialogue: 0,0:51:39.60,0:51:41.14,English,,0,0,0,,So court by in posix
Dialogue: 0,0:51:41.70,0:51:49.12,English,,0,0,0,,A thread is actually executed by executing the the code and a function called the thread routine
Dialogue: 0,0:51:50.04,0:51:52.82,English,,0,0,0,,Okay and posix imposes
Dialogue: 0,0:51:54.76,0:51:59.54,English,,0,0,0,,This thread routine takes a generic pointer as an argument optional generic pointer
Dialogue: 0,0:52:00.16,0:52:02.02,English,,0,0,0,,And it returns a generic pointer
Dialogue: 0,0:52:02.82,0:52:10.54,English,,0,0,0,,Okay so anytime if you want to pass anything to a thread you somehow got to pack up all that data into a single object
Dialogue: 0,0:52:10.78,0:52:12.78,English,,0,0,0,,Then you can take an address up
Dialogue: 0,0:52:14.78,0:52:17.68,English,,0,0,0,,But it's it you can see that this is extremely general-purpose
Dialogue: 0,0:52:21.22,0:52:28.00,English,,0,0,0,,So our hello world program creates a thread by calling pthread create
Dialogue: 0,0:52:29.72,0:52:32.84,English,,0,0,0,,And we say instead of a child we call this a peer thread
Dialogue: 0,0:52:34.56,0:52:42.18,English,,0,0,0,,Okay so that there's no parent-child relationship any threads can threads can reap other threads
Dialogue: 0,0:52:43.74,0:52:45.76,English,,0,0,0,,Whether they created those threads or not
Dialogue: 0,0:52:46.44,0:52:49.34,English,,0,0,0,,Okay so you don't have the strict parent-child hierarchy
Dialogue: 0,0:52:50.48,0:52:59.90,English,,0,0,0,,Pthread_create creates a thread that executes the thread routine in this third argument
Dialogue: 0,0:53:01.12,0:53:09.14,English,,0,0,0,,And returns the thread id of that thread in the address and the integer pointed at by the first argument
Dialogue: 0,0:53:10.82,0:53:18.42,English,,0,0,0,,The second argument is set up there's ways to set attributes of threads
Dialogue: 0,0:53:19.20,0:53:24.82,English,,0,0,0,,That are beyond the scope of this course and will always just have those no
Dialogue: 0,0:53:28.20,0:53:33.34,English,,0,0,0,,And then that this fourth argument is the optional argument that you want to pass to your thread routine
Dialogue: 0,0:53:33.92,0:53:37.56,English,,0,0,0,,Okay so in this case we're saying call the thread routine
Dialogue: 0,0:53:38.26,0:53:42.08,English,,0,0,0,,That's called thread with no arguments
Dialogue: 0,0:53:44.24,0:53:47.34,English,,0,0,0,,And then our thread routine just prints out hello world
Dialogue: 0,0:53:48.08,0:53:52.14,English,,0,0,0,,And then it returns in this case it doesn't return anything so it returns no
Dialogue: 0,0:53:53.04,0:53:57.82,English,,0,0,0,,But if we wanted to return something to the calling program
Dialogue: 0,0:53:58.26,0:54:03.82,English,,0,0,0,,We could have returned something a pointer to some generic object okay
Dialogue: 0,0:54:11.40,0:54:17.62,English,,0,0,0,,Okay so the thread id the thread attributes are null that the thread routine the thread arguments are void *p
Dialogue: 0,0:54:18.24,0:54:23.60,English,,0,0,0,,And the return value is a void double
Dialogue: 0,0:54:23.64,0:54:26.40,English,,0,0,0,,So it's a pointer to the pointer that you want to return
Dialogue: 0,0:54:29.44,0:54:32.18,English,,0,0,0,,Okay all right so let's look at what happens when we execute hello world
Dialogue: 0,0:54:33.26,0:54:36.04,English,,0,0,0,,So the main thread runs for a while
Dialogue: 0,0:54:37.94,0:54:39.74,English,,0,0,0,,Then it calls pthread_create
Dialogue: 0,0:54:42.14,0:54:45.90,English,,0,0,0,,Which creates the peer thread which now is a concurrent flow that's running
Dialogue: 0,0:54:46.38,0:54:53.86,English,,0,0,0,,Once the pthread create returns then we're running two concurrent flows or running the main thread
Dialogue: 0,0:54:54.64,0:54:57.10,English,,0,0,0,,And we're running the peer thread
Dialogue: 0,0:55:02.04,0:55:08.40,English,,0,0,0,,And so in this case hello world waits for the peer thread to finish by calling pthread_join
Dialogue: 0,0:55:11.44,0:55:16.52,English,,0,0,0,,The peer thread after the calls it's printf it returns
Dialogue: 0,0:55:18.04,0:55:26.76,English,,0,0,0,,No which terminates the thread at that point the pthread_join returns and the main thread continues
Dialogue: 0,0:55:31.16,0:55:40.32,English,,0,0,0,,Okay so using these create this create function how would we write a thread based concurrent echo server
Dialogue: 0,0:55:41.14,0:55:44.58,English,,0,0,0,,And again it's very similar to the way we did it with the process based design
Dialogue: 0,0:55:49.82,0:55:58.08,English,,0,0,0,,So we call we acquire a listening descriptor by calling open_listenfd function just as before
Dialogue: 0,0:55:59.06,0:56:01.22,English,,0,0,0,,And then inside this infinite server loop
Dialogue: 0,0:56:04.00,0:56:06.44,English,,0,0,0,,We get the size of the
Dialogue: 0,0:56:08.04,0:56:17.30,English,,0,0,0,,We get the size of the clientaddr struct so which is going to be a large enough to fit any address
Dialogue: 0,0:56:19.02,0:56:23.48,English,,0,0,0,,And then we malloc space for the connected file descriptor
Dialogue: 0,0:56:23.52,0:56:31.30,English,,0,0,0,,So we're making a one integer sized portion of dynamic storage for this connected descriptor
Dialogue: 0,0:56:32.06,0:56:34.58,English,,0,0,0,,That were that we're going to get back from accept
Dialogue: 0,0:56:35.70,0:56:41.34,English,,0,0,0,,And we're going to...this is actually really important to avoid a nasty race condition
Dialogue: 0,0:56:41.98,0:56:43.28,English,,0,0,0,,They'll show you in a second
Dialogue: 0,0:56:45.46,0:56:52.84,English,,0,0,0,,So now we call except with our listening descriptor and clientaddr and clientlen just like before
Dialogue: 0,0:56:53.86,0:57:03.62,English,,0,0,0,,And except returns the connected descriptor and then we dereference this connected descriptor pointer
Dialogue: 0,0:57:04.02,0:57:09.56,English,,0,0,0,,And so and store that the value returned by except in this location in the heap
Dialogue: 0,0:57:11.38,0:57:13.06,English,,0,0,0,,And then we call pthread_create
Dialogue: 0,0:57:15.24,0:57:17.08,English,,0,0,0,,By giving it the name of our thread routine
Dialogue: 0,0:57:17.08,0:57:22.54,English,,0,0,0,,Which in this case is just simply a function we defined in our program called thread
Dialogue: 0,0:57:23.92,0:57:27.50,English,,0,0,0,,And the pointer to the connected file descriptor
Dialogue: 0,0:57:29.66,0:57:34.60,English,,0,0,0,,Okay now a client which a client which a thread routine then will use to interact with the client
Dialogue: 0,0:57:37.26,0:57:44.30,English,,0,0,0,,Okay so the thread routine dereferences the argument a member is a pointer to a connected file descriptor
Dialogue: 0,0:57:44.36,0:57:51.52,English,,0,0,0,,So it dereferences that pointer to get the to get the actual integer connected descriptor
Dialogue: 0,0:57:54.68,0:57:56.86,English,,0,0,0,,And then it detaches the thread
Dialogue: 0,0:57:59.80,0:58:01.60,English,,0,0,0,,So by default threads
Dialogue: 0,0:58:02.70,0:58:05.92,English,,0,0,0,,Threads run an independent and attached mode
Dialogue: 0,0:58:06.24,0:58:11.04,English,,0,0,0,,So they can be you know they can be joined by other threads and they can be killed by other threads
Dialogue: 0,0:58:12.68,0:58:13.10,English,,0,0,0,,But
Dialogue: 0,0:58:15.42,0:58:16.54,English,,0,0,0,,By default if
Dialogue: 0,0:58:17.64,0:58:27.22,English,,0,0,0,,It's similar when threads are running in unattached mode or undetected mode
Dialogue: 0,0:58:28.76,0:58:35.96,English,,0,0,0,,That when they die they have wait by a join function to acquire those resources
Dialogue: 0,0:58:36.76,0:58:44.90,English,,0,0,0,,But if we detach a thread then it can't be joined by any threads but when it dies the kernel will automatically
Dialogue: 0,0:58:46.56,0:58:51.52,English,,0,0,0,,Restore the the resources associated with that thread
Dialogue: 0,0:58:52.14,0:58:54.70,English,,0,0,0,,So in this case we're going to just detach this thread
Dialogue: 0,0:58:54.70,0:59:00.26,English,,0,0,0,,So we're not to worry about reaping it when it finishes
Dialogue: 0,0:59:01.98,0:59:06.04,English,,0,0,0,,And then we're going to free this memory that was malloc
Dialogue: 0,0:59:06.16,0:59:12.90,English,,0,0,0,,So this is important we have to free this memory that was malloc by the main thread in order to avoid a memory leak
Dialogue: 0,0:59:15.90,0:59:22.20,English,,0,0,0,,And then we call echo function so we interact with the the echo client until the echo clients finished
Dialogue: 0,0:59:23.84,0:59:29.04,English,,0,0,0,,And then we close this descriptor again to avoid a potentially fatal memory leak
Dialogue: 0,0:59:35.92,0:59:41.94,English,,0,0,0,,So this thread based education model is very similar to the execution model that we saw with processes
Dialogue: 0,0:59:42.66,0:59:47.38,English,,0,0,0,,So we have a list of a main thread that's listening for connection requests
Dialogue: 0,0:59:48.34,0:59:51.60,English,,0,0,0,,They were waiting for connection requests via accept
Dialogue: 0,0:59:52.88,0:59:59.28,English,,0,0,0,,And then we have for each client we have a peer thread that interacts with that client
Dialogue: 0,1:00:00.14,1:00:04.34,English,,0,0,0,,Using the connected descriptor that was passed in when we created the thread
Dialogue: 0,1:00:05.56,1:00:15.80,English,,0,0,0,,And then each thread has its own since it has its own stack it has separate space for its local variables
Dialogue: 0,1:00:16.68,1:00:19.86,English,,0,0,0,,And this is really the powerful thing about threads
Dialogue: 0,1:00:19.86,1:00:26.98,English,,0,0,0,,Now we can with these things they by declaring these local variables we can
Dialogue: 0,1:00:28.28,1:00:31.64,English,,0,0,0,,We can create threads that won't interact with each other
Dialogue: 0,1:00:32.00,1:00:33.42,English,,0,0,0,,It won't and can run independently yes
Dialogue: 0,1:00:33.42,1:00:43.16,English,,0,0,0,,[student speaking]
Dialogue: 0,1:00:43.16,1:00:46.54,English,,0,0,0,,I guess yeah the question is there any time you wouldn't want to run detached
Dialogue: 0,1:00:48.58,1:00:53.20,English,,0,0,0,,So when you run detached you give up the power to kill other threads so
Dialogue: 0,1:00:54.18,1:00:58.70,English,,0,0,0,,So i don't know if you it's hard to it's hard to come up with good example right that
Dialogue: 0,1:01:00.06,1:01:03.90,English,,0,0,0,,But if you wanted the ability to be able to terminate other threads
Dialogue: 0,1:01:04.90,1:01:06.28,English,,0,0,0,,You know maybe if you had
Dialogue: 0,1:01:07.14,1:01:09.90,English,,0,0,0,,Maybe if you were running a pool of like worker threads
Dialogue: 0,1:01:10.78,1:01:11.56,English,,0,0,0,,And at some point
Dialogue: 0,1:01:15.30,1:01:22.52,English,,0,0,0,,At some point if i mean i guess you can imagine a scenario where suppose you're running a pool of worker threads
Dialogue: 0,1:01:22.52,1:01:26.66,English,,0,0,0,,You give them all jobs to do the first one that finishes you take the result
Dialogue: 0,1:01:26.84,1:01:28.70,English,,0,0,0,,And you don't need the results from the other threads
Dialogue: 0,1:01:29.32,1:01:31.50,English,,0,0,0,,So you might just want to kill those threads
Dialogue: 0,1:01:33.76,1:01:39.26,English,,0,0,0,,But it's yeah it's hard to come up with a really compelling reason
Dialogue: 0,1:01:47.20,1:01:50.84,English,,0,0,0,,Okay so there's a few things to think about when you're writing thread based servers
Dialogue: 0,1:01:50.88,1:01:55.56,English,,0,0,0,,So the first is that yet you need to run detach to avoid potential memory leaks
Dialogue: 0,1:01:57.90,1:02:01.36,English,,0,0,0,,I'm sorry i forgot this word so it the opposite of detached is joinable
Dialogue: 0,1:02:02.06,1:02:09.28,English,,0,0,0,,And so joinable threads like i mentioned can be reaped and killed by other threads detached threads cannot
Dialogue: 0,1:02:09.90,1:02:13.76,English,,0,0,0,,And their resources are automatically claimed on termination
Dialogue: 0,1:02:14.88,1:02:21.62,English,,0,0,0,,So the default states joinable and you have to use this detach function call to make the thread detached
Dialogue: 0,1:02:24.24,1:02:26.46,English,,0,0,0,,The biggest issue with threads though
Dialogue: 0,1:02:26.76,1:02:30.56,English,,0,0,0,,Like the beautiful thing about threads is that you're sharing the same global address space
Dialogue: 0,1:02:30.58,1:02:36.16,English,,0,0,0,,So it's very easy to share data structures you know if you had multiple threads
Dialogue: 0,1:02:36.16,1:02:42.16,English,,0,0,0,,If you had a web server a concurrent web server that was built with multiple threads be very easy to implement a cache
Dialogue: 0,1:02:43.48,1:02:47.96,English,,0,0,0,,All the threads could use right because they're all sharing that same virtual address space
Dialogue: 0,1:02:48.80,1:02:51.60,English,,0,0,0,,But the thing that makes threads so nice
Dialogue: 0,1:02:51.64,1:02:58.62,English,,0,0,0,,This ease the ease with which you can share resources  is also the thing that makes them very tricky to deal with
Dialogue: 0,1:02:59.02,1:03:04.46,English,,0,0,0,,So as soon as just like we saw with our shell lab handlers
Dialogue: 0,1:03:05.28,1:03:10.61,English,,0,0,0,,You know as soon as you have multiple flows accessing shared resources you have to be very careful
Dialogue: 0,1:03:11.30,1:03:12.86,English,,0,0,0,,It's very easy to make mistakes
Dialogue: 0,1:03:15.04,1:03:23.48,English,,0,0,0,,And it's very easy to or it's possible to to share resources in unexpected and unintended ways
Dialogue: 0,1:03:24.56,1:03:34.12,English,,0,0,0,,For example if one thread passes the address of a local variable on its stack to another thread
Dialogue: 0,1:03:34.70,1:03:39.66,English,,0,0,0,,Then now that the called thread now has access to the callers thread
Dialogue: 0,1:03:40.00,1:03:49.04,English,,0,0,0,,And there's nothing to prevent that called thread from manipulating local variables on the caller stack
Dialogue: 0,1:03:49.88,1:03:51.64,English,,0,0,0,,You know that would be a very bad thing to do
Dialogue: 0,1:03:52.38,1:04:01.04,English,,0,0,0,,But it's possible you might forget,you know you might forget that the variable you're passing is a local variable, not a global
Dialogue: 0,1:04:07.10,1:04:10.10,English,,0,0,0,,Okay and a really bad mistake
Dialogue: 0,1:04:11.98,1:04:15.76,English,,0,0,0,,In our hello...in our echo server example
Dialogue: 0,1:04:17.44,1:04:20.66,English,,0,0,0,,Notice we were very careful to malloc you space for this
Dialogue: 0,1:04:25.52,1:04:31.10,English,,0,0,0,,For this connected file descriptor that we passed into that the peer thread that we are creating
Dialogue: 0,1:04:32.34,1:04:41.48,English,,0,0,0,,Would have been much easier just to pass the address of the connected file descriptor into a peer thread be much easier
Dialogue: 0,1:04:41.48,1:04:42.72,English,,0,0,0,,But it would also be wrong
Dialogue: 0,1:04:50.74,1:04:51.58,English,,0,0,0,,Can you say why
Dialogue: 0,1:04:58.54,1:05:00.96,English,,0,0,0,,Okay let's say right here when we call pthread_create
Dialogue: 0,1:05:01.94,1:05:06.76,English,,0,0,0,,Instead of passing a pointer to a separately allocated region of the heap
Dialogue: 0,1:05:07.70,1:05:11.92,English,,0,0,0,,Instead of doing that we just pass the address of the connected file descriptor same thing right
Dialogue: 0,1:05:17.98,1:05:27.40,English,,0,0,0,,Okay and then in a thread routine we dereference that pointer to to get the connected file descriptor
Dialogue: 0,1:05:28.66,1:05:31.28,English,,0,0,0,,Okay if we just pass the address of the connected file descriptor
Dialogue: 0,1:05:32.54,1:05:35.14,English,,0,0,0,,This is real it's really bad,can you say why
Dialogue: 0,1:05:36.32,1:05:43.58,English,,0,0,0,,Yes
Dialogue: 0,1:05:43.60,1:05:47.36,English,,0,0,0,,It does, okay that's true, and why is that
Dialogue: 0,1:05:51.74,1:05:58.90,English,,0,0,0,,[student speaking]
Dialogue: 0,1:05:58.90,1:06:03.16,English,,0,0,0,,That's yeah that's right so what this is assuming
Dialogue: 0,1:06:04.16,1:06:08.62,English,,0,0,0,,This entered this bypassing the address of this connected file descriptor
Dialogue: 0,1:06:09.88,1:06:10.98,English,,0,0,0,,We're introducing a race
Dialogue: 0,1:06:12.62,1:06:14.68,English,,0,0,0,,Okay in the race what we're assuming
Dialogue: 0,1:06:17.38,1:06:21.54,English,,0,0,0,,That the peer thread will be able to dereference that pointer
Dialogue: 0,1:06:22.52,1:06:23.24,English,,0,0,0,,Before
Dialogue: 0,1:06:25.32,1:06:29.16,English,,0,0,0,,The main thread goes back up and gets a new connected file descriptor
Dialogue: 0,1:06:32.48,1:06:34.14,English,,0,0,0,,Right so what happens what happens
Dialogue: 0,1:06:34.82,1:06:39.60,English,,0,0,0,,Right we can't in a concurrent system,we can't make any assumptions about how the kernel is going to schedule things
Dialogue: 0,1:06:40.04,1:06:42.08,English,,0,0,0,,Right we saw the same thing with processes
Dialogue: 0,1:06:43.82,1:06:46.26,English,,0,0,0,,So what happens if after pthread_create
Dialogue: 0,1:06:48.40,1:06:50.64,English,,0,0,0,,The main thread runs instead of the peer thread
Dialogue: 0,1:06:53.10,1:06:54.34,English,,0,0,0,,Okay so we've passed the
Dialogue: 0,1:06:55.30,1:06:59.54,English,,0,0,0,,We've passed the address of the connected file descriptor for this client
Dialogue: 0,1:07:01.02,1:07:03.76,English,,0,0,0,,That we accepted the connection requests from
Dialogue: 0,1:07:05.92,1:07:13.54,English,,0,0,0,,And then before the peer thread can dereference that argument we get a new connected file descriptor
Dialogue: 0,1:07:16.98,1:07:17.32,English,,0,0,0,,Okay
Dialogue: 0,1:07:20.16,1:07:21.44,English,,0,0,0,,And now the child runs
Dialogue: 0,1:07:22.32,1:07:25.98,English,,0,0,0,,And it dereferences that descriptor
Dialogue: 0,1:07:27.26,1:07:29.38,English,,0,0,0,,But what it gets now is the descriptor
Dialogue: 0,1:07:30.92,1:07:36.08,English,,0,0,0,,That's corresponding to the second child not the first child
Dialogue: 0,1:07:37.74,1:07:43.16,English,,0,0,0,,So now we have two threads talking to the same client using the same descriptor
Dialogue: 0,1:07:50.54,1:07:51.72,English,,0,0,0,,So do you see them hope
Dialogue: 0,1:07:54.98,1:08:01.48,English,,0,0,0,,So it's very tricky this is like a real subtle this is an example of sort of subtle errors that you can introduce
Dialogue: 0,1:08:01.52,1:08:03.12,English,,0,0,0,,Because of this unintended sharing
Dialogue: 0,1:08:08.32,1:08:15.94,English,,0,0,0,,And it's cause the root cause is as you correctly pointed out is that they're both sharing the same memory on the caller stack
Dialogue: 0,1:08:16.34,1:08:16.82,English,,0,0,0,,Yes
Dialogue: 0,1:08:17.18,1:08:21.30,English,,0,0,0,,[student speaking]
Dialogue: 0,1:08:21.36,1:08:28.66,English,,0,0,0,,In this case of what you could do there's another thing you could do you could just pass the descriptor itself
Dialogue: 0,1:08:29.98,1:08:35.72,English,,0,0,0,,Okay so you could just cast the descriptor to a generic pointer and just pass that
Dialogue: 0,1:08:36.60,1:08:39.18,English,,0,0,0,,And that that's just kind of yucky though that that would work
Dialogue: 0,1:08:40.88,1:08:47.12,English,,0,0,0,,Because instead of dereferencing it the child would just use it directly
Dialogue: 0,1:08:50.08,1:08:51.00,English,,0,0,0,,Okay so good that's good
Dialogue: 0,1:09:01.22,1:09:02.02,English,,0,0,0,,Okay the
Dialogue: 0,1:09:05.98,1:09:09.98,English,,0,0,0,,So the really good things with with threads is ease of sharing
Dialogue: 0,1:09:09.98,1:09:16.18,English,,0,0,0,,But that sharing also introduces can introduce complications
Dialogue: 0,1:09:16.82,1:09:22.96,English,,0,0,0,,In fact that's we're going to look at ways to sort of control the the sharing
Dialogue: 0,1:09:24.84,1:09:30.44,English,,0,0,0,,So that we do...so that we don't get unintended unintended sharing
Dialogue: 0,1:09:36.72,1:09:39.90,English,,0,0,0,,Okay so to summarize the the approaches to concurrency that
Dialogue: 0,1:09:40.58,1:09:44.18,English,,0,0,0,,That we've looked at we have process based concurrency
Dialogue: 0,1:09:44.62,1:09:47.94,English,,0,0,0,,So it's hard to share resources but
Dialogue: 0,1:09:48.56,1:09:52.96,English,,0,0,0,,It's easy to avoid unintended sharing so in some ways it's safer and easier to program
Dialogue: 0,1:09:53.66,1:09:57.90,English,,0,0,0,,Event based so it's it's very low-level very tedious
Dialogue: 0,1:09:58.34,1:10:04.92,English,,0,0,0,,You have to be very careful about how you the granularity of the work that you do in response to events
Dialogue: 0,1:10:07.00,1:10:11.54,English,,0,0,0,,But you have total control over scheduling so you can decide which
Dialogue: 0,1:10:12.38,1:10:18.90,English,,0,0,0,,Which descriptors you're going to service and in which order it sits since there's a single flow of control
Dialogue: 0,1:10:19.18,1:10:21.32,English,,0,0,0,,You can debug it with a debugger
Dialogue: 0,1:10:23.16,1:10:27.30,English,,0,0,0,,But it doesn't make use of multi-core so there's a handful of trade-offs there
Dialogue: 0,1:10:27.96,1:10:29.16,English,,0,0,0,,And with thread based systems
Dialogue: 0,1:10:30.48,1:10:36.02,English,,0,0,0,,It's very easy to share resources but that sharing can create problems of its own
Dialogue: 0,1:10:37.28,1:10:40.34,English,,0,0,0,,It's fairly efficient compared to two processors
Dialogue: 0,1:10:40.84,1:10:43.04,English,,0,0,0,,You don't have much control over the scheduling
Dialogue: 0,1:10:43.04,1:10:49.84,English,,0,0,0,,So just like we saw you're not you can't really control which threads get executed in which order
Dialogue: 0,1:10:51.28,1:10:52.70,English,,0,0,0,,And it can be difficult to debug
Dialogue: 0,1:10:52.78,1:10:59.92,English,,0,0,0,,Because there can be races that occur very rarely very infrequently and
Dialogue: 0,1:10:59.98,1:11:06.18,English,,0,0,0,,So the probability of sort of creating one of those race conditions is difficult
Dialogue: 0,1:11:10.04,1:11:12.16,English,,0,0,0,,Okay so that's it for today
Dialogue: 0,1:11:12.80,1:11:16.06,English,,0,0,0,,Tomorrow we'll look at thread based servers in more detail
Dialogue: 0,1:11:17.44,1:11:22.69,English,,0,0,0,,And how to write thread based systems efficiently and correctly
