[Script Info]
; Script generated by Aegisub r8942
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: TV.601
PlayResX: 1280
PlayResY: 720

[Aegisub Project Garbage]
Last Style Storage: Default
Audio File: ../../../../Desktop/csapp/Lecture 12  Cache Memories.mp4
Video File: ../../../../Desktop/csapp/Lecture 12  Cache Memories.mp4
Video AR Mode: 4
Video AR Value: 1.777778
Video Zoom Percent: 1.125000
Scroll Position: 844
Active Line: 849
Video Position: 141193

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: English,Source Han Sans CN,30,&H00FFFFFF,&H00412A2C,&H00412A2C,&H00412A2C,0,0,0,0,100,100,0,0,1,2.2,1,2,10,10,10,1
Style: Chinese,Source Han Sans CN,34,&H00FBFD00,&H00FFFFFF,&H00362A28,&H00FFFFFF,0,0,0,0,100,100,0,0,1,2,0.2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.00,0:00:04.16,English,,0,0,0,,Good afternoon everyone welcome to 213 it's good to see you
Dialogue: 0,0:00:06.14,0:00:12.46,English,,0,0,0,,Just a reminder that your attack lab is due tonight at 11:59 p.m.
Dialogue: 0,0:00:13.22,0:00:16.18,English,,0,0,0,,You have one grace day for this lab
Dialogue: 0,0:00:17.00,0:00:19.82,English,,0,0,0,,And cache lab we'll go out it right about the same time
Dialogue: 0,0:00:21.36,0:00:25.68,English,,0,0,0,,Now it's going to be a little tight for cache labs it'll be due next Thursday
Dialogue: 0,0:00:27.04,0:00:29.84,English,,0,0,0,,So you might want to you might want to get started on that soon
Dialogue: 0,0:00:32.92,0:00:36.90,English,,0,0,0,,Last lecture we learned about the memory hierarchy and the idea of caching
Dialogue: 0,0:00:37.54,0:00:41.24,English,,0,0,0,,Today we're going to look at a very important kind of cache
Dialogue: 0,0:00:43.28,0:00:46.22,English,,0,0,0,,Which are called cache memories
Dialogue: 0,0:00:47.40,0:00:49.96,English,,0,0,0,,And they're very important to you as a programmer
Dialogue: 0,0:00:49.96,0:00:53.40,English,,0,0,0,,Because they can have such a big impact on the performance of your program
Dialogue: 0,0:00:53.96,0:00:57.87,English,,0,0,0,,So if you know about these the existence of these cache memories and you know how they work
Dialogue: 0,0:00:59.30,0:01:02.20,English,,0,0,0,,As a programmer you'll be able to take advantage of that in your programs
Dialogue: 0,0:01:08.42,0:01:15.58,English,,0,0,0,,So last time we looked at the memory hierarchy is a collection of storage devices
Dialogue: 0,0:01:16.30,0:01:31.02,English,,0,0,0,,With smaller,costlier and faster devices at the top and slower cheaper and much larger devices at the at the bottom
Dialogue: 0,0:01:32.04,0:01:34.80,English,,0,0,0,,And then at each level in this hierarchy
Dialogue: 0,0:01:35.48,0:01:42.76,English,,0,0,0,,The device at level k serves as a cache holds a subset of the blocks of
Dialogue: 0,0:01:43.28,0:01:47.92,English,,0,0,0,,That are contained in the device at the lower level at level k+1
Dialogue: 0,0:01:52.06,0:01:55.94,English,,0,0,0,,Now recall the general idea of caching so we have a memory
Dialogue: 0,0:01:57.10,0:02:01.70,English,,0,0,0,,It's an array of bytes and we break it up arbitrarily into a collection of blocks
Dialogue: 0,0:02:03.52,0:02:08.30,English,,0,0,0,,And these this memory is larger slower and cheaper
Dialogue: 0,0:02:08.48,0:02:14.98,English,,0,0,0,,And so it's,and it's much larger than than a cache which is smaller faster and more expensive
Dialogue: 0,0:02:15.76,0:02:20.82,English,,0,0,0,,And which holds a subset of the blocks that are contained in the main memory
Dialogue: 0,0:02:22.66,0:02:27.96,English,,0,0,0,,And then blocks are copied back and forth between the cache in the memory in these block size transfer units
Dialogue: 0,0:02:29.00,0:02:35.62,English,,0,0,0,,So for example if our program requests a word that's in contained in block number 4
Dialogue: 0,0:02:38.16,0:02:43.30,English,,0,0,0,,It asks the cache to return the word that's contained in block 4
Dialogue: 0,0:02:43.90,0:02:48.16,English,,0,0,0,,The cache looks and it's at the blocks that it's the subset of the blocks
Dialogue: 0,0:02:48.16,0:02:51.34,English,,0,0,0,,That it's stored discovers that block 4 is not there
Dialogue: 0,0:02:52.04,0:02:54.44,English,,0,0,0,,So it asks the main memory to send it block 4
Dialogue: 0,0:02:55.00,0:02:59.40,English,,0,0,0,,Which it does and when that block arrives at the cache
Dialogue: 0,0:02:59.90,0:03:04.30,English,,0,0,0,,The cache stores it but potentially overwriting some existing block
Dialogue: 0,0:03:05.04,0:03:08.48,English,,0,0,0,,Similarly if our program asks for a data word
Dialogue: 0,0:03:09.08,0:03:10.74,English,,0,0,0,,If that's contained within block 10
Dialogue: 0,0:03:11.42,0:03:14.24,English,,0,0,0,,The cache looks sees that it doesn't have that block
Dialogue: 0,0:03:14.58,0:03:19.04,English,,0,0,0,,So it requests that block for memory which copies it into the cache
Dialogue: 0,0:03:19.77,0:03:22.58,English,,0,0,0,,Which overwrites an existing block
Dialogue: 0,0:03:23.40,0:03:28.86,English,,0,0,0,,Now subsequently if our program asks for a request
Dialogue: 0,0:03:29.04,0:03:32.56,English,,0,0,0,,If our program references a word that's contained in block 10
Dialogue: 0,0:03:32.62,0:03:38.98,English,,0,0,0,,For example then the cache then we have a hit and the cache can return that block immediately
Dialogue: 0,0:03:39.34,0:03:45.94,English,,0,0,0,,Without going through the expensive operation of contacting memory and fetching that block from memory
Dialogue: 0,0:03:50.22,0:03:54.95,English,,0,0,0,,Now there's a very important class of caches these so called cache memories
Dialogue: 0,0:03:55.80,0:03:59.12,English,,0,0,0,,Which are contained in the CPU chip itself
Dialogue: 0,0:04:00.02,0:04:01.98,English,,0,0,0,,And are managed completely by hardware
Dialogue: 0,0:04:02.56,0:04:05.88,English,,0,0,0,,And they're implemented using fast SRAM memories
Dialogue: 0,0:04:07.56,0:04:11.88,English,,0,0,0,,And the idea for this cache which write next to the register file
Dialogue: 0,0:04:13.50,0:04:18.50,English,,0,0,0,,Is to hold frequently access blocks or blocks from main memory that are accessed frequently
Dialogue: 0,0:04:19.06,0:04:22.40,English,,0,0,0,,Okay so hopefully because of the principle of locality
Dialogue: 0,0:04:23.20,0:04:31.02,English,,0,0,0,,Most of our requests for data will actually be served out of this cache memory and a few cycles
Dialogue: 0,0:04:31.48,0:04:35.32,English,,0,0,0,,Rather than from this slow main memory
Dialogue: 0,0:04:40.40,0:04:43.04,English,,0,0,0,,Now cache memories are managed completely in hardware
Dialogue: 0,0:04:44.00,0:04:49.32,English,,0,0,0,,So this means that the heart there's ,here has to be hardware logic that knows
Dialogue: 0,0:04:49.80,0:04:55.62,English,,0,0,0,,How to look for blocks in the cache and determine whether or not a particular block is contained there
Dialogue: 0,0:04:55.62,0:05:00.96,English,,0,0,0,,So cache memories are have to be organized in a very kind of strict simple way
Dialogue: 0,0:05:00.96,0:05:03.80,English,,0,0,0,,So that the logic the lookup logic can be pretty simple
Dialogue: 0,0:05:04.78,0:05:10.22,English,,0,0,0,,So this is very all cache memories are organized in the following way
Dialogue: 0,0:05:11.00,0:05:17.76,English,,0,0,0,,You can think of the cache as an array of S=2^s sets
Dialogue: 0,0:05:20.35,0:05:25.12,English,,0,0,0,,Ok each set consists of E=2^e lines
Dialogue: 0,0:05:28.60,0:05:37.48,English,,0,0,0,,Where each line consists of a block of B=2^b bytes of data
Dialogue: 0,0:05:39.48,0:05:48.68,English,,0,0,0,,A valid bit which indicates whether these data bits are actually that the bits and the data block are actually meaningful right
Dialogue: 0,0:05:49.30,0:05:53.78,English,,0,0,0,,It's possible they could just be random bits like you know when you first turn on the machine
Dialogue: 0,0:05:54.20,0:05:55.50,English,,0,0,0,,There's nothing in the cache
Dialogue: 0,0:05:56.12,0:05:59.58,English,,0,0,0,,But those bits will have values right that they'll lead to be ones or zeros
Dialogue: 0,0:05:59.58,0:06:01.36,English,,0,0,0,,But they won't actually correspond to data
Dialogue: 0,0:06:01.94,0:06:08.28,English,,0,0,0,,Okay so the valid bit tells us if these B bytes actually mean anything
Dialogue: 0,0:06:09.14,0:06:11.94,English,,0,0,0,,And then there's some additional bits called the tag bits
Dialogue: 0,0:06:12.52,0:06:16.53,English,,0,0,0,,Which will help us search for blocks which I'll show you in a minute
Dialogue: 0,0:06:17.44,0:06:19.26,English,,0,0,0,,Now when we talk about our cache size
Dialogue: 0,0:06:19.92,0:06:25.40,English,,0,0,0,,We're referring to the number of data bytes that are contained in blocks
Dialogue: 0,0:06:26.88,0:06:31.60,English,,0,0,0,,And so each cache has there's S sets
Dialogue: 0,0:06:32.84,0:06:36.32,English,,0,0,0,,There's E blocks per set
Dialogue: 0,0:06:36.98,0:06:38.46,English,,0,0,0,,And there's B bytes per block
Dialogue: 0,0:06:38.56,0:06:42.96,English,,0,0,0,,Ok so the total cache size C=S*E*B
Dialogue: 0,0:06:44.44,0:06:49.36,English,,0,0,0,,Ok now so there's a lot of terms to sort of keep straight and it's very easy to get
Dialogue: 0,0:06:50.42,0:06:55.54,English,,0,0,0,,To confuse the difference between lines and blocks and lines and sets
Dialogue: 0,0:06:56.14,0:07:01.42,English,,0,0,0,,Okay so we'll go through some examples and hopefully these will see these will start to make more sense
Dialogue: 0,0:07:03.06,0:07:11.00,English,,0,0,0,,Now let's look at in general how the cache hardware implements a read
Dialogue: 0,0:07:12.26,0:07:17.02,English,,0,0,0,,So when our program accesses, when our program executes an instruction
Dialogue: 0,0:07:17.88,0:07:20.80,English,,0,0,0,,That references some word in memory
Dialogue: 0,0:07:22.66,0:07:26.88,English,,0,0,0,,The CPU sends that address to the cache
Dialogue: 0,0:07:27.16,0:07:34.44,English,,0,0,0,,And asks and it asks the cache to return the word the word at that address
Dialogue: 0,0:07:37.00,0:07:39.24,English,,0,0,0,,So the cache takes that address
Dialogue: 0,0:07:41.38,0:07:46.02,English,,0,0,0,,This would be a 64-bit address in case of x86-64
Dialogue: 0,0:07:47.12,0:07:52.36,English,,0,0,0,,And it divides the address into a number of regions
Dialogue: 0,0:07:52.98,0:07:56.18,English,,0,0,0,,Which are determined by the organization of the cache
Dialogue: 0,0:07:56.96,0:08:01.24,English,,0,0,0,,Okay they're determined by those parameters S sets
Dialogue: 0,0:08:01.86,0:08:07.48,English,,0,0,0,,The s the number of sets a the number of lines per set and b the size of each data block
Dialogue: 0,0:08:09.04,0:08:15.82,English,,0,0,0,,So the low order bits there are b low order bits which determine the offset in the block
Dialogue: 0,0:08:16.18,0:08:17.68,English,,0,0,0,,That that word starts at
Dialogue: 0,0:08:20.36,0:08:25.88,English,,0,0,0,,Okay the next s bits are treated as an unsigned integer
Dialogue: 0,0:08:26.42,0:08:30.20,English,,0,0,0,,Which serves as an index into the array of sets
Dialogue: 0,0:08:31.02,0:08:34.74,English,,0,0,0,,Okay remember we just think of these as think of this cache as an array of set
Dialogue: 0,0:08:35.78,0:08:41.30,English,,0,0,0,,The set index bits provide the index into this array of sets
Dialogue: 0,0:08:42.12,0:08:43.74,English,,0,0,0,,And then all of the remaining bits
Dialogue: 0,0:08:46.06,0:08:51.14,English,,0,0,0,,All of the remaining t bits constitute what we call tag
Dialogue: 0,0:08:51.56,0:08:53.22,English,,0,0,0,,Which will help us when we do our search
Dialogue: 0,0:08:54.40,0:08:56.68,English,,0,0,0,,So the cache logic takes this address
Dialogue: 0,0:08:58.42,0:09:01.14,English,,0,0,0,,And it first extracts the the set index
Dialogue: 0,0:09:01.42,0:09:06.66,English,,0,0,0,,And uses that to,as an index into this array to identify the set that
Dialogue: 0,0:09:07.20,0:09:11.32,English,,0,0,0,,If this block is in the set,I'm sorry,if the data word
Dialogue: 0,0:09:12.20,0:09:17.78,English,,0,0,0,,If the block that contains the data word at this address is in the cache
Dialogue: 0,0:09:18.54,0:09:23.54,English,,0,0,0,,It's going to be in the set denoted by the the set index
Dialogue: 0,0:09:27.16,0:09:30.52,English,,0,0,0,,So first it identifies which index to look in
Dialogue: 0,0:09:36.28,0:09:41.80,English,,0,0,0,,And then it checks the tag,it checks all of the lines in that set
Dialogue: 0,0:09:43.30,0:09:46.66,English,,0,0,0,,To see if there's any of those lines have a matching tag
Dialogue: 0,0:09:46.88,0:09:50.88,English,,0,0,0,,That a tag that matches the the t the tag bits and the address
Dialogue: 0,0:09:53.58,0:09:56.52,English,,0,0,0,,And it checks to see if the valid bit is turned on
Dialogue: 0,0:09:56.52,0:09:59.70,English,,0,0,0,,So if those two conditions holds if there's a line anywhere in the set
Dialogue: 0,0:10:00.28,0:10:05.30,English,,0,0,0,,As of where the valid bit is one and there's a matching tag
Dialogue: 0,0:10:06.06,0:10:10.84,English,,0,0,0,,Then we have a hit okay then the block that we're looking for is contained in this set
Dialogue: 0,0:10:15.58,0:10:20.42,English,,0,0,0,,Okay if we...once we determine that with that we've identified the block
Dialogue: 0,0:10:20.72,0:10:25.48,English,,0,0,0,,Then the cache uses that the low-order b bits to determine where that
Dialogue: 0,0:10:26.40,0:10:30.28,English,,0,0,0,,Where the data we're interested in begins okay within that block
Dialogue: 0,0:10:32.38,0:10:37.24,English,,0,0,0,,All right let's look at a more specific example for a the simplest kind of cache
Dialogue: 0,0:10:38.00,0:10:42.00,English,,0,0,0,,Which is when E=1 when there's only one line per set
Dialogue: 0,0:10:43.68,0:10:46.12,English,,0,0,0,,Okay so E=1 one line per set
Dialogue: 0,0:10:46.78,0:10:49.04,English,,0,0,0,,This kind of cache is called a direct mapped cache
Dialogue: 0,0:10:52.26,0:10:55.98,English,,0,0,0,,So here we have S sets each set consists of a single line
Dialogue: 0,0:10:56.64,0:11:00.74,English,,0,0,0,,And now suppose our program references the data item
Dialogue: 0,0:11:01.02,0:11:06.52,English,,0,0,0,,And a particular address the CPU sense that address to the cache
Dialogue: 0,0:11:07.20,0:11:11.68,English,,0,0,0,,The cache takes that address breaks it up into these into these three fields
Dialogue: 0,0:11:12.76,0:11:17.92,English,,0,0,0,,For this particular address the block offset is four
Dialogue: 0,0:11:19.46,0:11:26.30,English,,0,0,0,,And the set index is one and then there's some tag bits which we'll just denote with is a color pink
Dialogue: 0,0:11:28.04,0:11:33.02,English,,0,0,0,,So the cache extracts the set index which is one
Dialogue: 0,0:11:33.16,0:11:36.46,English,,0,0,0,,And then it uses that as the index into the set
Dialogue: 0,0:11:42.00,0:11:44.94,English,,0,0,0,,And then it just ignores all the other the sets
Dialogue: 0,0:11:45.62,0:11:50.65,English,,0,0,0,,If the block we're looking for is...is in the cache it's going to be in this inset number one
Dialogue: 0,0:11:51.50,0:11:54.84,English,,0,0,0,,Then it does the comparison of the tag bits and the valid bits
Dialogue: 0,0:11:55.42,0:11:58.64,English,,0,0,0,,And assume that they assume that valid bits on and that it matches
Dialogue: 0,0:12:00.16,0:12:02.54,English,,0,0,0,,Then it looks at the block offset which is four
Dialogue: 0,0:12:03.94,0:12:11.76,English,,0,0,0,,And which tells it that the four bit in suppose that that's what the instruction was referencing
Dialogue: 0,0:12:12.28,0:12:14.82,English,,0,0,0,,The four byte in begins that offset for
Dialogue: 0,0:12:15.28,0:12:22.38,English,,0,0,0,,So now the cache takes this int and it sends it back to the to the CPU which puts it in the register
Dialogue: 0,0:12:28.96,0:12:36.22,English,,0,0,0,,Okay if the tag doesn't match then the old-line, if the tag doesn't match then there's a miss
Dialogue: 0,0:12:38.16,0:12:43.64,English,,0,0,0,,And in that case the cache has to fetch the block the corresponding block from memory
Dialogue: 0,0:12:44.36,0:12:47.68,English,,0,0,0,,And then overwrite this block in the line
Dialogue: 0,0:12:48.98,0:12:55.92,English,,0,0,0,,And then it can serve,then it can fetch,it can get the word out of the block and send it back to the processor
Dialogue: 0,0:12:57.80,0:13:02.24,English,,0,0,0,,Okay now let me ask you a question just to see you kind of check to see you're following along with this
Dialogue: 0,0:13:03.22,0:13:04.20,English,,0,0,0,,So if there's a miss
Dialogue: 0,0:13:05.34,0:13:09.10,English,,0,0,0,,And the cache has two requests the block for memory
Dialogue: 0,0:13:09.90,0:13:14.34,English,,0,0,0,,Fetch it from memory and then overwrite the the block in the current line
Dialogue: 0,0:13:16.54,0:13:22.18,English,,0,0,0,,Does it also have to change the tag bits or do those stay the same?
Dialogue: 0,0:13:22.18,0:13:28.58,English,,0,0,0,,So does the do the tag bits that were in this line get overwritten with a different value
Dialogue: 0,0:13:30.04,0:13:39.42,English,,0,0,0,,Or is it the same? same? different? same? different?
Dialogue: 0,0:13:43.70,0:13:44.98,English,,0,0,0,,Now why would it be different ?
Dialogue: 0,0:13:48.46,0:13:59.24,English,,0,0,0,,-we haven't changed yes-[student speaking]-I'm sorry oh-[student speaking]
Dialogue: 0,0:13:59.54,0:14:02.52,English,,0,0,0,,Oh it almost certainly has different data
Dialogue: 0,0:14:02.92,0:14:04.16,English,,0,0,0,,But just have a different address
Dialogue: 0,0:14:13.24,0:14:17.60,English,,0,0,0,,Exactly it missed because the tag
Dialogue: 0,0:14:18.18,0:14:19.82,English,,0,0,0,,It missed because the tag didn't match
Dialogue: 0,0:14:22.90,0:14:26.94,English,,0,0,0,,If the valid bit was false and the tag match then that would also be a miss
Dialogue: 0,0:14:28.74,0:14:34.74,English,,0,0,0,,Oh then you wouldn't...okay that's right,that's...okay good good good,okay,great
Dialogue: 0,0:14:39.70,0:14:48.10,English,,0,0,0,,All right let me do a little, let me do a really simple specific example of a how direct map cache works
Dialogue: 0,0:14:48.92,0:14:53.56,English,,0,0,0,,I want you to understand in real detail how this would work
Dialogue: 0,0:14:53.56,0:14:59.42,English,,0,0,0,,But I also want to make a point for about the weakness of direct mapped cache is and why
Dialogue: 0,0:15:00.08,0:15:02.62,English,,0,0,0,,Why you would want to have more than one line per set
Dialogue: 0,0:15:04.96,0:15:11.48,English,,0,0,0,,Okay this is a really simple we have our memory system consists of 16 bytes
Dialogue: 0,0:15:11.70,0:15:15.30,English,,0,0,0,,Ok so it's not a very useful system with 4 bit addresses
Dialogue: 0,0:15:16.94,0:15:20.42,English,,0,0,0,,And it's broken up into blocks of 2 bytes each
Dialogue: 0,0:15:21.74,0:15:26.34,English,,0,0,0,,Our cache consists of 4 sets with one block per set
Dialogue: 0,0:15:28.32,0:15:31.26,English,,0,0,0,,Now 4 bytes,4 bit addresses
Dialogue: 0,0:15:33.08,0:15:38.56,English,,0,0,0,,Because B=2, that's 2^1 we only need 1 block offset bit
Dialogue: 0,0:15:38.80,0:15:44.74,English,,0,0,0,,Right there's only 2 bytes in a block so the byte we're looking for is either at 0 or 1
Dialogue: 0,0:15:46.80,0:15:51.82,English,,0,0,0,,Okay because we have 4 sets we need set index bits
Dialogue: 0,0:15:52.52,0:15:57.20,English,,0,0,0,,And then the remaining bits are always tag bits in this case there's just one tag bit
Dialogue: 0,0:15:58.66,0:16:05.38,English,,0,0,0,,All right now let's suppose that our program executes instructions
Dialogue: 0,0:16:05.54,0:16:12.30,English,,0,0,0,,That reference the following memory address is 0,1,7,8 and 0
Dialogue: 0,0:16:14.10,0:16:18.34,English,,0,0,0,,And these references are reads that they're reading one byte per read
Dialogue: 0,0:16:19.04,0:16:20.92,English,,0,0,0,,Okay like I said this is a really simple system
Dialogue: 0,0:16:23.46,0:16:24.90,English,,0,0,0,,So let's look at what happens now
Dialogue: 0,0:16:24.90,0:16:33.82,English,,0,0,0,,We start tag,initially,cache is empty valid bits are all set to zero
Dialogue: 0,0:16:35.76,0:16:40.64,English,,0,0,0,,And now the cache receives the request for the byte that's at address 0
Dialogue: 0,0:16:43.02,0:16:46.90,English,,0,0,0,,So it extracts the set index bits which in this case are 00
Dialogue: 0,0:16:47.96,0:16:51.02,English,,0,0,0,,So these so it's going to look in set 0
Dialogue: 0,0:16:52.58,0:16:58.36,English,,0,0,0,,For and in this case since valid is 0 it's just a miss ok
Dialogue: 0,0:16:59.76,0:17:04.64,English,,0,0,0,,So it fetches that block from memory sticks the block
Dialogue: 0,0:17:06.04,0:17:11.60,English,,0,0,0,,So this memory this is the...this is using array notation for memory
Dialogue: 0,0:17:11.60,0:17:18.78,English,,0,0,0,,So this is like the the bytes that extend from offset 0 to offset 1 inclusive in memory
Dialogue: 0,0:17:20.84,0:17:25.42,English,,0,0,0,,The tag bit is 0 and the valid bit is 1
Dialogue: 0,0:17:26.14,0:17:30.76,English,,0,0,0,,Ok now the next address that comes by is for address 1
Dialogue: 0,0:17:32.86,0:17:40.14,English,,0,0,0,,Well that's a hit right...because we...that block the block that contains the byte at address 1 is already in the cache
Dialogue: 0,0:17:41.82,0:17:45.62,English,,0,0,0,,The tag and the tags match okay so we're good that's a hit
Dialogue: 0,0:17:47.16,0:17:48.64,English,,0,0,0,,You now we get address 7
Dialogue: 0,0:17:50.62,0:17:57.72,English,,0,0,0,,So the cache extracts the set index bits,which in this case are 11 or 4 or 3 rather
Dialogue: 0,0:17:59.80,0:18:07.42,English,,0,0,0,,Looks in set 3 there's no valid bit,so that's a miss and it loads the the data from memory
Dialogue: 0,0:18:08.22,0:18:11.14,English,,0,0,0,,That spans bytes 6~7
Dialogue: 0,0:18:13.04,0:18:16.68,English,,0,0,0,,In this case the the tag bit is 0
Dialogue: 0,0:18:17.74,0:18:21.20,English,,0,0,0,,Okay so we record that in our metadata
Dialogue: 0,0:18:22.84,0:18:24.98,English,,0,0,0,,Okay the next reference that comes by is 8
Dialogue: 0,0:18:27.06,0:18:31.02,English,,0,0,0,,Now 8 has a set index of 0,00
Dialogue: 0,0:18:31.84,0:18:36.70,English,,0,0,0,,But that's currently occupied by block zero one
Dialogue: 0,0:18:37.20,0:18:41.96,English,,0,0,0,,And we can tell that because address eight has a tag of one
Dialogue: 0,0:18:42.40,0:18:49.70,English,,0,0,0,,And the existing block,the block at the earlier address at address zero has a tag of zero so that's a miss so
Dialogue: 0,0:18:51.32,0:18:58.82,English,,0,0,0,,So now we have to go fetch the block containing byte number eight into memory
Dialogue: 0,0:18:58.82,0:19:03.88,English,,0,0,0,,So now we have bytes 8-9 and we in our new tag bit
Dialogue: 0,0:19:05.82,0:19:11.12,English,,0,0,0,,Okay now the next instruction is for byte 0
Dialogue: 0,0:19:12.62,0:19:18.30,English,,0,0,0,,And we just replaced,we had that it,we had that in our cache and we just replaced it
Dialogue: 0,0:19:19.76,0:19:22.32,English,,0,0,0,,So it's another miss so that's unfortunate
Dialogue: 0,0:19:23.24,0:19:30.02,English,,0,0,0,,And it's the only reason we missed it is because we've got just one line per set
Dialogue: 0,0:19:31.06,0:19:33.00,English,,0,0,0,,Right so we were forced to overwrite
Dialogue: 0,0:19:38.22,0:19:45.92,English,,0,0,0,,That that block containing bytes,the block zero one when we missed on block eight nine
Dialogue: 0,0:19:47.88,0:19:54.14,English,,0,0,0,,Okay and you see there's plenty of room in our cache we've still got,we've got two lines
Dialogue: 0,0:19:54.78,0:19:58.96,English,,0,0,0,,That we haven't even access right so we've our cache is plenty big
Dialogue: 0,0:19:59.62,0:20:04.54,English,,0,0,0,,But just because of the low associativity of our cache
Dialogue: 0,0:20:05.18,0:20:08.96,English,,0,0,0,,And the the sort of the pattern the access pattern that we were presented with
Dialogue: 0,0:20:09.36,0:20:12.88,English,,0,0,0,,We've got a miss that really was kind of unnecessary
Dialogue: 0,0:20:13.68,0:20:15.06,English,,0,0,0,,So oh yeah sorry
Dialogue: 0,0:20:15.06,0:20:29.06,English,,0,0,0,,[student speaking]
Dialogue: 0,0:20:29.10,0:20:34.20,English,,0,0,0,,Six,so when we referenced a seven
Dialogue: 0,0:20:35.26,0:20:40.72,English,,0,0,0,,It's actually the it's at offset one in that block 6-7
Dialogue: 0,0:20:41.50,0:20:45.38,English,,0,0,0,,Okay since blocks are two bytes they'll always start on an even multiple
Dialogue: 0,0:20:51.58,0:20:52.80,English,,0,0,0,,Any other questions
Dialogue: 0,0:20:57.64,0:21:06.78,English,,0,0,0,,Okay so this sort of is the reason why you have caches have higher associativity,higher values of E
Dialogue: 0,0:21:08.24,0:21:17.96,English,,0,0,0,,So let's look at...and so for values of E greater...for values of E greater than greater than 1
Dialogue: 0,0:21:19.22,0:21:22.98,English,,0,0,0,,We refer to them as E way set associative caches
Dialogue: 0,0:21:24.34,0:21:27.92,English,,0,0,0,,So here E=2 so it's a 2-way it's 2-way associative
Dialogue: 0,0:21:30.44,0:21:36.00,English,,0,0,0,,Let's suppose we have a 2-way associative cache
Dialogue: 0,0:21:36.00,0:21:43.94,English,,0,0,0,,So here we have an array of sets and now each set contains two lines ok instead of one line
Dialogue: 0,0:21:46.28,0:21:51.18,English,,0,0,0,,And suppose we're presented with an address with the following form
Dialogue: 0,0:21:53.50,0:21:58.40,English,,0,0,0,,We're looking for the word that begins at an off set of four inside our block
Dialogue: 0,0:22:01.04,0:22:05.20,English,,0,0,0,,At within set number one
Dialogue: 0,0:22:08.04,0:22:11.00,English,,0,0,0,,Okay so the cache expects tracts that set index
Dialogue: 0,0:22:12.02,0:22:20.34,English,,0,0,0,,So this is set 0,this is set 1,this is set 2,throws away all the other sets
Dialogue: 0,0:22:21.50,0:22:33.58,English,,0,0,0,,And now in parallel it searches,it searches the tags,it searches for a matching tag in both of these lines
Dialogue: 0,0:22:35.18,0:22:40.22,English,,0,0,0,,And a valid bit so if we get a matching tag and a valid bit true
Dialogue: 0,0:22:40.74,0:22:45.06,English,,0,0,0,,Then we've got a hit
Dialogue: 0,0:22:47.38,0:22:49.38,English,,0,0,0,,Now that yes yes
Dialogue: 0,0:22:49.38,0:22:56.84,English,,0,0,0,,[student speaking]
Dialogue: 0,0:22:56.92,0:23:01.60,English,,0,0,0,,Oh it's a very good question,so there's hardware logic that does that compare
Dialogue: 0,0:23:02.60,0:23:09.82,English,,0,0,0,,And that's the reason that as the number of...as the associativity goes up that logic gets more and more expensive
Dialogue: 0,0:23:10.78,0:23:15.42,English,,0,0,0,,Okay it's like something...like you're kind of doing some kind of tree search
Dialogue: 0,0:23:17.32,0:23:19.46,English,,0,0,0,,And so that actually is the limit that's why..
Dialogue: 0,0:23:19.56,0:23:25.04,English,,0,0,0,,I mean because in general right that if you take this to the limit there's just one set
Dialogue: 0,0:23:26.36,0:23:32.24,English,,0,0,0,,With there's just...we call that a fully associative cache so there's just one set
Dialogue: 0,0:23:33.08,0:23:35.74,English,,0,0,0,,And now any block...a block can go anywhere
Dialogue: 0,0:23:36.16,0:23:38.44,English,,0,0,0,,Right there's no constraints now where you place a block
Dialogue: 0,0:23:39.66,0:23:44.74,English,,0,0,0,,But because of the complexity of that fully associative search
Dialogue: 0,0:23:45.10,0:23:53.22,English,,0,0,0,,Those are very rare in fact we do see we'll see fully associative caches but their software caches
Dialogue: 0,0:23:54.70,0:23:59.12,English,,0,0,0,,Okay so in software,so the complexity the hardware
Dialogue: 0,0:23:59.90,0:24:03.40,English,,0,0,0,,And sort of doesn't
Dialogue: 0,0:24:05.22,0:24:12.06,English,,0,0,0,,Doesn't it's not worth the complexity of the hardware for the penalty of having a lower associativity
Dialogue: 0,0:24:12.78,0:24:16.24,English,,0,0,0,,Okay but there are some systems later on when we study virtual memory
Dialogue: 0,0:24:17.20,0:24:23.74,English,,0,0,0,,In a virtual memory system the DRAM serves as a cache for data stored on the disk
Dialogue: 0,0:24:24.78,0:24:28.04,English,,0,0,0,,And as we saw last time the penalty for a miss
Dialogue: 0,0:24:28.88,0:24:32.22,English,,0,0,0,,If you have a cache on DRAM and you miss and you have to go to disk
Dialogue: 0,0:24:32.96,0:24:34.68,English,,0,0,0,,The penalty is huge for that
Dialogue: 0,0:24:35.14,0:24:42.74,English,,0,0,0,,And so because of that it's worth while having very complex search algorithms
Dialogue: 0,0:24:42.96,0:24:51.11,English,,0,0,0,,In particular in a virtual memory system that the DRAM is implements a fully associative cache where blocks from disk can go anywhere
Dialogue: 0,0:24:51.52,0:24:54.40,English,,0,0,0,,We'll get into that later when we look in virtual memory
Dialogue: 0,0:24:54.60,0:24:56.22,English,,0,0,0,,But you're right you'll see in real systems
Dialogue: 0,0:24:56.70,0:25:01.16,English,,0,0,0,,Nowadays that the number goes up right because feature sizes are going down and
Dialogue: 0,0:25:01.56,0:25:04.86,English,,0,0,0,,Designers can afford to implement more expensive hardware
Dialogue: 0,0:25:04.86,0:25:12.02,English,,0,0,0,,But the largest associativity are Intel systems that I know of is 16-way associative L3 caches
Dialogue: 0,0:25:12.60,0:25:14.14,English,,0,0,0,,And then the others are 8-ways associative
Dialogue: 0,0:25:14.14,0:25:17.14,English,,0,0,0,,So that's sort of the order of magnitude that's state of the art right now
Dialogue: 0,0:25:20.76,0:25:25.24,English,,0,0,0,,Okay so then once we've identified a match we use the set offset bits
Dialogue: 0,0:25:26.46,0:25:33.20,English,,0,0,0,,In this case we're accessing a short int, so four is the offset within the block of this
Dialogue: 0,0:25:33.90,0:25:37.48,English,,0,0,0,,The two byte short int which then we can return to the processor
Dialogue: 0,0:25:39.44,0:25:42.02,English,,0,0,0,,Alright so let's do that same simulation that we did before
Dialogue: 0,0:25:42.02,0:25:46.10,English,,0,0,0,,But this time with a 2-way associative cache
Dialogue: 0,0:25:46.78,0:25:49.02,English,,0,0,0,,Now memory system is the same
Dialogue: 0,0:25:50.42,0:25:52.90,English,,0,0,0,,But now instead of one set we have two sets
Dialogue: 0,0:25:54.50,0:25:58.86,English,,0,0,0,,And I mean I'm sorry instead of four sets we have two sets
Dialogue: 0,0:25:58.94,0:26:02.06,English,,0,0,0,,So the cache,this is the same sized cache
Dialogue: 0,0:26:02.14,0:26:07.12,English,,0,0,0,,But we're just going to organize it differently, instead of 1-way instead of a direct mapped cache
Dialogue: 0,0:26:08.36,0:26:12.00,English,,0,0,0,,With four lines containing four lines,one line per set
Dialogue: 0,0:26:12.48,0:26:18.50,English,,0,0,0,,We're going to implement a 2-way associative cache where we have two sets with two lines per set
Dialogue: 0,0:26:18.68,0:26:22.10,English,,0,0,0,,Okay so each case there's four total lines
Dialogue: 0,0:26:22.46,0:26:22.86,English,,0,0,0,,Question
Dialogue: 0,0:26:24.30,0:26:29.80,English,,0,0,0,,[student speaking]
Dialogue: 0,0:26:29.80,0:26:32.08,English,,0,0,0,,Oh so that that comes in with the request somehow
Dialogue: 0,0:26:32.62,0:26:34.74,English,,0,0,0,,And I actually don't know the details of that
Dialogue: 0,0:26:35.90,0:26:39.36,English,,0,0,0,,It may...I guess there...it could ask for just...
Dialogue: 0,0:26:40.42,0:26:48.84,English,,0,0,0,,there could just be a default sighs maybe it's always a 64-byte word and then the processor extracts that the current bits
Dialogue: 0,0:26:48.84,0:26:51.68,English,,0,0,0,,I actually don't know the details of that
Dialogue: 0,0:26:51.98,0:26:59.10,English,,0,0,0,,But it either comes in on the request or there's a standard size that the processor then parses out
Dialogue: 0,0:27:01.46,0:27:05.00,English,,0,0,0,,We'll just assume that the cache knows the what size to return yes
Dialogue: 0,0:27:05.22,0:27:08.80,English,,0,0,0,,[student speaking]
Dialogue: 0,0:27:08.82,0:27:12.06,English,,0,0,0,,How do you decide which block to replace?that's a really good question
Dialogue: 0,0:27:12.06,0:27:13.70,English,,0,0,0,,So there's a lot of different algorithms
Dialogue: 0,0:27:14.32,0:27:19.48,English,,0,0,0,,The most common algorithm or a common algorithm is least recently used
Dialogue: 0,0:27:20.26,0:27:26.32,English,,0,0,0,,So by locality you want to keep blocks in the cache that are being used a lot
Dialogue: 0,0:27:27.32,0:27:34.94,English,,0,0,0,,And so if a block isn't referenced for a long time by the principle of locality by sort of the inverse locality principle
Dialogue: 0,0:27:35.40,0:27:39.32,English,,0,0,0,,It's likely not to be addressed referenced in the near future
Dialogue: 0,0:27:40.00,0:27:42.92,English,,0,0,0,,So that's one algorithm
Dialogue: 0,0:27:42.96,0:27:48.60,English,,0,0,0,,That you just keep track of and I'm not showing there needs to be additional bits
Dialogue: 0,0:27:49.06,0:27:53.44,English,,0,0,0,,In the line to sort of keep like sort of virtual timestamps that
Dialogue: 0,0:27:54.28,0:27:56.92,English,,0,0,0,,But that's sort of the general way you do it
Dialogue: 0,0:27:56.92,0:28:04.78,English,,0,0,0,,Just try to keep the things that are the blocks being accessed the most frequently,most recently yes
Dialogue: 0,0:28:04.92,0:28:09.02,English,,0,0,0,,[student speaking]
Dialogue: 0,0:28:09.04,0:28:11.74,English,,0,0,0,,Okay the question is what determines the block size
Dialogue: 0,0:28:12.92,0:28:15.28,English,,0,0,0,,That's determined by the design of the memory system
Dialogue: 0,0:28:15.98,0:28:20.86,English,,0,0,0,,So that's a fixed parameter of the memory system
Dialogue: 0,0:28:21.38,0:28:26.50,English,,0,0,0,,So when the intel designers decided to put cache memories on their processors
Dialogue: 0,0:28:27.18,0:28:30.20,English,,0,0,0,,They decided that the block size would be 64 bytes
Dialogue: 0,0:28:33.30,0:28:37.28,English,,0,0,0,,Sorry
Dialogue: 0,0:28:39.28,0:28:43.68,English,,0,0,0,,So the block size comes the block size comes first
Dialogue: 0,0:28:44.76,0:28:48.46,English,,0,0,0,,Then you determine how big you want your cache to be
Dialogue: 0,0:28:49.66,0:28:52.78,English,,0,0,0,,Okay and you determine the associativity
Dialogue: 0,0:28:54.54,0:28:59.08,English,,0,0,0,,And then once you've determined the associativity and you know how big your cache is
Dialogue: 0,0:28:59.54,0:29:01.32,English,,0,0,0,,Then that determines the number of sets
Dialogue: 0,0:29:02.68,0:29:09.23,English,,0,0,0,,Okay so basically all of those the
Dialogue: 0,0:29:09.70,0:29:16.62,English,,0,0,0,,The the number of lines and the cat and the capacity
Dialogue: 0,0:29:16.72,0:29:21.70,English,,0,0,0,,The number of lines per set is sort of a fixed high-level parameter design parameter
Dialogue: 0,0:29:21.98,0:29:26.50,English,,0,0,0,,The size of the cache is a high-level design parameter
Dialogue: 0,0:29:26.84,0:29:29.88,English,,0,0,0,,And then the number of sets then is induced from that
Dialogue: 0,0:29:30.94,0:29:31.70,English,,0,0,0,,Okay yes
Dialogue: 0,0:29:32.32,0:29:40.74,English,,0,0,0,,[student speaking]
Dialogue: 0,0:29:41.06,0:29:45.38,English,,0,0,0,,Ah that's yeah how does...so that's the replacement policy
Dialogue: 0,0:29:46.28,0:29:51.74,English,,0,0,0,,So the question is how does it when there's multiple lines in a set how does it determine which to over overwrite
Dialogue: 0,0:29:52.32,0:29:55.88,English,,0,0,0,,And that was the previous question probably maybe I should have repeated it
Dialogue: 0,0:29:56.34,0:30:00.14,English,,0,0,0,,So you try to pick a line that was least recently used
Dialogue: 0,0:30:00.90,0:30:03.42,English,,0,0,0,,So lines that haven't been accessed
Dialogue: 0,0:30:04.36,0:30:06.98,English,,0,0,0,,Recently are good candidates for replacement because
Dialogue: 0,0:30:07.14,0:30:09.90,English,,0,0,0,,Because of the sort of inverse locality principle right that
Dialogue: 0,0:30:10.18,0:30:14.58,English,,0,0,0,,They haven't been inverse referenced recently chances are they won't be referenced
Dialogue: 0,0:30:15.16,0:30:19.06,English,,0,0,0,,Again it
Dialogue: 0,0:30:19.08,0:30:22.28,English,,0,0,0,,Oh yeah there's additional bits that I'm not showing here that you have to
Dialogue: 0,0:30:22.82,0:30:25.58,English,,0,0,0,,So when you replace a line in the set
Dialogue: 0,0:30:26.16,0:30:30.56,English,,0,0,0,,If that data is has changed then it has to be written back to memory
Dialogue: 0,0:30:30.58,0:30:31.94,English,,0,0,0,,And that's another bit I haven't shown
Dialogue: 0,0:30:31.94,0:30:32.20,English,,0,0,0,,Yes
Dialogue: 0,0:30:32.36,0:30:44.46,English,,0,0,0,,[student speaking]
Dialogue: 0,0:30:44.46,0:30:52.14,English,,0,0,0,,Ah so yeah,so this is a really this is really tricky parameter writing
Dialogue: 0,0:30:52.14,0:30:56.72,English,,0,0,0,,It's a high level system parameter that it goes on for years
Dialogue: 0,0:30:57.62,0:31:02.02,English,,0,0,0,,So the idea you want to have blocks in order to exploit spatial locality
Dialogue: 0,0:31:03.10,0:31:07.20,English,,0,0,0,,Right think about if you're going to go to the trouble of if you have a miss in cache
Dialogue: 0,0:31:07.68,0:31:11.94,English,,0,0,0,,And you're going to go to the trouble of going all the way to memory to get some data
Dialogue: 0,0:31:12.64,0:31:18.94,English,,0,0,0,,You want to amortize the cost of fetching that data by fetching more than one byte
Dialogue: 0,0:31:19.78,0:31:21.66,English,,0,0,0,,That's the motivation for blocks
Dialogue: 0,0:31:22.46,0:31:29.34,English,,0,0,0,,Because by the principle of locality in spatial locality in particular
Dialogue: 0,0:31:30.64,0:31:32.90,English,,0,0,0,,If you reference a word inside of a block
Dialogue: 0,0:31:32.90,0:31:36.60,English,,0,0,0,,Chances are you're going to reference a nearby word which will also be an epilogue
Dialogue: 0,0:31:37.52,0:31:42.98,English,,0,0,0,,Okay so blocks the whole purpose of blocks is to exploit spatial locality
Dialogue: 0,0:31:43.40,0:31:49.28,English,,0,0,0,,Now if you make your block too small then you don't amortize
Dialogue: 0,0:31:49.66,0:31:52.32,English,,0,0,0,,You don't get the same amortization right you maybe get one
Dialogue: 0,0:31:52.80,0:31:53.98,English,,0,0,0,,You bring the block in
Dialogue: 0,0:31:55.10,0:31:57.62,English,,0,0,0,,So there's a reference you get a miss you bring the block in
Dialogue: 0,0:31:58.12,0:32:01.60,English,,0,0,0,,There's another reference nearby you get a hit because the blocks in memory
Dialogue: 0,0:32:02.24,0:32:04.76,English,,0,0,0,,But then the next reference is in a different block
Dialogue: 0,0:32:05.32,0:32:06.90,English,,0,0,0,,Because your block sizes are too small
Dialogue: 0,0:32:07.22,0:32:11.68,English,,0,0,0,,Right so you kind of want to make blocks big as big as possible
Dialogue: 0,0:32:12.10,0:32:13.74,English,,0,0,0,,But without slowing the system down
Dialogue: 0,0:32:13.74,0:32:17.84,English,,0,0,0,,So if you made your block size too big it would just take too long to bring that block in
Dialogue: 0,0:32:19.06,0:32:26.10,English,,0,0,0,,Plus now your blocks,that your blocks are taking up bits in your cache memory
Dialogue: 0,0:32:26.48,0:32:28.12,English,,0,0,0,,So now there's no room for other blocks
Dialogue: 0,0:32:28.12,0:32:32.12,English,,0,0,0,,Right so it's a really tricky design problem right and
Dialogue: 0,0:32:32.60,0:32:37.54,English,,0,0,0,,If we're doing it taking an architecture class then we would sort of dive into the
Dialogue: 0,0:32:38.12,0:32:41.46,English,,0,0,0,,You know how how architects make those design decisions
Dialogue: 0,0:32:41.86,0:32:44.72,English,,0,0,0,,But in general that's what it's kind of a balancing act right
Dialogue: 0,0:32:46.34,0:32:48.06,English,,0,0,0,,Were there any other questions? yes
Dialogue: 0,0:32:48.06,0:32:56.02,English,,0,0,0,,[student speaking]
Dialogue: 0,0:32:56.18,0:32:58.72,English,,0,0,0,,Oh the question is every time there's a miss
Dialogue: 0,0:32:58.72,0:33:02.34,English,,0,0,0,,Do you have to select a victim line and override it
Dialogue: 0,0:33:03.00,0:33:07.32,English,,0,0,0,,Yeah I don't know of any caches that don't do that
Dialogue: 0,0:33:07.92,0:33:09.48,English,,0,0,0,,Now we'll see when we look at rights
Dialogue: 0,0:33:10.70,0:33:14.88,English,,0,0,0,,We'll see there's an option of whether we're only looking at reads right now
Dialogue: 0,0:33:15.16,0:33:17.02,English,,0,0,0,,But with rights that question does come up
Dialogue: 0,0:33:17.86,0:33:24.02,English,,0,0,0,,If you wait in a couple of slides we'll go over that
Dialogue: 0,0:33:24.78,0:33:25.80,English,,0,0,0,,Any other questions
Dialogue: 0,0:33:29.04,0:33:32.48,English,,0,0,0,,Okay so let's look at this two-way associative cache now
Dialogue: 0,0:33:33.94,0:33:36.78,English,,0,0,0,,There's one block offset bit
Dialogue: 0,0:33:37.30,0:33:40.18,English,,0,0,0,,We only have two sets so that we only need one set index
Dialogue: 0,0:33:41.04,0:33:42.96,English,,0,0,0,,And then the remaining two bits are tagged
Dialogue: 0,0:33:44.02,0:33:52.16,English,,0,0,0,,So let's go through our trace so address zero has a set is in set zero right here that's a miss
Dialogue: 0,0:33:53.06,0:33:54.54,English,,0,0,0,, so we load that into memory
Dialogue: 0,0:33:59.72,0:34:01.50,English,,0,0,0,,With the reference to address one
Dialogue: 0,0:34:02.24,0:34:08.10,English,,0,0,0,,That's in set zero and that's a hit because that byte is in a block
Dialogue: 0,0:34:09.08,0:34:13.04,English,,0,0,0,,The reference to seven is a miss that's in set one so we load that
Dialogue: 0,0:34:13.56,0:34:17.36,English,,0,0,0,,And we were just picking randomly pick one of these two over right
Dialogue: 0,0:34:18.54,0:34:20.80,English,,0,0,0,,Because the cache is empty
Dialogue: 0,0:34:21.34,0:34:25.72,English,,0,0,0,,The next reference is to address number eight which is in set zero
Dialogue: 0,0:34:26.20,0:34:31.80,English,,0,0,0,,Now here's the difference between the direct mapped cache and this 2-way set associative cache
Dialogue: 0,0:34:32.86,0:34:35.66,English,,0,0,0,,When we reference address 8
Dialogue: 0,0:34:36.90,0:34:42.26,English,,0,0,0,,That block has to the corresponding block has to go into set 0
Dialogue: 0,0:34:42.98,0:34:44.74,English,,0,0,0,,Because of this zero set index bit
Dialogue: 0,0:34:45.46,0:34:49.38,English,,0,0,0,,But we've got room now because we are set to have room for two lines instead of one
Dialogue: 0,0:34:50.52,0:34:55.18,English,,0,0,0,,So when we load that in well if we have an available empty slot
Dialogue: 0,0:34:55.18,0:35:01.02,English,,0,0,0,,We'll put it there we won't overwrite anything right so if possible always try to overwrite empty lines
Dialogue: 0,0:35:02.64,0:35:07.86,English,,0,0,0,,So now we've got in this set...we've got block 0-1 and block 8-9
Dialogue: 0,0:35:08.38,0:35:12.46,English,,0,0,0,,So when we get our reference to address zero
Dialogue: 0,0:35:13.04,0:35:16.94,English,,0,0,0,,Whereas before with the when we had a conflict miss in the direct map cache
Dialogue: 0,0:35:16.94,0:35:19.94,English,,0,0,0,,Now we can satisfy that that request
Dialogue: 0,0:35:20.60,0:35:25.28,English,,0,0,0,,It hits in memory and the cache can satisfy it from the cache instead of going to memory
Dialogue: 0,0:35:26.78,0:35:30.40,English,,0,0,0,,Okay so that makes sense
Dialogue: 0,0:35:35.38,0:35:36.68,English,,0,0,0,,Okay now what about rights?
Dialogue: 0,0:35:39.20,0:35:41.38,English,,0,0,0,,So there's multiple copies of the data
Dialogue: 0,0:35:42.08,0:35:49.96,English,,0,0,0,,Right we're sub setting as we move up the hierarchy we're creating subsets of the data in the caches
Dialogue: 0,0:35:50.88,0:36:00.62,English,,0,0,0,,So what we do a write to a word within a block that's currently in the cache
Dialogue: 0,0:36:02.86,0:36:04.82,English,,0,0,0,,Okay we have two options
Dialogue: 0,0:36:05.46,0:36:09.02,English,,0,0,0,,We can write that block immediately to memory right
Dialogue: 0,0:36:09.68,0:36:13.26,English,,0,0,0,,We're got a block that's like this big and we're updating a little chunk of it
Dialogue: 0,0:36:15.10,0:36:19.24,English,,0,0,0,,So we can either do the update and then flush it to memory immediately
Dialogue: 0,0:36:19.28,0:36:25.02,English,,0,0,0,,So that memory always mirrors the contents of memory always mirror the contents of the cache
Dialogue: 0,0:36:26.42,0:36:29.60,English,,0,0,0,,Okay but that's expensive right
Dialogue: 0,0:36:29.62,0:36:32.44,English,,0,0,0,,I mean you know memory accesses are expensive
Dialogue: 0,0:36:33.68,0:36:36.96,English,,0,0,0,,The other so the other option is what what's called write back
Dialogue: 0,0:36:37.90,0:36:40.56,English,,0,0,0,,So in this case when we write to a block in the cache
Dialogue: 0,0:36:41.66,0:36:44.80,English,,0,0,0,,We don't flush it to memory
Dialogue: 0,0:36:45.14,0:36:50.94,English,,0,0,0,,Until we elect that particular line as a victim that's going to be overwritten
Dialogue: 0,0:36:52.38,0:36:55.90,English,,0,0,0,,And only then when we're just we sort of defer
Dialogue: 0,0:36:55.92,0:36:58.56,English,,0,0,0,,The writing to memory until the last possible minute
Dialogue: 0,0:36:59.22,0:37:04.71,English,,0,0,0,,We defer it until just before the cache would overwrite that  that data block
Dialogue: 0,0:37:05.36,0:37:06.80,English,,0,0,0,,Okay so that's called right back
Dialogue: 0,0:37:07.20,0:37:12.90,English,,0,0,0,,And for right back you need to have some an extra bit in the line that indicates whether that blocks been written to
Dialogue: 0,0:37:13.60,0:37:18.88,English,,0,0,0,,So the algorithm is when the cache identifies a particular line to overwrite
Dialogue: 0,0:37:19.62,0:37:25.50,English,,0,0,0,,It checks the dirty bit on that line if it's set then it writes that data to back to disk
Dialogue: 0,0:37:28.18,0:37:33.26,English,,0,0,0,,Okay if the data hasn't...if that block hasn't been written there's no point there's no need to write it back
Dialogue: 0,0:37:33.26,0:37:38.14,English,,0,0,0,,Because it has the same value as the copy of the block on disk
Dialogue: 0,0:37:41.20,0:37:45.74,English,,0,0,0,,Okay now so what about...so that's a right here now what happens if we have a right miss
Dialogue: 0,0:37:46.46,0:37:48.98,English,,0,0,0,,So we're doing a write to memory
Dialogue: 0,0:37:50.60,0:37:55.88,English,,0,0,0,,And the word that we're writing is not contained in any block that's in our cache
Dialogue: 0,0:37:57.90,0:38:02.58,English,,0,0,0,,So we have two options we can do what's called write allocate so we can treat it if there's a miss
Dialogue: 0,0:38:03.24,0:38:07.32,English,,0,0,0,,We can do sort of the symmetric thing that we did with a hit which was create a new
Dialogue: 0,0:38:08.08,0:38:11.26,English,,0,0,0,,A newline possibly overwriting an existing line
Dialogue: 0,0:38:11.96,0:38:18.18,English,,0,0,0,,And then write in so we could create that cache enter that cache line
Dialogue: 0,0:38:18.48,0:38:21.48,English,,0,0,0,,Fetch it from memory and then do the right
Dialogue: 0,0:38:23.18,0:38:25.34,English,,0,0,0,,Okay so this is sort of symmetric to reads right then
Dialogue: 0,0:38:25.80,0:38:32.10,English,,0,0,0,,So every right if it misses when the write finishes the that block will be in the cache
Dialogue: 0,0:38:33.10,0:38:35.56,English,,0,0,0,,And if we do a subsequent read we get a hit
Dialogue: 0,0:38:36.04,0:38:38.18,English,,0,0,0,,Okay so that's the reason you might want to do that
Dialogue: 0,0:38:40.56,0:38:42.92,English,,0,0,0,,The the other option is just to
Dialogue: 0,0:38:43.44,0:38:50.18,English,,0,0,0,,Don't allocate an entry in the cache don't allocate a new line just write write the data directly to memory
Dialogue: 0,0:38:52.48,0:38:56.00,English,,0,0,0,,You don't really need to understand the distinction between these two things
Dialogue: 0,0:38:56.44,0:38:58.66,English,,0,0,0,,Different caches use different policies
Dialogue: 0,0:38:59.27,0:39:06.18,English,,0,0,0,,For your own mental model a good model to use is just to assume write back write allocate
Dialogue: 0,0:39:07.08,0:39:10.22,English,,0,0,0,,So just assume that we won't copy the data to disk
Dialogue: 0,0:39:10.76,0:39:14.84,English,,0,0,0,,If there's a hit we won't write it back to disk until the last possible minute
Dialogue: 0,0:39:15.28,0:39:19.72,English,,0,0,0,,And every time there's a write miss what will create a new entry in the cache
Dialogue: 0,0:39:20.28,0:39:23.84,English,,0,0,0,,Okay so that's...that I think that's sort of the simplest model that
Dialogue: 0,0:39:24.32,0:39:30.84,English,,0,0,0,,And it's a reason it's a reasonable model that you can use regardless of the particular cache implementation
Dialogue: 0,0:39:33.78,0:39:38.04,English,,0,0,0,,Now in a real system so far we've only looked at we've only assumed that there's a single cache
Dialogue: 0,0:39:38.52,0:39:44.86,English,,0,0,0,,But in a in real systems there's multiple caches
Dialogue: 0,0:39:46.66,0:39:53.86,English,,0,0,0,,So modern core i7 has well architecture from intel
Dialogue: 0,0:39:56.24,0:39:58.48,English,,0,0,0,,Contains multiple processor cores
Dialogue: 0,0:39:59.40,0:40:02.32,English,,0,0,0,,So 4 is a typical number for like desktop systems
Dialogue: 0,0:40:02.80,0:40:06.12,English,,0,0,0,,8~12 is typical for server class systems
Dialogue: 0,0:40:06.74,0:40:12.58,English,,0,0,0,,These processor cores can each execute their own independent instruction stream in parallel
Dialogue: 0,0:40:14.54,0:40:21.34,English,,0,0,0,,And each processor core can contains general-purpose registers which that's level 0 in the cache
Dialogue: 0,0:40:22.46,0:40:26.30,English,,0,0,0,,And then two different kinds of L1 caches
Dialogue: 0,0:40:27.20,0:40:29.16,English,,0,0,0,,The data cache the L1 d-cache
Dialogue: 0,0:40:30.26,0:40:33.86,English,,0,0,0,,And the i-cache is the...which is the instruction cache
Dialogue: 0,0:40:35.28,0:40:40.16,English,,0,0,0,,And these are fairly small 32 k bytes they're eight-way associative
Dialogue: 0,0:40:40.74,0:40:44.14,English,,0,0,0,,And they can be accessed in a very small number of cycles
Dialogue: 0,0:40:45.60,0:40:51.40,English,,0,0,0,,The next level of the hierarchy is L2 cache
Dialogue: 0,0:40:52.12,0:40:58.58,English,,0,0,0,,Which is still fairly small 256 k bytes same associativity
Dialogue: 0,0:40:59.12,0:41:02.30,English,,0,0,0,,And it has a slightly longer access time
Dialogue: 0,0:41:03.14,0:41:08.68,English,,0,0,0,,And it's unified in the sense that the L2 cache contains both data and instructions
Dialogue: 0,0:41:11.42,0:41:14.10,English,,0,0,0,,Ok so that's all within a single core on the chip
Dialogue: 0,0:41:15.74,0:41:20.80,English,,0,0,0,,And then also on the chip but external to all the cores and shared by all the cores
Dialogue: 0,0:41:21.36,0:41:27.62,English,,0,0,0,,Is at L3 unified cache which is 8 megabytes and 16 way associative
Dialogue: 0,0:41:28.06,0:41:31.74,English,,0,0,0,,With an access time that's like 40 to 75 cycles
Dialogue: 0,0:41:33.84,0:41:43.06,English,,0,0,0,,So if there's a miss in L1,then the L1 sense or tries to sends a request to L2 to try to find the data in L2
Dialogue: 0,0:41:43.08,0:41:47.60,English,,0,0,0,,Since L2 is a little bigger maybe maybe the data hasn't been flushed out of L2 yet
Dialogue: 0,0:41:48.14,0:41:55.10,English,,0,0,0,,If L2 can't find it,it sends a request to L3 to see if they can find the data in L3
Dialogue: 0,0:41:55.88,0:41:59.62,English,,0,0,0,,If L3 can't find it then it gives up and it goes off chip to memory
Dialogue: 0,0:42:00.46,0:42:03.88,English,,0,0,0,,Yes question
Dialogue: 0,0:42:03.92,0:42:09.32,English,,0,0,0,,Yes name memory is this that's it's the DRAM built of DRAM chips
Dialogue: 0,0:42:09.94,0:42:13.96,English,,0,0,0,,It's separate,it's in a separate separate set of chips on the motherboard
Dialogue: 0,0:42:14.90,0:42:18.22,English,,0,0,0,,Connected by those that I/O bridge
Dialogue: 0,0:42:18.22,0:42:22.90,English,,0,0,0,,That we and the bus various buses then that we talked about last time
Dialogue: 0,0:42:29.14,0:42:34.02,English,,0,0,0,,And for all different for all of these different caches that block size is 64 bytes
Dialogue: 0,0:42:38.88,0:42:44.60,English,,0,0,0,,Now there's a number of different ways to I think about the performance of caches
Dialogue: 0,0:42:46.72,0:42:51.36,English,,0,0,0,,My most common way is using a metric called the miss rate
Dialogue: 0,0:42:52.08,0:42:54.96,English,,0,0,0,,So what this is the fraction of references that miss
Dialogue: 0,0:42:56.50,0:43:00.34,English,,0,0,0,,So we're very so I thought and it's 1- hit rate
Dialogue: 0,0:43:01.56,0:43:06.22,English,,0,0,0,,So typical for caches to work that miss rate has to be pretty low
Dialogue: 0,0:43:07.58,0:43:11.70,English,,0,0,0,,And fortunately because of locality these miss rates are low
Dialogue: 0,0:43:13.38,0:43:16.66,English,,0,0,0,,Another metric is the hit time
Dialogue: 0,0:43:16.66,0:43:21.34,English,,0,0,0,,So if we do have a hit in the cache how long does it actually take to...
Dialogue: 0,0:43:21.90,0:43:26.78,English,,0,0,0,,Sort of look up the you know do the lookup to determine that there was a hit and then return the value
Dialogue: 0,0:43:29.08,0:43:36.66,English,,0,0,0,,So for L1 and in an intel system this is 4 clock cycles,10 clock cycles for L2
Dialogue: 0,0:43:37.74,0:43:39.40,English,,0,0,0,,And then there's an additional cost if
Dialogue: 0,0:43:39.92,0:43:43.62,English,,0,0,0,,So you always have to pay the hit time right the hit time is the best you can do
Dialogue: 0,0:43:45.42,0:43:49.08,English,,0,0,0,,But if you have a miss then it's you pay the hit time
Dialogue: 0,0:43:50.66,0:43:54.80,English,,0,0,0,,Because you have to do the search and eventually you're going to have to return the word back to the requester
Dialogue: 0,0:43:55.32,0:43:57.34,English,,0,0,0,,But you then you have this additional cost
Dialogue: 0,0:43:57.34,0:44:01.24,English,,0,0,0,,Which you have to go which is going to the the memory ready to fetch the data
Dialogue: 0,0:44:02.16,0:44:05.76,English,,0,0,0,,Okay so that miss penalty,that's what called miss penalty
Dialogue: 0,0:44:07.40,0:44:10.16,English,,0,0,0,,Is on the order of hundreds of cycles for main memory
Dialogue: 0,0:44:11.00,0:44:14.35,English,,0,0,0,,But at other levels of the hierarchy it can be huge
Dialogue: 0,0:44:14.84,0:44:18.02,English,,0,0,0,,So the miss penalty if you have a cache in main memory
Dialogue: 0,0:44:18.64,0:44:23.12,English,,0,0,0,,That's caching blocks that are stored on disk, the miss penalty is enormous
Dialogue: 0,0:44:28.60,0:44:32.00,English,,0,0,0,,So it's kind of interesting if you think about it
Dialogue: 0,0:44:34.00,0:44:39.74,English,,0,0,0,,The performance of these systems is very sensitive to the miss rate much more sensitive than you would think
Dialogue: 0,0:44:41.32,0:44:49.66,English,,0,0,0,,And in fact 99% hit rate is twice as good as a 97% hit rate
Dialogue: 0,0:44:50.64,0:44:51.04,English,,0,0,0,,Yes
Dialogue: 0,0:44:51.60,0:45:06.74,English,,0,0,0,,[student speaking]
Dialogue: 0,0:45:06.86,0:45:13.50,English,,0,0,0,,Yeah they hit,so the question is does the hit time include the time to access the tag and yes
Dialogue: 0,0:45:13.80,0:45:17.38,English,,0,0,0,,So the hit time is the time it takes to just to search
Dialogue: 0,0:45:17.96,0:45:22.16,English,,0,0,0,,To determine if that item is in the cache and then return it
Dialogue: 0,0:45:22.16,0:45:39.20,English,,0,0,0,,[student speaking]
Dialogue: 0,0:45:39.22,0:45:48.20,English,,0,0,0,,Yeah so the yeah so the the miss the miss penalty is the time it takes for the cash to fetch the data from memory
Dialogue: 0,0:45:48.94,0:45:51.92,English,,0,0,0,,So that's all the latency you know going across the buses
Dialogue: 0,0:45:52.28,0:45:55.32,English,,0,0,0,,The time it takes the memory to respond to the requests
Dialogue: 0,0:45:55.83,0:45:59.78,English,,0,0,0,,The time it takes the data to flow back over the buses back to the the cache
Dialogue: 0,0:46:00.48,0:46:05.10,English,,0,0,0,,So the time for a miss is going to be the hit time plus the miss penalty that clear
Dialogue: 0,0:46:09.76,0:46:17.00,English,,0,0,0,,So I mean imagine suppose there's a hit time of one cycle and a miss penalty of 100 cycles that those are reasonable numbers
Dialogue: 0,0:46:18.48,0:46:22.76,English,,0,0,0,,So the the average access time if you have 97% hits
Dialogue: 0,0:46:24.22,0:46:29.48,English,,0,0,0,,It's the hit time plus the percentage of misses times the miss penalty
Dialogue: 0,0:46:30.06,0:46:32.66,English,,0,0,0,,So that's four cycles for the average access time
Dialogue: 0,0:46:33.36,0:46:36.56,English,,0,0,0,,But if we just increase the hit rate by two percent
Dialogue: 0,0:46:37.38,0:46:42.72,English,,0,0,0,,The average access time drops by 50% a factor of two
Dialogue: 0,0:46:47.26,0:46:51.72,English,,0,0,0,,All right so why is this stuff important why should you care about it
Dialogue: 0,0:46:53.42,0:46:57.78,English,,0,0,0,,So cash is that as we've seen are these these they're automatic they're all built in hardware
Dialogue: 0,0:46:58.44,0:47:04.48,English,,0,0,0,,There's no part of the sort of the visible instruction set that
Dialogue: 0,0:47:05.02,0:47:09.94,English,,0,0,0,,Lets you manipulate caches and your assembly machine code programs
Dialogue: 0,0:47:11.72,0:47:16.32,English,,0,0,0,,So that it all happens behind the scenes automatically in hardware
Dialogue: 0,0:47:17.42,0:47:21.26,English,,0,0,0,,But if you know how kit...if you know about the existence of caches
Dialogue: 0,0:47:21.26,0:47:24.18,English,,0,0,0,,And you have this general idea of how you can work how they work
Dialogue: 0,0:47:24.84,0:47:27.76,English,,0,0,0,,Then you can write code that's cache friendly
Dialogue: 0,0:47:27.76,0:47:36.68,English,,0,0,0,,In the sense that your code will have a higher higher miss rate than code that that isn't cache friendly
Dialogue: 0,0:47:37.96,0:47:44.76,English,,0,0,0,,So the idea is to...you want to focus on making the common case go fast
Dialogue: 0,0:47:44.82,0:47:48.90,English,,0,0,0,,Don't spend your time on code  that sort of code that doesn't get execute very much
Dialogue: 0,0:47:50.16,0:47:53.00,English,,0,0,0,,So look at the most commonly called functions
Dialogue: 0,0:47:54.32,0:47:57.12,English,,0,0,0,,And then within those functions look at the inner loops of those functions
Dialogue: 0,0:47:57.12,0:47:59.36,English,,0,0,0,,Because it's the inner loops that are executing the most
Dialogue: 0,0:48:00.30,0:48:04.78,English,,0,0,0,,Right so you can as a first approximation you can just ignore sort of stuff
Dialogue: 0,0:48:04.78,0:48:08.76,English,,0,0,0,,If you have nested loops you can ignore stuff that's going on in the outer loops
Dialogue: 0,0:48:08.78,0:48:11.02,English,,0,0,0,,And just focus on the code in the inner loop
Dialogue: 0,0:48:12.36,0:48:15.22,English,,0,0,0,,Now what you want to do is try to minimize the misses in the inner loop
Dialogue: 0,0:48:16.62,0:48:21.91,English,,0,0,0,,Okay so repeated references to a variable is variables are good
Dialogue: 0,0:48:22.34,0:48:24.20,English,,0,0,0,,Especially if those are local variables
Dialogue: 0,0:48:24.78,0:48:27.56,English,,0,0,0,,Right so remember if you declare a local variable and see
Dialogue: 0,0:48:28.52,0:48:30.90,English,,0,0,0,,The compiler can put that in a register
Dialogue: 0,0:48:32.30,0:48:34.30,English,,0,0,0,,Right if you're referencing global variables
Dialogue: 0,0:48:34.72,0:48:37.26,English,,0,0,0,,Maybe not,the compiler doesn't know what's going on
Dialogue: 0,0:48:37.72,0:48:44.30,English,,0,0,0,,So it can't put the reference to that variable to see in a register
Dialogue: 0,0:48:44.98,0:48:50.18,English,,0,0,0,,Okay so repeated references to local variables stored on the stack are good
Dialogue: 0,0:48:50.48,0:48:52.78,English,,0,0,0,,Because those will get turned into register accesses
Dialogue: 0,0:48:53.06,0:48:54.30,English,,0,0,0,,You'll never go to memory
Dialogue: 0,0:48:55.42,0:48:58.90,English,,0,0,0,,Okay also stride one accesses two arrays are good
Dialogue: 0,0:48:59.98,0:49:02.60,English,,0,0,0,,And they're good because of the existence of these blocks
Dialogue: 0,0:49:03.20,0:49:07.48,English,,0,0,0,,Right so the only way you'd know that stride one references our good is if you knew that
Dialogue: 0,0:49:07.90,0:49:10.20,English,,0,0,0,,Caches have the 64-byte blocks
Dialogue: 0,0:49:11.88,0:49:18.34,English,,0,0,0,,Okay so the and stride one reference will have half the miss rate as a stride to reference
Dialogue: 0,0:49:19.74,0:49:26.42,English,,0,0,0,,Because if you're doing stride one references the first reference to a word and a block will miss
Dialogue: 0,0:49:27.70,0:49:29.68,English,,0,0,0,,But then subsequent references will hit
Dialogue: 0,0:49:31.64,0:49:37.42,English,,0,0,0,,Right and you'll hit if you're doing a stride one reference you're going to hit every word in that block
Dialogue: 0,0:49:38.10,0:49:41.84,English,,0,0,0,,If your drive if you're doing stride two references you're only going to hit every other word
Dialogue: 0,0:49:42.78,0:49:46.92,English,,0,0,0,,Right so you'll only get,you'll get sort of half the so you'll missed at twice the rate
Dialogue: 0,0:49:50.98,0:49:51.92,English,,0,0,0,,So
Dialogue: 0,0:49:54.38,0:49:59.28,English,,0,0,0,,So basically the point I want to make to you is that our understanding of caches
Dialogue: 0,0:49:59.84,0:50:06.20,English,,0,0,0,,Allow us to sort of quantify this qualitative notion of locality that we developed the last time
Dialogue: 0,0:50:06.20,0:50:11.06,English,,0,0,0,,Right the last time we looked at we said if it's doing stride one references that's good if
Dialogue: 0,0:50:11.64,0:50:16.98,English,,0,0,0,,If we're doing if we're accessing the same variable over and over that's good
Dialogue: 0,0:50:17.62,0:50:21.74,English,,0,0,0,,But if we understand caches now we can quantify it in terms of miss rate
Dialogue: 0,0:50:26.42,0:50:28.82,English,,0,0,0,,All right so let's finish up the rest of the class
Dialogue: 0,0:50:28.82,0:50:32.50,English,,0,0,0,,We're going to look at the performance impact of caches on your code
Dialogue: 0,0:50:33.00,0:50:36.43,English,,0,0,0,,Okay and why you need to why you need to know about these things
Dialogue: 0,0:50:37.16,0:50:39.10,English,,0,0,0,,And that the impact that they can have
Dialogue: 0,0:50:41.52,0:50:44.58,English,,0,0,0,,So there's a very interesting function
Dialogue: 0,0:50:45.22,0:50:48.32,English,,0,0,0,,This actually plotted on the cover of your text book
Dialogue: 0,0:50:49.12,0:50:50.50,English,,0,0,0,,That we call the memory mountain
Dialogue: 0,0:50:51.10,0:50:59.14,English,,0,0,0,,I learned about this from a graduate student here at Carnegie Mellon back in the 90s who developed this notion made Tom Stricker
Dialogue: 0,0:51:00.34,0:51:07.66,English,,0,0,0,,And what it's a the memory mountain plots a measure called read through put or read bandwidth
Dialogue: 0,0:51:08.18,0:51:10.38,English,,0,0,0,,Which is the number of bytes read from memory
Dialogue: 0,0:51:10.66,0:51:16.08,English,,0,0,0,,So if you have a loop and you're scanning over a vector
Dialogue: 0,0:51:16.36,0:51:19.40,English,,0,0,0,,So you have a vector of say double words
Dialogue: 0,0:51:20.20,0:51:23.86,English,,0,0,0,,And you're reading those elements from a vector one after the other
Dialogue: 0,0:51:24.68,0:51:30.04,English,,0,0,0,,The read throughput is the number of megabytes per second that you can perform that task
Dialogue: 0,0:51:31.14,0:51:34.62,English,,0,0,0,,At and the memory mountain plots read throughput
Dialogue: 0,0:51:35.02,0:51:39.48,English,,0,0,0,,As a function of the temporal and spatial locality in that loop
Dialogue: 0,0:51:41.20,0:51:48.60,English,,0,0,0,,Okay so in a sense it's looking at a wide range of locality options or characteristics in a program
Dialogue: 0,0:51:48.80,0:51:55.68,English,,0,0,0,,And it's plotting the performance of that memory system on that across that range as a two-dimensional function
Dialogue: 0,0:51:56.08,0:51:58.23,English,,0,0,0,,So in some ways the memory mountain is a
Dialogue: 0,0:51:59.10,0:52:02.56,English,,0,0,0,,Kind of a fingerprint right every system has its own unique memory mountain
Dialogue: 0,0:52:03.26,0:52:06.04,English,,0,0,0,,That we can measure right by writing a simple program
Dialogue: 0,0:52:07.50,0:52:14.22,English,,0,0,0,,And so the idea here is that to construct the memory mountain
Dialogue: 0,0:52:15.08,0:52:16.78,English,,0,0,0,,We write a program called test
Dialogue: 0,0:52:35.50,0:52:36.60,English,,0,0,0,,Oh shoot
Dialogue: 0,0:52:51.54,0:52:53.04,English,,0,0,0,,For some reason it's not
Dialogue: 0,0:52:57.02,0:53:01.56,English,,0,0,0,,Okay right
Dialogue: 0,0:53:04.22,0:53:06.66,English,,0,0,0,,So when we build a memory mountain
Dialogue: 0,0:53:07.38,0:53:12.66,English,,0,0,0,,We're given a vector that consists of a collection of double words
Dialogue: 0,0:53:17.60,0:53:24.40,English,,0,0,0,,And then we write a loop that reads those words that read some number of words in this case
Dialogue: 0,0:53:25.54,0:53:42.84,English,,0,0,0,,Hmm
Dialogue: 0,0:53:42.92,0:53:43.86,English,,0,0,0,,There we go
Dialogue: 0,0:53:44.90,0:53:49.60,English,,0,0,0,,So it reads it reads elems number of elements right
Dialogue: 0,0:53:49.90,0:53:55.28,English,,0,0,0,,So we've got each of these double word elements with a stride of stride
Dialogue: 0,0:53:56.62,0:53:58.40,English,,0,0,0,,Okay so if we have a stride of one
Dialogue: 0,0:54:02.46,0:54:04.18,English,,0,0,0,,I know that was kind of redundant huh
Dialogue: 0,0:54:04.44,0:54:06.00,English,,0,0,0,,So if we have a stride of one
Dialogue: 0,0:54:09.18,0:54:12.86,English,,0,0,0,,Then we'll have our loop wills was sort of looped through
Dialogue: 0,0:54:13.62,0:54:17.62,English,,0,0,0,,And read these elements until we've read elems number of those elements
Dialogue: 0,0:54:18.54,0:54:23.14,English,,0,0,0,,Okay and then we'll do it again and then that warms up the cache
Dialogue: 0,0:54:23.88,0:54:27.66,English,,0,0,0,,Then we do it again and do exactly the same thing
Dialogue: 0,0:54:28.54,0:54:32.82,English,,0,0,0,,So if we're doing this with a stride of two then we would be reading
Dialogue: 0,0:54:33.76,0:54:41.80,English,,0,0,0,,We would read this word zero or elem 2 elem 4 and so on
Dialogue: 0,0:54:45.60,0:54:50.02,English,,0,0,0,,Okay so well then what all we're doing we're just for wide range of strides
Dialogue: 0,0:54:50.84,0:54:53.20,English,,0,0,0,,And a wide range of sizes
Dialogue: 0,0:54:53.78,0:55:00.12,English,,0,0,0,,We're scanning over this vector and just recording how long it takes to do that read
Dialogue: 0,0:55:00.56,0:55:04.03,English,,0,0,0,,And then convert we convert that into megabytes per second
Dialogue: 0,0:55:05.32,0:55:12.74,English,,0,0,0,,And in order to I just wanted to show you this is we don't need,we're not going to go into detail about this but
Dialogue: 0,0:55:13.09,0:55:17.86,English,,0,0,0,,This is actually how I generated the the cover on the book and
Dialogue: 0,0:55:18.10,0:55:23.72,English,,0,0,0,,In order to use to exploit the parallelism inside the intel processor
Dialogue: 0,0:55:23.86,0:55:27.30,English,,0,0,0,,Like you learned about last week there's a lot of parallel functional units
Dialogue: 0,0:55:27.88,0:55:33.68,English,,0,0,0,,In order to exploit those I did 4x4 loop unrolling
Dialogue: 0,0:55:34.20,0:55:36.96,English,,0,0,0,,So I'm actually doing sort of four scans in parallel
Dialogue: 0,0:55:38.66,0:55:41.00,English,,0,0,0,,But the general idea is just what I've showed you here
Dialogue: 0,0:55:41.78,0:55:47.34,English,,0,0,0,,And this this 4x4 loop unrolling is just an optimization
Dialogue: 0,0:55:47.92,0:55:51.74,English,,0,0,0,,But I wanted to show it to you because it actually it's the exact same principles
Dialogue: 0,0:55:51.74,0:55:57.24,English,,0,0,0,,You learned about last week professor Bryant talked about code optimization
Dialogue: 0,0:55:58.78,0:56:05.48,English,,0,0,0,,So what we do is,we call this test function with these various ranges of elems and stride
Dialogue: 0,0:56:06.22,0:56:09.96,English,,0,0,0,,And then we measure the performance and we get this beautiful picture
Dialogue: 0,0:56:10.38,0:56:14.44,English,,0,0,0,,This beautiful function ,to me it's beautiful I don't know does it look beautiful to you
Dialogue: 0,0:56:22.40,0:56:24.92,English,,0,0,0,,So on the z
Dialogue: 0,0:56:25.32,0:56:29.90,English,,0,0,0,,On the z axis is plotting read throughput in megabytes per second
Dialogue: 0,0:56:29.90,0:56:37.58,English,,0,0,0,,Ranging from 2000 megabytes per second up to 16,000 megabytes per second
Dialogue: 0,0:56:42.66,0:56:48.62,English,,0,0,0,,This this axis is measuring is stride
Dialogue: 0,0:56:49.28,0:56:53.00,English,,0,0,0,,So going from stride 1 up to stride 12
Dialogue: 0,0:56:55.08,0:57:03.86,English,,0,0,0,,And this axis is...so as we as we increase stride we're decreasing the spatial locality
Dialogue: 0,0:57:05.54,0:57:06.34,English,,0,0,0,,Alright
Dialogue: 0,0:57:09.68,0:57:18.00,English,,0,0,0,,And this axis is the size axis so we're going from I think 16k up to 128 megabytes
Dialogue: 0,0:57:18.72,0:57:23.16,English,,0,0,0,,So this is the number of elements we're going to read each pass through
Dialogue: 0,0:57:25.60,0:57:29.10,English,,0,0,0,,So as we as we increase the size
Dialogue: 0,0:57:30.38,0:57:36.08,English,,0,0,0,,We're sort of decreasing the impact of temporal locality because word
Dialogue: 0,0:57:36.82,0:57:41.80,English,,0,0,0,,As we increase the size there's fewer and fewer caches in our hierarchy can hold all that data
Dialogue: 0,0:57:44.62,0:57:49.94,English,,0,0,0,,And so this so we've got spatial locality decreasing in this direction
Dialogue: 0,0:57:50.44,0:57:53.06,English,,0,0,0,,And temporal locality decreasing in this direction
Dialogue: 0,0:57:54.76,0:57:58.46,English,,0,0,0,,So as a programmer what you want to do you want to be up here
Dialogue: 0,0:57:59.86,0:58:02.52,English,,0,0,0,,Right good spatial locality good temporal locality
Dialogue: 0,0:58:02.92,0:58:07.60,English,,0,0,0,,Because you can get like 14 gigabytes per second measure agreed throughput
Dialogue: 0,0:58:08.98,0:58:10.70,English,,0,0,0,,You don't want to be down here
Dialogue: 0,0:58:11.98,0:58:15.94,English,,0,0,0,,Which is only about 100 megabytes per second where you're reading out of memory
Dialogue: 0,0:58:16.18,0:58:20.42,English,,0,0,0,,Right so the difference between reading all of your data from memory
Dialogue: 0,0:58:21.42,0:58:28.80,English,,0,0,0,,And reading it from some part of the the caches is huge it's enormous
Dialogue: 0,0:58:29.60,0:58:32.52,English,,0,0,0,,Ok so because you're 213 students you'll be up here
Dialogue: 0,0:58:33.08,0:58:35.94,English,,0,0,0,,And all the students that didn't take 213 they'll be down here
Dialogue: 0,0:58:38.10,0:58:43.04,English,,0,0,0,,And I've actually had I've actually had people several people write back
Dialogue: 0,0:58:43.04,0:58:48.40,English,,0,0,0,,To tell me about their experiences you know in internships and jobs after they left CMU
Dialogue: 0,0:58:48.94,0:58:51.74,English,,0,0,0,,Where they were given some code that that was down here
Dialogue: 0,0:58:52.96,0:58:59.52,English,,0,0,0,,And they recognized the locality issues and they got it you know better up here or close at least better
Dialogue: 0,0:59:01.18,0:59:07.22,English,,0,0,0,,So this picture this so-called memory mountain has all kinds of interesting features
Dialogue: 0,0:59:08.10,0:59:11.40,English,,0,0,0,,First of all there's these what I call ridges of temporal locality
Dialogue: 0,0:59:12.02,0:59:15.86,English,,0,0,0,,Where these ridges see these ridge lines if you think of this is like a mountain
Dialogue: 0,0:59:15.86,0:59:19.62,English,,0,0,0,,You see this ridge line and you see this ridge line
Dialogue: 0,0:59:20.34,0:59:23.24,English,,0,0,0,,And here's another ridge line and then here's a here's another one
Dialogue: 0,0:59:23.56,0:59:25.70,English,,0,0,0,,These correspond to different levels in the hierarchy
Dialogue: 0,0:59:25.70,0:59:29.58,English,,0,0,0,,So this this top ridge line is where you're reading directly out of L1
Dialogue: 0,0:59:30.78,0:59:33.16,English,,0,0,0,,And it should be perfectly flat and
Dialogue: 0,0:59:33.90,0:59:38.80,English,,0,0,0,,It's so fast that we're getting like measurement jitter performance jitter right
Dialogue: 0,0:59:39.90,0:59:45.60,English,,0,0,0,,But it's and this little drop off here is a measurement artifact it it should it shouldn't be there
Dialogue: 0,0:59:45.60,0:59:49.22,English,,0,0,0,,It should be flat and go all the way to the wall back here
Dialogue: 0,0:59:52.50,0:59:58.48,English,,0,0,0,,And then here this ridge line is where we're accessing L2
Dialogue: 0,0:59:59.16,1:00:00.94,English,,0,0,0,,This is where we're accessing L3
Dialogue: 0,1:00:02.00,1:00:05.26,English,,0,0,0,,And here's what we're accessing mostly from memory
Dialogue: 0,1:00:06.38,1:00:08.58,English,,0,0,0,,So you have these ridges of temporal locality
Dialogue: 0,1:00:08.94,1:00:12.26,English,,0,0,0,,And then you have these slopes of decreasing spatial locality
Dialogue: 0,1:00:13.00,1:00:14.42,English,,0,0,0,,So you see the slope here
Dialogue: 0,1:00:15.52,1:00:20.42,English,,0,0,0,,As work so as we're moving from the top of the slope down to the bottom
Dialogue: 0,1:00:21.84,1:00:28.76,English,,0,0,0,,We're decreasing our spatial locality so we're getting less benefit for these blocks that we're bringing in
Dialogue: 0,1:00:29.27,1:00:33.32,English,,0,0,0,,So you can see the we're getting less benefit out of the cost
Dialogue: 0,1:00:33.68,1:00:36.64,English,,0,0,0,,That we went through of importing of fetching these blocks
Dialogue: 0,1:00:38.18,1:00:42.28,English,,0,0,0,,And once the stride hits the block size
Dialogue: 0,1:00:43.40,1:00:45.58,English,,0,0,0,,Now every reference is hitting a different block
Dialogue: 0,1:00:46.02,1:00:50.40,English,,0,0,0,,And so and then it flattens out then you get you're getting noes benefit from spatial locality
Dialogue: 0,1:00:53.68,1:00:59.50,English,,0,0,0,,And similarly here is where this this slope is where we're reading from L3
Dialogue: 0,1:01:00.44,1:01:09.80,English,,0,0,0,,And it flattens out always they always flatten out at the block size which is a stride these are double words right
Dialogue: 0,1:01:10.44,1:01:14.12,English,,0,0,0,,So it's stride of eight is 64 bytes
Dialogue: 0,1:01:14.12,1:01:16.66,English,,0,0,0,,So once you exceed a stride of eight then you're no longer
Dialogue: 0,1:01:17.18,1:01:20.14,English,,0,0,0,,You're missing every time in a different block
Dialogue: 0,1:01:23.48,1:01:26.18,English,,0,0,0,,There's this interesting this one puzzled me for a while
Dialogue: 0,1:01:26.94,1:01:32.88,English,,0,0,0,,You might be wondering like how come like over here is we increase the size
Dialogue: 0,1:01:33.52,1:01:39.86,English,,0,0,0,,We can sort of getting the...we're sort of as we increase the size
Dialogue: 0,1:01:39.86,1:01:45.60,English,,0,0,0,,We're doing most of our references out of caches that are lower in the cache hierarchy
Dialogue: 0,1:01:47.60,1:01:50.68,English,,0,0,0,,Okay but except when we're doing stride one references
Dialogue: 0,1:01:51.48,1:01:55.48,English,,0,0,0,,You can see all the way up to right at the end
Dialogue: 0,1:01:56.80,1:01:59.76,English,,0,0,0,,Right before it exceeds the size of L3
Dialogue: 0,1:02:01.24,1:02:01.96,English,,0,0,0,,It's flat
Dialogue: 0,1:02:05.74,1:02:07.78,English,,0,0,0,,And it's running at the L2 rate
Dialogue: 0,1:02:08.08,1:02:13.06,English,,0,0,0,,Alright so here's the L1 rate and then it drops off and then it's running at a constant L2 rate
Dialogue: 0,1:02:13.64,1:02:15.92,English,,0,0,0,,Until the data no longer fits in L3
Dialogue: 0,1:02:17.24,1:02:20.36,English,,0,0,0,,So I think what's going on here is that the the hardware
Dialogue: 0,1:02:21.28,1:02:28.72,English,,0,0,0,,The cache L2 cache hardware is recognizing or maybe it's an L1 but
Dialogue: 0,1:02:28.90,1:02:34.28,English,,0,0,0,,Some logic in the cache system is recognizing the stride one reference pattern
Dialogue: 0,1:02:35.44,1:02:37.26,English,,0,0,0,,Right because it sees all the addresses
Dialogue: 0,1:02:37.96,1:02:40.84,English,,0,0,0,,It's recognizing that stride one pattern
Dialogue: 0,1:02:41.58,1:02:46.36,English,,0,0,0,,And then it's aggressively prefetching from L3 into L2
Dialogue: 0,1:02:46.90,1:02:51.04,English,,0,0,0,,So that those so it's fetching ahead of time it's anticipating
Dialogue: 0,1:02:51.04,1:02:54.52,English,,0,0,0,,It's saying look I've gotten five stride one references in a row
Dialogue: 0,1:02:55.14,1:02:58.10,English,,0,0,0,,I'm going to go grab a whole bunch of blocks and load them all up
Dialogue: 0,1:02:58.10,1:03:02.66,English,,0,0,0,,Because by the principle of spatial locality those blocks
Dialogue: 0,1:03:03.32,1:03:05.84,English,,0,0,0,,Those blocks are going to be referenced in the near future
Dialogue: 0,1:03:07.10,1:03:09.94,English,,0,0,0,,So this was really neat and this only happened within the last couple years
Dialogue: 0,1:03:09.94,1:03:12.58,English,,0,0,0,,So the intel engineers are always hard at work
Dialogue: 0,1:03:13.14,1:03:20.70,English,,0,0,0,,And maybe by the time the time we do the next the next edition of the memory mountain
Dialogue: 0,1:03:22.12,1:03:26.78,English,,0,0,0,,Those systems will recognize stride 2 and you know other stride pattarns two
Dialogue: 0,1:03:27.72,1:03:31.04,English,,0,0,0,,But from this data it appears that it's only recognizing stride one
Dialogue: 0,1:03:36.16,1:03:44.42,English,,0,0,0,,Ok so you can real...you we can improve the spatial and temporal locality of our programs
Dialogue: 0,1:03:45.84,1:03:51.52,English,,0,0,0,,In several different ways that one way to improve the spatial locality is to rearrange loops
Dialogue: 0,1:03:52.64,1:03:55.08,English,,0,0,0,,And I'll use matrix multiplication as an example
Dialogue: 0,1:03:56.48,1:04:03.58,English,,0,0,0,,So here's a sort of a simple matrix multiplication in code
Dialogue: 0,1:04:03.58,1:04:08.14,English,,0,0,0,,Where we're multiplying a times b and adding it
Dialogue: 0,1:04:08.58,1:04:13.82,English,,0,0,0,,We're taking what's in of the c[i][j]
Dialogue: 0,1:04:15.18,1:04:24.72,English,,0,0,0,,And then to that we're adding the sum the inner product of row i of a and the row j column j of b
Dialogue: 0,1:04:26.06,1:04:30.28,English,,0,0,0,,Okay and then so we're going through and for each i,j in this matrix c
Dialogue: 0,1:04:30.30,1:04:35.96,English,,0,0,0,,We're computing an inner product and then creating that sum
Dialogue: 0,1:04:38.20,1:04:42.64,English,,0,0,0,,So we can actually turns out there's a lot of different ways to do matrix multiply
Dialogue: 0,1:04:43.76,1:04:46.08,English,,0,0,0,,And this is we can permute these these loops
Dialogue: 0,1:04:47.36,1:04:50.50,English,,0,0,0,,In any of six different possible permutations
Dialogue: 0,1:04:51.44,1:04:56.10,English,,0,0,0,,So this is a permutation where it's i followed by j followed by k
Dialogue: 0,1:04:56.10,1:05:00.48,English,,0,0,0,,But five other possibilities are feasible
Dialogue: 0,1:05:01.94,1:05:05.56,English,,0,0,0,,And so we can actually analyze those those different permutations
Dialogue: 0,1:05:05.90,1:05:08.24,English,,0,0,0,,And predict which one will have the best performance
Dialogue: 0,1:05:10.24,1:05:12.02,English,,0,0,0,,Okay so what we'll do is we'll look at the inner loop
Dialogue: 0,1:05:15.56,1:05:18.02,English,,0,0,0,,And we'll look at the access pattern of the inner loops
Dialogue: 0,1:05:18.02,1:05:22.34,English,,0,0,0,,And it's in the access pattern on arrays c,a and b
Dialogue: 0,1:05:25.22,1:05:30.94,English,,0,0,0,,Okay so let's look at the i,j,k implementation that I just showed you
Dialogue: 0,1:05:31.76,1:05:33.60,English,,0,0,0,,So as always we focus on the inner loop
Dialogue: 0,1:05:35.38,1:05:41.54,English,,0,0,0,,And if you notice this inner loop is doing a row wise access of column a
Dialogue: 0,1:05:42.98,1:05:47.18,English,,0,0,0,,And a column wise access,I'm sorry, a row wise access of array a
Dialogue: 0,1:05:47.82,1:05:51.00,English,,0,0,0,,And column wise access of row b
Dialogue: 0,1:05:52.24,1:05:56.62,English,,0,0,0,,So row wise of a,column wise of b,we don't really care about c
Dialogue: 0,1:05:56.62,1:05:59.82,English,,0,0,0,,Because it's not in the inner loop okay so just ignore that
Dialogue: 0,1:06:01.88,1:06:08.26,English,,0,0,0,,So given our assumption that we can hold in this case we're assuming that
Dialogue: 0,1:06:08.26,1:06:13.80,English,,0,0,0,,We can hold for of these integer elements in a in one block
Dialogue: 0,1:06:15.52,1:06:20.22,English,,0,0,0,,So the row wise access which has good spatial locality will miss one every four accesses
Dialogue: 0,1:06:21.14,1:06:24.68,English,,0,0,0,,Okay the very first reference will miss and then the next three will hit
Dialogue: 0,1:06:25.58,1:06:27.76,English,,0,0,0,,And then the next reference after that will hit a new block
Dialogue: 0,1:06:28.62,1:06:33.24,English,,0,0,0,,Okay so one out of four references to a will miss
Dialogue: 0,1:06:33.94,1:06:41.22,English,,0,0,0,,But because the access pattern for b is column wise every reference to b will miss
Dialogue: 0,1:06:42.46,1:06:45.84,English,,0,0,0,,Okay so the average number of misses per loop iteration is 1.25
Dialogue: 0,1:06:47.30,1:06:50.92,English,,0,0,0,,Okay the j,i,k version is exactly the same pattern
Dialogue: 0,1:06:55.08,1:06:57.78,English,,0,0,0,,K,i,j is a little different here
Dialogue: 0,1:06:59.22,1:07:01.34,English,,0,0,0,,We're doing row wise access of b
Dialogue: 0,1:07:03.12,1:07:05.64,English,,0,0,0,,And a row wise access of c, so that's good right
Dialogue: 0,1:07:06.00,1:07:09.30,English,,0,0,0,,So now we've got stride one accesses on both b and c
Dialogue: 0,1:07:10.16,1:07:13.58,English,,0,0,0,,And the reference to a is outside of the loop,so we don't care about it
Dialogue: 0,1:07:14.84,1:07:18.96,English,,0,0,0,,So so both b and c will miss one quarter of the time
Dialogue: 0,1:07:20.40,1:07:24.34,English,,0,0,0,,Okay so the total average number of misses per loop iteration will be 0.5
Dialogue: 0,1:07:25.40,1:07:29.70,English,,0,0,0,,That's pretty good and i,k,j has the same similar behavior
Dialogue: 0,1:07:30.90,1:07:33.28,English,,0,0,0,,Now j,k,i is sort of the exact opposite
Dialogue: 0,1:07:33.28,1:07:37.74,English,,0,0,0,,j,k,i does column wise access of a
Dialogue: 0,1:07:38.48,1:07:42.20,English,,0,0,0,,And column-wise access of c so right we know that's a stinker right
Dialogue: 0,1:07:42.96,1:07:51.62,English,,0,0,0,,And we qualitative well you know it's bad and we can compute that it will miss a one time per loop iteration
Dialogue: 0,1:07:52.46,1:07:55.36,English,,0,0,0,,So that will be two total of two misses per iteration
Dialogue: 0,1:07:55.54,1:07:57.90,English,,0,0,0,,And k,j,i has the same bad pattern
Dialogue: 0,1:07:58.86,1:08:01.02,English,,0,0,0,,Okay so if we look at all these permutations
Dialogue: 0,1:08:02.42,1:08:08.68,English,,0,0,0,,You can see that i,j,k and j,i,k miss 1.25 have 1.25 misses
Dialogue: 0,1:08:09.72,1:08:13.86,English,,0,0,0,,k,i,j has 0.5 misses and j,k,i has 2 misses
Dialogue: 0,1:08:15.06,1:08:20.92,English,,0,0,0,,So clearly it looks like k,i,j and its brethren are the best option
Dialogue: 0,1:08:20.92,1:08:25.56,English,,0,0,0,,The only difference is that k,i,j has this additional store
Dialogue: 0,1:08:25.56,1:08:29.54,English,,0,0,0,,So there might be a question that is that going to create is that going to slow things down
Dialogue: 0,1:08:30.74,1:08:37.20,English,,0,0,0,,Well it turns out in systems in any kind storage systems rights
Dialogue: 0,1:08:37.74,1:08:39.46,English,,0,0,0,,Are much easier to deal with them reads
Dialogue: 0,1:08:40.94,1:08:42.90,English,,0,0,0,,Can you think about why that might be true
Dialogue: 0,1:08:44.28,1:08:47.32,English,,0,0,0,,So writes you have a lot more flexibility than you do with reads
Dialogue: 0,1:08:53.06,1:08:57.26,English,,0,0,0,,I mean yes
Dialogue: 0,1:08:57.28,1:09:02.78,English,,0,0,0,,That's exactly so you can you have options you can do you can write back defer you can defer writing
Dialogue: 0,1:09:03.60,1:09:07.04,English,,0,0,0,,Until the value that you're written is actually used
Dialogue: 0,1:09:07.38,1:09:09.04,English,,0,0,0,,But when you read an item you're stuck
Dialogue: 0,1:09:10.02,1:09:12.64,English,,0,0,0,,You can't do anything until you get that data
Dialogue: 0,1:09:13.08,1:09:17.30,English,,0,0,0,,So it turns out that rights don't really that this additional store doesn't really hurt us
Dialogue: 0,1:09:18.54,1:09:22.22,English,,0,0,0,,And so when we measure these on a modern system
Dialogue: 0,1:09:22.66,1:09:29.34,English,,0,0,0,,You can see that that the k,i,j which has the the fewest number of misses
Dialogue: 0,1:09:29.82,1:09:36.32,English,,0,0,0,,Has you see we're getting like one miss what we're plotting here is cycles per inter loop iteration
Dialogue: 0,1:09:36.32,1:09:40.50,English,,0,0,0,,So each iteration is taking about one cycle which is really good
Dialogue: 0,1:09:41.54,1:09:46.30,English,,0,0,0,,This i,j,k pattern which is kind of the intermediate 1.2 misses
Dialogue: 0,1:09:47.00,1:09:51.98,English,,0,0,0,,That's sort of in between and the j,k,i which has two misses per iteration is the worst
Dialogue: 0,1:09:52.86,1:09:57.16,English,,0,0,0,,Ok so what's interesting is we could actually just by doing a little bit of analysis
Dialogue: 0,1:09:57.88,1:10:02.08,English,,0,0,0,,Simple analysis we could actually predict what this graph would look like
Dialogue: 0,1:10:03.74,1:10:08.16,English,,0,0,0,,Okay in the last ten minutes of the class
Dialogue: 0,1:10:08.46,1:10:11.28,English,,0,0,0,,We're going to look at how to improve temporal locality
Dialogue: 0,1:10:12.04,1:10:15.00,English,,0,0,0,,Now so what we did with... when we rearranged our loops
Dialogue: 0,1:10:15.38,1:10:20.26,English,,0,0,0,,With in the matrix multiplication what we were doing was in improving our spatial locality right
Dialogue: 0,1:10:21.30,1:10:24.96,English,,0,0,0,,But we didn't really do anything to improve the temporal locality
Dialogue: 0,1:10:25.58,1:10:28.58,English,,0,0,0,,To improve temporal locality you have to use a technique called blocking
Dialogue: 0,1:10:29.50,1:10:34.78,English,,0,0,0,,And this is important to understand because you're going to need it in your cache lab for one thing
Dialogue: 0,1:10:35.18,1:10:37.02,English,,0,0,0,,But it's also a very general technique
Dialogue: 0,1:10:37.50,1:10:40.82,English,,0,0,0,,Anytime you need,any time you're having issues with temporal locality
Dialogue: 0,1:10:42.08,1:10:42.82,English,,0,0,0,,Okay so
Dialogue: 0,1:10:46.02,1:10:51.00,English,,0,0,0,,We're not going to go into too much detail this code but what I did I rewrote the matrix multiply
Dialogue: 0,1:10:53.06,1:10:58.18,English,,0,0,0,,So that it operates you know a two-dimensional matrix that you can really just think of it as a contiguous array of bytes
Dialogue: 0,1:10:58.68,1:11:03.02,English,,0,0,0,,So I just rewrote this code to operate on a contiguous array one-dimensional array
Dialogue: 0,1:11:03.60,1:11:06.04,English,,0,0,0,,And then I'm doing the indexing explicitly here
Dialogue: 0,1:11:06.30,1:11:11.22,English,,0,0,0,,So here at c[i*n+j] this is n matrix
Dialogue: 0,1:11:11.22,1:11:16.30,English,,0,0,0,,So what I'm doing is...I'm accessing the I'm computing where the I throw starts
Dialogue: 0,1:11:16.92,1:11:21.66,English,,0,0,0,,And then I'm going to the j column of that row and then accessing that element
Dialogue: 0,1:11:27.50,1:11:30.08,English,,0,0,0,,All right so let's...but it's the same idea as before
Dialogue: 0,1:11:31.04,1:11:38.16,English,,0,0,0,,So let's look at the miss rate for this, this is just an original this is an original unblocked matrix multiplied
Dialogue: 0,1:11:39.34,1:11:44.78,English,,0,0,0,,So what we're doing is we're computing c[0][0]
Dialogue: 0,1:11:45.46,1:11:50.24,English,,0,0,0,,And we're doing that by taking an inner product of row 0 and column 0
Dialogue: 0,1:11:53.80,1:11:59.80,English,,0,0,0,,So if you look at the we're assuming that the cache blocks holds eight doubles
Dialogue: 0,1:12:00.06,1:12:04.64,English,,0,0,0,,And that the matrix elements are doubles then we're going to miss one eight of the time
Dialogue: 0,1:12:05.96,1:12:08.06,English,,0,0,0,,Okay so in the first iteration
Dialogue: 0,1:12:11.42,1:12:16.22,English,,0,0,0,,We're going to miss the first iteration does n of these things
Dialogue: 0,1:12:16.66,1:12:19.34,English,,0,0,0,,And since we're missing n over eight of the time
Dialogue: 0,1:12:19.72,1:12:25.12,English,,0,0,0,,We're missing one block for every eight references
Dialogue: 0,1:12:28.30,1:12:32.48,English,,0,0,0,,For each for the first iteration we're going to miss n over eight
Dialogue: 0,1:12:34.22,1:12:37.44,English,,0,0,0,,And since there's n for each element for each block I'm sorry
Dialogue: 0,1:12:37.90,1:12:45.56,English,,0,0,0,,And then oh so this is the number of blocks and the number of misses and then we have n elements
Dialogue: 0,1:12:45.98,1:12:52.46,English,,0,0,0,,So that the total number of misses is 9n/8 misses for the first iteration
Dialogue: 0,1:12:54.24,1:12:58.56,English,,0,0,0,,Okay the second iteration will have the same number of misses
Dialogue: 0,1:12:58.56,1:13:01.92,English,,0,0,0,,Because of our assumptions about the the size of this array
Dialogue: 0,1:13:02.36,1:13:06.00,English,,0,0,0,,So this these rows are way too big to fit in the cache
Dialogue: 0,1:13:06.20,1:13:09.46,English,,0,0,0,,So we never get any we don't get any temporal locality
Dialogue: 0,1:13:11.24,1:13:18.30,English,,0,0,0,,Okay so the total number of misses is 9n/8 times the number of elements that we're updating which is n squared
Dialogue: 0,1:13:18.94,1:13:23.20,English,,0,0,0,,Okay so our total misses is 9n/8*n^3
Dialogue: 0,1:13:25.62,1:13:28.38,English,,0,0,0,,Now let's rewrite the code to use blocking and so
Dialogue: 0,1:13:30.44,1:13:31.94,English,,0,0,0,,You can look at this code later
Dialogue: 0,1:13:32.40,1:13:36.78,English,,0,0,0,,But it's much simpler just to look at it pictorially
Dialogue: 0,1:13:36.80,1:13:42.06,English,,0,0,0,,So what we're doing instead of updating one element at a time
Dialogue: 0,1:13:43.30,1:13:46.82,English,,0,0,0,,We're updating a sub block a b by b sub block
Dialogue: 0,1:13:48.96,1:13:55.38,English,,0,0,0,,And we're doing that just totally analogously to when an original case where b=1
Dialogue: 0,1:13:56.54,1:14:04.10,English,,0,0,0,,This b by b sub block and c is computed by taking an inner product of the sub blocks
Dialogue: 0,1:14:05.66,1:14:11.66,English,,0,0,0,,Of a set of sub blocks in an a with a set of sub blocks in b
Dialogue: 0,1:14:12.32,1:14:15.48,English,,0,0,0,,And for each one of those we're doing a little mini matrix multiplication
Dialogue: 0,1:14:15.48,1:14:21.42,English,,0,0,0,,So we're taking this sub block times this sub block
Dialogue: 0,1:14:22.62,1:14:29.26,English,,0,0,0,,Plus the second sub block of a times the second sub block of of b
Dialogue: 0,1:14:30.06,1:14:34.22,English,,0,0,0,,Plus the third sub block of a times the third sub block of b and so on
Dialogue: 0,1:14:34.88,1:14:37.38,English,,0,0,0,,Okay so we're doing the same inner product  operation
Dialogue: 0,1:14:37.38,1:14:42.50,English,,0,0,0,,But instead of scalars we're doing it with these little sub these little tiny matrices
Dialogue: 0,1:14:45.82,1:14:50.76,English,,0,0,0,,Ok all right so let's look at,let's look at what happens to the miss rate when we do this
Dialogue: 0,1:14:53.80,1:15:03.38,English,,0,0,0,,So there's there's n over b blocks in any row or column
Dialogue: 0,1:15:05.08,1:15:11.26,English,,0,0,0,,And since there's b squared items in each block b*b
Dialogue: 0,1:15:11.76,1:15:14.30,English,,0,0,0,,There's B^2/8 misses for each block
Dialogue: 0,1:15:19.66,1:15:25.60,English,,0,0,0,,Okay and so then and then since there's there's n over b blocks in each matrix and there's two matrices
Dialogue: 0,1:15:26.30,1:15:34.82,English,,0,0,0,,There's 2n/B*B^2/8 misses for this first iteration
Dialogue: 0,1:15:34.82,1:15:38.66,English,,0,0,0,,So that works out to be an nB/4 and
Dialogue: 0,1:15:41.50,1:15:46.30,English,,0,0,0,,And the second iteration has the same miss rate
Dialogue: 0,1:15:47.08,1:15:54.20,English,,0,0,0,,So the total number of misses is the number of misses for each iteration
Dialogue: 0,1:15:55.44,1:16:01.22,English,,0,0,0,,Times the number of elements in C that we're updating
Dialogue: 0,1:16:01.56,1:16:04.06,English,,0,0,0,,Okay which is (n/B)^2
Dialogue: 0,1:16:05.80,1:16:10.20,English,,0,0,0,,So that all works out too it's still in n^3*(1/4B)
Dialogue: 0,1:16:12.86,1:16:19.48,English,,0,0,0,,So in our first case with no blocking although that the number of misses is asymptotically the same
Dialogue: 0,1:16:20.06,1:16:25.40,English,,0,0,0,,But there's this pretty, this big difference in the constant factor so for no blocking it's 9/8
Dialogue: 0,1:16:26.26,1:16:31.08,English,,0,0,0,,For blocking it's 1/4b we're now we can we can just sort of drive that down
Dialogue: 0,1:16:31.52,1:16:36.38,English,,0,0,0,,By by increasing the block size so this gives us some control
Dialogue: 0,1:16:39.02,1:16:44.68,English,,0,0,0,,But we still we have...we can't make the block the blocks too big because we need to fit three blocks
Dialogue: 0,1:16:45.44,1:16:47.46,English,,0,0,0,,In cache at any one point in time
Dialogue: 0,1:16:50.44,1:16:52.80,English,,0,0,0,,Ok so the reason this is a dramatic difference right
Dialogue: 0,1:16:54.34,1:16:59.80,English,,0,0,0,,And the reason for this is that by doing the blocking we're sort of exploiting
Dialogue: 0,1:17:00.54,1:17:04.74,English,,0,0,0,,Once we load a block into memory we're sort of reusing its items over and over again
Dialogue: 0,1:17:04.74,1:17:06.74,English,,0,0,0,,So we're exploiting more temporal locality
Dialogue: 0,1:17:08.30,1:17:12.68,English,,0,0,0,,And matrix multiplication has this into this implicit locality
Dialogue: 0,1:17:12.68,1:17:18.88,English,,0,0,0,,Because the computation is order n cubed but the size of the data is n squared
Dialogue: 0,1:17:19.98,1:17:23.36,English,,0,0,0,,Right so we must be reusing some data items
Dialogue: 0,1:17:24.18,1:17:29.71,English,,0,0,0,,Right the problem with our scalar approach is that we were when we were reusing them they weren't in the cache
Dialogue: 0,1:17:29.71,1:17:32.67,English,,0,0,0,,Ok
Dialogue: 0,1:17:35.66,1:17:40.00,English,,0,0,0,,All right so the point that I wanted to make with you is that
Dialogue: 0,1:17:40.72,1:17:47.06,English,,0,0,0,,Cache memories although they're sort of built-in automatic hardware storage devices
Dialogue: 0,1:17:47.72,1:17:49.60,English,,0,0,0,,And you can't really control them
Dialogue: 0,1:17:50.04,1:17:54.20,English,,0,0,0,,If you know about them you can take advantage of your knowledge
Dialogue: 0,1:17:54.68,1:17:58.16,English,,0,0,0,,And exploit them and make your code run faster
Dialogue: 0,1:17:59.36,1:18:03.14,English,,0,0,0,,Okay and the way you do this is like I said focus on the inner loops
Dialogue: 0,1:18:05.84,1:18:10.00,English,,0,0,0,,Do is try to do try to do accesses that a stride one
Dialogue: 0,1:18:10.66,1:18:14.02,English,,0,0,0,,And try to maximize to to maximize spatial locality
Dialogue: 0,1:18:14.40,1:18:18.42,English,,0,0,0,,And try to maximize temporal locality by reusing local variables
Dialogue: 0,1:18:18.50,1:18:20.08,English,,0,0,0,,Which can then be put into registers
Dialogue: 0,1:18:21.90,1:18:26.46,English,,0,0,0,,Okay so that's it for today good luck with your attack lab if you haven't finished it and
Dialogue: 0,1:18:27.18,1:18:32.30,English,,0,0,0,,Don't forget to get started on cache lab this weekend 
