[Script Info]
; Script generated by Aegisub 3.2.2
; http://www.aegisub.org/
Title: Default Aegisub file
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: TV.601
PlayResX: 1280
PlayResY: 720

[Aegisub Project Garbage]
Audio File: ../../../Desktop/csapp/Lecture 26 Thread Level Parallelism.mp4
Video File: ../../../Desktop/csapp/Lecture 26 Thread Level Parallelism.mp4
Video AR Mode: 4
Video AR Value: 1.777778
Video Zoom Percent: 1.000000
Scroll Position: 787
Active Line: 795
Video Position: 117614

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,20,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: newcsapp,Source Han Sans CN,34,&H00D6DA3A,&H00FFFFFF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1
Style: csapp,Source Han Sans CN,34,&H00FFFFFF,&H00FFFFFF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.10,0:00:01.12,csapp,,0,0,0,,well hello everyone
Dialogue: 0,0:00:03.78,0:00:08.28,csapp,,0,0,0,, interesting how far fewer seats are filled than at the beginning of the course 
Dialogue: 0,0:00:09.70,0:00:14.56,csapp,,0,0,0,,so that of course we're in the final stretch of this course 
Dialogue: 0,0:00:15.14,0:00:16.72,csapp,,0,0,0,,you're working on the last lab 
Dialogue: 0,0:00:17.44,0:00:23.48,csapp,,0,0,0,,and the material that we're covering both this lecture and next lecture are not on the exam 
Dialogue: 0,0:00:23.48,0:00:28.50,csapp,,0,0,0,,and you don't need them for your web so at some level you could just tune out and skip it all 
Dialogue: 0,0:00:28.50,0:00:31.40,csapp,,0,0,0,,and if your only purpose in taking this course is to pass it 
Dialogue: 0,0:00:33.14,0:00:37.52,csapp,,0,0,0,,or to get some grade in it and that's it well go ahead to now 
Dialogue: 0,0:00:38.16,0:00:44.90,csapp,,0,0,0,, but on the other hand the material we're talking about is very relevant to where computers are today 
Dialogue: 0,0:00:44.90,0:00:46.38,csapp,,0,0,0,,and where they're going in the future
Dialogue: 0,0:00:46.88,0:00:49.28,csapp,,0,0,0,, and so if you think about the longer term 
Dialogue: 0,0:00:49.28,0:00:54.02,csapp,,0,0,0,,and whatever your investment is in the computer industry and computer technology is
Dialogue: 0,0:00:54.54,0:00:56.86,csapp,,0,0,0,, then I think you'll find these very worthwhile 
Dialogue: 0,0:00:57.48,0:01:01.36,csapp,,0,0,0,,but so think of this more as the icing on the cake
Dialogue: 0,0:01:01.40,0:01:05.28,csapp,,0,0,0,,you've learned the hard stuff you've you've done the grinding part 
Dialogue: 0,0:01:05.70,0:01:12.44,csapp,,0,0,0,,and now you get to think beyond the sort of narrow confines of the course material and and think bigger 
Dialogue: 0,0:01:12.50,0:01:18.00,csapp,,0,0,0,,but that's really the way you should be viewing this lecture in the last lecture which will be on Thursday 
Dialogue: 0,0:01:18.68,0:01:21.30,csapp,,0,0,0,, so today what we're going to talk about is parallelism 
Dialogue: 0,0:01:22.98,0:01:24.20,csapp,,0,0,0,,and the issue is that 
Dialogue: 0,0:01:25.98,0:01:26.86,csapp,,0,0,0,,that 
Dialogue: 0,0:01:28.12,0:01:28.54,csapp,,0,0,0,,Wow
Dialogue: 0,0:01:33.18,0:01:37.26,csapp,,0,0,0,,that PowerPoint is a product made by a certain company in Seattle that
Dialogue: 0,0:01:38.08,0:01:40.08,csapp,,0,0,0,, it's not always reliable but 
Dialogue: 0,0:01:41.04,0:01:49.30,csapp,,0,0,0,,the issue is as you know nowadays when you buy a computer you don't get just one CPU on the processor chip 
Dialogue: 0,0:01:49.38,0:01:52.42,csapp,,0,0,0,,you have at least two on a typical laptop
Dialogue: 0,0:01:53.10,0:02:00.50,csapp,,0,0,0,,even my phone has two cores in it and as well as for graphic processing units 
Dialogue: 0,0:02:01.34,0:02:07.80,csapp,,0,0,0,,and a typical the the next generation of iPad will be a six core processor 
Dialogue: 0,0:02:07.92,0:02:15.90,csapp,,0,0,0,,so these have become not just the the sort of specialized domain of of high-end machines 
Dialogue: 0,0:02:16.00,0:02:18.60,csapp,,0,0,0,,but actually there all the time
Dialogue: 0,0:02:19.06,0:02:25.80,csapp,,0,0,0,, and actually we'll talk some next time why is it that instead of having one fast computer 
Dialogue: 0,0:02:25.80,0:02:31.20,csapp,,0,0,0,,you get to a medium size medium performance processors on a chip or more 
Dialogue: 0,0:02:31.68,0:02:35.24,csapp,,0,0,0,,and that that's actually a really interesting technology issue that I'll talk about next time 
Dialogue: 0,0:02:36.18,0:02:39.96,csapp,,0,0,0,,but it's the way it is so you can think of it when you write a program 
Dialogue: 0,0:02:41.92,0:02:44.10,csapp,,0,0,0,,ah and it runs as a single thread 
Dialogue: 0,0:02:44.50,0:02:50.40,csapp,,0,0,0,,then you're basically not making use of the computing resources that you have available to you
Dialogue: 0,0:02:51.10,0:02:56.98,csapp,,0,0,0,, so the natural thing is well could we make our programs run faster by doing multiple threads 
Dialogue: 0,0:02:57.60,0:02:58.64,csapp,,0,0,0,,so you've already learned 
Dialogue: 0,0:02:59.16,0:03:04.52,csapp,,0,0,0,,or you're in the process of applying a multi-threaded programming 
Dialogue: 0,0:03:05.00,0:03:06.10,csapp,,0,0,0,,as a way to
Dialogue: 0,0:03:09.58,0:03:13.88,csapp,,0,0,0,,deal with a concurrency of external events 
Dialogue: 0,0:03:14.08,0:03:17.82,csapp,,0,0,0,,there's multiple clients  who want to make use of a server
Dialogue: 0,0:03:18.24,0:03:22.32,csapp,,0,0,0,, and instead of serving one and then another and then another if you can handle them all
Dialogue: 0,0:03:23.00,0:03:25.80,csapp,,0,0,0,, it's sort of a an external use of concurrency
Dialogue: 0,0:03:26.60,0:03:29.56,csapp,,0,0,0,,but what we'll talk about today is more in internal use 
Dialogue: 0,0:03:29.82,0:03:37.40,csapp,,0,0,0,,can I make use of multiple threads running on multiple cores  to make a program run a single program run faster 
Dialogue: 0,0:03:38.16,0:03:41.14,csapp,,0,0,0,,and the message behind that is yes but 
Dialogue: 0,0:03:41.86,0:03:50.02,csapp,,0,0,0,, and what I mean is it is truly possible and people spend a lot of time making programs run faster by using multiple threads
Dialogue: 0,0:03:50.50,0:03:53.76,csapp,,0,0,0,, but it's harder than you'd think it should be and 
Dialogue: 0,0:03:53.98,0:03:57.92,csapp,,0,0,0,,it's fraught with as you probably already experienced programming bugs 
Dialogue: 0,0:03:58.40,0:04:03.88,csapp,,0,0,0,,but also it's just really darn hard to get the kind of performance out of a multi-core processor 
Dialogue: 0,0:04:03.88,0:04:06.34,csapp,,0,0,0,,that you should would think it'd be available 
Dialogue: 0,0:04:06.86,0:04:08.12,csapp,,0,0,0,,so we'll talk about some of that 
Dialogue: 0,0:04:09.48,0:04:12.72,csapp,,0,0,0,,and then we'll finish it up a little bit understanding of 
Dialogue: 0,0:04:13.28,0:04:20.46,csapp,,0,0,0,,how how when you writing concurrent programs you want to think about the state of memory
Dialogue: 0,0:04:20.92,0:04:27.62,csapp,,0,0,0,,and how that's a challenge for multi-core processors or in fact any concurrent concurrent system 
Dialogue: 0,0:04:30.58,0:04:37.90,csapp,,0,0,0,,so there's actually two sources of concurrency on a modern processor multiple cores
Dialogue: 0,0:04:37.96,0:04:43.08,csapp,,0,0,0,, which is you have actually a multiple CPUs on a single chip 
Dialogue: 0,0:04:43.54,0:04:45.48,csapp,,0,0,0,,but there's also something called hyper-threading 
Dialogue: 0,0:04:46.18,0:04:48.54,csapp,,0,0,0,,which is in my experience less useful 
Dialogue: 0,0:04:48.98,0:04:50.02,csapp,,0,0,0,,but let me go through this 
Dialogue: 0,0:04:50.50,0:04:56.36,csapp,,0,0,0,,so this is what a typical modern processor looks like processor chip
Dialogue: 0,0:04:57.04,0:04:58.96,csapp,,0,0,0,, is that there's actually on a single chip
Dialogue: 0,0:04:59.66,0:05:03.28,csapp,,0,0,0,, there's multiple independent CPUs 
Dialogue: 0,0:05:04.72,0:05:09.32,csapp,,0,0,0,,and each of them has some part of the cache hierarchy 
Dialogue: 0,0:05:10.44,0:05:15.28,csapp,,0,0,0,,which is private to that particular core 
Dialogue: 0,0:05:15.94,0:05:20.04,csapp,,0,0,0,,and then there is another part of the cache hierarchy that's shared across cores 
Dialogue: 0,0:05:20.60,0:05:23.66,csapp,,0,0,0,,and then they all have a common interface to main memory 
Dialogue: 0,0:05:25.28,0:05:28.34,csapp,,0,0,0,,so if these cores are running and this is what happens a lot is
Dialogue: 0,0:05:28.42,0:05:32.28,csapp,,0,0,0,, they're running programs that are completely independent have nothing to do with each
Dialogue: 0,0:05:32.78,0:05:36.90,csapp,,0,0,0,,other then they more or less just exist and run and they're happy as can be
Dialogue: 0,0:05:36.90,0:05:40.06,csapp,,0,0,0,, they are caching parts of their own state 
Dialogue: 0,0:05:40.70,0:05:49.42,csapp,,0,0,0,,and you know sometimes this cache will get polluted by the junk from other programs in terms of performance 
Dialogue: 0,0:05:49.66,0:05:51.74,csapp,,0,0,0,,but it will matter with functionality 
Dialogue: 0,0:05:52.46,0:05:56.52,csapp,,0,0,0,,the trick when you're trying to do multi-core programming as a parallel computing thing 
Dialogue: 0,0:05:56.62,0:06:05.30,csapp,,0,0,0,, somehow getting all these cores working on parts of different parts of a single problem in a way that 
Dialogue: 0,0:06:06.52,0:06:09.12,csapp,,0,0,0,,makes it so that you get the performance out of it
Dialogue: 0,0:06:09.16,0:06:13.72,csapp,,0,0,0,,they don't spend all their time basically arguing with each other about who has access to what 
Dialogue: 0,0:06:14.70,0:06:18.70,csapp,,0,0,0,,and also they're not stepping over each other and messing up each other's state 
Dialogue: 0,0:06:22.22,0:06:28.78,csapp,,0,0,0,,so hyper threading is a little bit more into the deep works of how a processor operates 
Dialogue: 0,0:06:29.46,0:06:35.06,csapp,,0,0,0,,you'll recall from the lecture on a performance or what's chapter 5 of the book
Dialogue: 0,0:06:35.76,0:06:43.86,csapp,,0,0,0,, that a modern microprocessor looks absolutely nothing like the model that you get by looking at assembly code instructions
Dialogue: 0,0:06:44.00,0:06:47.20,csapp,,0,0,0,,the model of assembly code is you execute one instruction 
Dialogue: 0,0:06:47.78,0:06:50.65,csapp,,0,0,0,,then you execute the next one then you execute the next one
Dialogue: 0,0:06:51.02,0:06:52.86,csapp,,0,0,0,,modern prostitutes don't do that at all
Dialogue: 0,0:06:53.34,0:06:59.00,csapp,,0,0,0,,they haven't done it for well they haven't done it that way for 30 years
Dialogue: 0,0:06:59.60,0:07:04.06,csapp,,0,0,0,,and since 1995 so since 20 years 
Dialogue: 0,0:07:04.60,0:07:06.60,csapp,,0,0,0,,they do it in this totally different way
Dialogue: 0,0:07:07.04,0:07:12.28,csapp,,0,0,0,,  which is sometimes referred to as out of order processing and  so just real quickly 
Dialogue: 0,0:07:13.14,0:07:20.74,csapp,,0,0,0,,but the basic idea is on the processor chip there's multiple functional units that are capable of doing different types of operation 
Dialogue: 0,0:07:21.24,0:07:26.76,csapp,,0,0,0,,there's ones for integer arithmetic these ones for floating-point arithmetic  and so forth 
Dialogue: 0,0:07:27.14,0:07:32.76,csapp,,0,0,0,,and then there's separate blocks that interface to the memory actually to the cache memories
Dialogue: 0,0:07:33.38,0:07:38.96,csapp,,0,0,0,, and they're both loading meaning reading from the memory and storing writing out to memory 
Dialogue: 0,0:07:39.76,0:07:43.14,csapp,,0,0,0,,but these units are sort of operate independently 
Dialogue: 0,0:07:43.60,0:07:50.88,csapp,,0,0,0,,and what happens is there's a block of logic which is actually enormous with huge block of logic in an x86 processor
Dialogue: 0,0:07:51.44,0:07:54.40,csapp,,0,0,0,, that reads the instructions out of the instruction stream
Dialogue: 0,0:07:54.86,0:08:01.16,csapp,,0,0,0,, rips them apart into little pieces keeps track of data dependencies and control dependencies 
Dialogue: 0,0:08:01.66,0:08:07.92,csapp,,0,0,0,,and then schedules all the various operations in your program  on these different functional units 
Dialogue: 0,0:08:08.02,0:08:14.70,csapp,,0,0,0,,so we talked some about that of in the context of how can you write up program that will sort of maximize 
Dialogue: 0,0:08:15.78,0:08:21.64,csapp,,0,0,0,,how much is going on down here  by writing your code in a particular ways 
Dialogue: 0,0:08:22.62,0:08:26.22,csapp,,0,0,0,,so all this is an introduction to say
Dialogue: 0,0:08:26.60,0:08:29.26,csapp,,0,0,0,, this is how you have to understand what hyper threading is 
Dialogue: 0,0:08:30.82,0:08:35.46,csapp,,0,0,0,,so in a single execution mode there's basically one instruction decoder 
Dialogue: 0,0:08:36.54,0:08:40.76,csapp,,0,0,0,,and it has its own set of state here its own program counter
Dialogue: 0,0:08:41.18,0:08:46.76,csapp,,0,0,0,, its own queue of operations that it's already decoded and haven't completed yet
Dialogue: 0,0:08:47.34,0:08:53.84,csapp,,0,0,0,,it has its own set of registers they're actually not registers like you'd expect they're they're highly virtualized registers 
Dialogue: 0,0:08:54.52,0:09:01.84,csapp,,0,0,0,,but all this state is there to help to service the execution of one thread of execution 
Dialogue: 0,0:09:02.82,0:09:05.26,csapp,,0,0,0,, with hyper threading basically what you do is 
Dialogue: 0,0:09:05.82,0:09:12.78,csapp,,0,0,0,,the idea that is to say 90% of all programs don't really make use of all these functional units 
Dialogue: 0,0:09:12.94,0:09:16.86,csapp,,0,0,0,,especially if you're blocking on a load 
Dialogue: 0,0:09:16.86,0:09:19.16,csapp,,0,0,0,,because there's a miss in a cache
Dialogue: 0,0:09:19.86,0:09:24.00,csapp,,0,0,0,,then all these arithmetic units are sitting there without anything useful work to do
Dialogue: 0,0:09:25.52,0:09:28.20,csapp,,0,0,0,,oh and so why don't we just double up 
Dialogue: 0,0:09:29.06,0:09:32.46,csapp,,0,0,0,,or quadruple upper K times up 
Dialogue: 0,0:09:33.04,0:09:39.54,csapp,,0,0,0,,the state associated with the decoding and control parts of the program 
Dialogue: 0,0:09:40.12,0:09:42.34,csapp,,0,0,0,,so that you can have multiple threads running 
Dialogue: 0,0:09:43.68,0:09:46.56,csapp,,0,0,0,,and sharing these functional units among each other 
Dialogue: 0,0:09:46.64,0:09:52.32,csapp,,0,0,0,,so they're operating really independently their states are not intertwined 
Dialogue: 0,0:09:52.90,0:09:59.24,csapp,,0,0,0,,but they're sort of making more use of the available hardware for performing functions 
Dialogue: 0,0:09:59.56,0:10:02.32,csapp,,0,0,0,, and so that's called hyper threading that's an Intel term 
Dialogue: 0,0:10:02.90,0:10:06.78,csapp,,0,0,0,,you also sometimes hear call SMT simultaneous multi-threading 
Dialogue: 0,0:10:07.58,0:10:11.46,csapp,,0,0,0,,and in my experience and we'll see here the numbers
Dialogue: 0,0:10:11.98,0:10:13.72,csapp,,0,0,0,, it doesn't really make that big a difference
Dialogue: 0,0:10:14.00,0:10:19.72,csapp,,0,0,0,, but it turns out to be in the sort of large picture things 
Dialogue: 0,0:10:20.16,0:10:24.60,csapp,,0,0,0,,a relatively inexpensive feature for them to throw on to processors and so they do
Dialogue: 0,0:10:25.28,0:10:32.30,csapp,,0,0,0,,and so nowadays at least with an x86 processor usually have to weigh hyper-threading in them 
Dialogue: 0,0:10:34.02,0:10:40.64,csapp,,0,0,0,,so given that if you look at our shark machines which are a little bit old there's sort of 2010 era machine 
Dialogue: 0,0:10:41.10,0:10:43.92,csapp,,0,0,0,,but they were high-end machines in their day 
Dialogue: 0,0:10:43.92,0:10:47.20,csapp,,0,0,0,,and so they still actually are more powerful than 
Dialogue: 0,0:10:47.72,0:10:53.56,csapp,,0,0,0,,what you'd buy as say a desktop and way more powerful than as a laptop  that you'd get today 
Dialogue: 0,0:10:53.56,0:10:55.54,csapp,,0,0,0,, so they're actually pretty decent machines 
Dialogue: 0,0:10:55.92,0:11:02.00,csapp,,0,0,0,,and actually we'll talk next time about why computers aren't a lot faster than they were five years ago
Dialogue: 0,0:11:02.00,0:11:04.10,csapp,,0,0,0,,that's actually an interesting technology thing
Dialogue: 0,0:11:05.18,0:11:12.22,csapp,,0,0,0,, so that they their server class machines so they have multiple cores
Dialogue: 0,0:11:12.80,0:11:23.54,csapp,,0,0,0,, and they have eight of them which is a lot you can buy ten core machines x86 machines on a single chip
Dialogue: 0,0:11:23.60,0:11:25.14,csapp,,0,0,0,,but I don't think you can get more yet
Dialogue: 0,0:11:25.36,0:11:29.16,csapp,,0,0,0,,so these were fairly advanced machine of their day 
Dialogue: 0,0:11:29.16,0:11:31.50,csapp,,0,0,0,,and they also have two-way hyper threading 
Dialogue: 0,0:11:32.64,0:11:33.52,csapp,,0,0,0,,so in theory 
Dialogue: 0,0:11:36.96,0:11:40.34,csapp,,0,0,0,,you should be able to get sixteen independent threads running 
Dialogue: 0,0:11:40.62,0:11:44.72,csapp,,0,0,0,,sort of 16 Way parallelism potentially out of a program
Dialogue: 0,0:11:44.74,0:11:50.36,csapp,,0,0,0,,if you can keep everything working and keep bad things from happening
Dialogue: 0,0:11:55.18,0:12:01.94,csapp,,0,0,0,,so let's give a really trivial application one that should be very simple to make run in parallel 
Dialogue: 0,0:12:02.42,0:12:07.74,csapp,,0,0,0,,that says imaginary we want to sum up the numbers between 0 and n-1
Dialogue: 0,0:12:07.74,0:12:12.38,csapp,,0,0,0,,which is by the way a really stupid thing to do because there's a very simple closed-form formula for it 
Dialogue: 0,0:12:13.28,0:12:15.58,csapp,,0,0,0,,which is good in the sense it will let us check our work 
Dialogue: 0,0:12:15.76,0:12:18.36,csapp,,0,0,0,,but it's a completely stupid application 
Dialogue: 0,0:12:19.00,0:12:21.02,csapp,,0,0,0,,but it just shows you this idea 
Dialogue: 0,0:12:21.74,0:12:24.44,csapp,,0,0,0,,and so what we're just going to do is is block off
Dialogue: 0,0:12:24.98,0:12:31.68,csapp,,0,0,0,,if we have n way parallelism we're just going to split our range of numbers n ways 
Dialogue: 0,0:12:32.58,0:12:37.60,csapp,,0,0,0,,and just have a single threads sum up one n of the numbers 
Dialogue: 0,0:12:38.20,0:12:43.50,csapp,,0,0,0,,and then they'll collectively sum together the result  in some way or another 
Dialogue: 0,0:12:43.82,0:12:46.42,csapp,,0,0,0,,so this is about as easy a parallel program as you could imagine 
Dialogue: 0,0:12:47.84,0:12:53.54,csapp,,0,0,0,,so let's do a 1 version which is said well gee I understand how to use threads 
Dialogue: 0,0:12:54.20,0:12:58.90,csapp,,0,0,0,,P threads and I know about these things called semaphores or mutual exclusion
Dialogue: 0,0:12:59.42,0:13:04.58,csapp,,0,0,0,, so what I'll do is just all I'll have one place in memory 
Dialogue: 0,0:13:04.58,0:13:08.14,csapp,,0,0,0,,where I'm collecting the sum over all n values 
Dialogue: 0,0:13:08.88,0:13:11.86,csapp,,0,0,0,,and for a thread to be able to add to that
Dialogue: 0,0:13:12.10,0:13:19.28,csapp,,0,0,0,,if it will lock it it will get a mutual exclusive access to it  increment it and then unlock it 
Dialogue: 0,0:13:19.40,0:13:24.66,csapp,,0,0,0,,and we'll just let all the threads go helter-skelter locking and unlocking this 
Dialogue: 0,0:13:25.72,0:13:27.52,csapp,,0,0,0,,so the code for that's pretty easy to write it
Dialogue: 0,0:13:27.86,0:13:36.46,csapp,,0,0,0,, it's a here's the the code of course all threaded code looks a lot messier than you think it should but 
Dialogue: 0,0:13:37.12,0:13:39.32,csapp,,0,0,0,,in the end it's a fairly straightforward code 
Dialogue: 0,0:13:40.16,0:13:41.32,csapp,,0,0,0,,so in particular 
Dialogue: 0,0:13:42.96,0:13:44.46,csapp,,0,0,0,,this is the thread routine is
Dialogue: 0,0:13:46.12,0:13:55.02,csapp,,0,0,0,,passing through this weird Varg pea structure that you do with with threads
Dialogue: 0,0:13:55.26,0:13:58.20,csapp,,0,0,0,,the way you pass arguments to a thread routine 
Dialogue: 0,0:13:58.60,0:14:02.84,csapp,,0,0,0,,but basically it's figuring out where is the start and end range of the numbers 
Dialogue: 0,0:14:03.56,0:14:09.52,csapp,,0,0,0,,then adding for all i between the start before the end 
Dialogue: 0,0:14:10.10,0:14:14.88,csapp,,0,0,0,,I'll lock that acquire a semaphore lock 
Dialogue: 0,0:14:15.64,0:14:19.08,csapp,,0,0,0,,I'll increment this global sum and then I'll release the lock 
Dialogue: 0,0:14:20.66,0:14:23.56,csapp,,0,0,0,,okay so pretty much the style of code that you've been working with 
Dialogue: 0,0:14:25.10,0:14:27.92,csapp,,0,0,0,,and what you find is actually this is really a bad idea 
Dialogue: 0,0:14:28.92,0:14:30.40,csapp,,0,0,0,,so running as a single thread
Dialogue: 0,0:14:31.22,0:14:33.04,csapp,,0,0,0,,it takes 51 seconds to do that 
Dialogue: 0,0:14:33.04,0:14:37.04,csapp,,0,0,0,,it would be by the way if you didn't lock and unlock because it's only one thread
Dialogue: 0,0:14:37.04,0:14:40.60,csapp,,0,0,0,,you'd blow this away it would take just a couple seconds so 
Dialogue: 0,0:14:41.30,0:14:44.02,csapp,,0,0,0,,and then you see as you add more threads it actually gets worse 
Dialogue: 0,0:14:44.80,0:14:47.60,csapp,,0,0,0,,and especially if you jump from one to two 
Dialogue: 0,0:14:48.12,0:14:51.32,csapp,,0,0,0,,you increase by a factor nine how much time  it takes 
Dialogue: 0,0:14:52.78,0:14:58.04,csapp,,0,0,0,,and it always starts to get better as you get up into eight threads and then it gets worse again 
Dialogue: 0,0:14:59.04,0:15:06.28,csapp,,0,0,0,,so the reason is that locking  unlocking is a very time-consuming task
Dialogue: 0,0:15:06.30,0:15:15.38,csapp,,0,0,0,,and basically you can think of is that you if you have that map of the multi-core processors with all their private caches in one shared cache 
Dialogue: 0,0:15:16.10,0:15:22.30,csapp,,0,0,0,,these threads are basically fighting with each other for control for that one memory address 
Dialogue: 0,0:15:22.48,0:15:25.90,csapp,,0,0,0,,that they that they're incrementing 
Dialogue: 0,0:15:26.48,0:15:31.14,csapp,,0,0,0,,and it has to grab the control away from one core to your the core 
Dialogue: 0,0:15:31.62,0:15:35.44,csapp,,0,0,0,,that's accessing it do the lock unlock
Dialogue: 0,0:15:35.98,0:15:39.60,csapp,,0,0,0,, and then it gets grabbed back for it so it's a miserable performance for cache 
Dialogue: 0,0:15:40.24,0:15:42.80,csapp,,0,0,0,,huge overhead for the semaphore activities 
Dialogue: 0,0:15:43.36,0:15:45.42,csapp,,0,0,0,,and just really a bad thing all around 
Dialogue: 0,0:15:45.44,0:15:51.31,csapp,,0,0,0,,and so lesson one is semaphores or mutexes are very expensive
Dialogue: 0,0:15:52.06,0:15:54.90,csapp,,0,0,0,,and if you're trying to do low level parallelism 
Dialogue: 0,0:15:54.90,0:15:58.90,csapp,,0,0,0,,you don't want fine-grained locking at that level otherwise 
Dialogue: 0,0:15:59.02,0:16:00.32,csapp,,0,0,0,,you're just completely sunk 
Dialogue: 0,0:16:00.50,0:16:02.92,csapp,,0,0,0,,and so that's not the way to do it 
Dialogue: 0,0:16:03.34,0:16:08.78,csapp,,0,0,0,,I won't go into it but there's quite a bit of literature about what they call lock free synchronization
Dialogue: 0,0:16:09.24,0:16:12.20,csapp,,0,0,0,,which is a way to avoid semaphores but get the effect 
Dialogue: 0,0:16:12.78,0:16:15.00,csapp,,0,0,0,,and they wouldn't work in this context either those
Dialogue: 0,0:16:15.54,0:16:17.36,csapp,,0,0,0,, just if you've ever heard that term 
Dialogue: 0,0:16:17.74,0:16:19.54,csapp,,0,0,0,,those are generally designed for
Dialogue: 0,0:16:20.30,0:16:25.80,csapp,,0,0,0,, examples where you expect relatively little contention  between the threads 
Dialogue: 0,0:16:25.80,0:16:30.30,csapp,,0,0,0,,and so you try and be optimistic and then roll back if something bad happens 
Dialogue: 0,0:16:31.90,0:16:33.52,csapp,,0,0,0,,this is a case where nope 
Dialogue: 0,0:16:33.78,0:16:38.24,csapp,,0,0,0,,all those threads are going to be pounding that one memory location and they're really fighting for it
Dialogue: 0,0:16:38.30,0:16:41.10,csapp,,0,0,0,, and so there is no good solution to that problem 
Dialogue: 0,0:16:43.70,0:16:47.48,csapp,,0,0,0,,the other thing I'll point out is this jump here shows you that 
Dialogue: 0,0:16:48.20,0:16:50.76,csapp,,0,0,0,,hyper threading isn't really helping us here 
Dialogue: 0,0:16:52.42,0:16:55.88,csapp,,0,0,0,,going from the fact that we slowed down from 8 to 16 
Dialogue: 0,0:16:55.88,0:16:59.16,csapp,,0,0,0,,means we can't really make use of 16 threads in this application 
Dialogue: 0,0:16:59.68,0:17:01.60,csapp,,0,0,0,,8 threads are better than 4 
Dialogue: 0,0:17:02.06,0:17:06.74,csapp,,0,0,0,,but obviously all that's kind of a waste of time because this is just really a bad idea all around 
Dialogue: 0,0:17:08.38,0:17:08.72,csapp,,0,0,0,,so 
Dialogue: 0,0:17:09.56,0:17:11.14,csapp,,0,0,0,,let's do something different 
Dialogue: 0,0:17:11.18,0:17:17.06,csapp,,0,0,0,,let's have each of them accumulate their own sum for their own sub range 
Dialogue: 0,0:17:18.60,0:17:22.16,csapp,,0,0,0,,and we'll give up so we'll have an array of accumulators 
Dialogue: 0,0:17:23.12,0:17:29.72,csapp,,0,0,0,,where the each thread is incrementing only a one element of this array
Dialogue: 0,0:17:30.56,0:17:33.38,csapp,,0,0,0,, so they're not fighting with each other directly for it 
Dialogue: 0,0:17:33.58,0:17:38.96,csapp,,0,0,0,,but they are fighting for if you think about it for the same cache line 
Dialogue: 0,0:17:39.84,0:17:43.92,csapp,,0,0,0,,because an array is typically stored and so it's not totally nice
Dialogue: 0,0:17:45.70,0:17:48.68,csapp,,0,0,0,, ah but this it gives you a pointer to this idea
Dialogue: 0,0:17:48.80,0:17:52.14,csapp,,0,0,0,,  if if we could sort of move into a private state 
Dialogue: 0,0:17:52.88,0:17:55.86,csapp,,0,0,0,,the stuff that we're making the most direct access to
Dialogue: 0,0:17:56.68,0:17:58.40,csapp,,0,0,0,,then we'll get better performance 
Dialogue: 0,0:17:59.68,0:18:01.44,csapp,,0,0,0,,so this is the thread routine 
Dialogue: 0,0:18:01.96,0:18:05.12,csapp,,0,0,0,,and that the point is that there's some global array called pesum 
Dialogue: 0,0:18:05.12,0:18:10.80,csapp,,0,0,0,,but it's only incrementing the the part of it that sort of assigned to this particular thread 
Dialogue: 0,0:18:12.92,0:18:15.44,csapp,,0,0,0,,and here you do see a performance improvement right
Dialogue: 0,0:18:15.56,0:18:19.78,csapp,,0,0,0,,so one thread takes 5 seconds remember before it was 58 
Dialogue: 0,0:18:19.92,0:18:25.16,csapp,,0,0,0,,so that shows you just the advantage of the cost of semaphores right 
Dialogue: 0,0:18:25.66,0:18:27.14,csapp,,0,0,0,,there is a factor of 10 
Dialogue: 0,0:18:27.74,0:18:33.12,csapp,,0,0,0,, and you see you are actually getting an improvement all across the line including up to 16 threads 
Dialogue: 0,0:18:33.14,0:18:34.50,csapp,,0,0,0,,you're still getting an improvement
Dialogue: 0,0:18:35.02,0:18:38.14,csapp,,0,0,0,, it would flatten out should have just shown the number 432
Dialogue: 0,0:18:38.14,0:18:40.02,csapp,,0,0,0,,but it would flatten out at this point
Dialogue: 0,0:18:40.56,0:18:43.78,csapp,,0,0,0,,but it actually is getting some advantage out of hyper-threading as well
Dialogue: 0,0:18:45.34,0:18:48.14,csapp,,0,0,0,,so that's good it's not an amazing speed up 
Dialogue: 0,0:18:48.46,0:18:57.48,csapp,,0,0,0,,so you can think of what they call the speed up is the performance of it running on a single core  versus the performance on n cores 
Dialogue: 0,0:18:57.48,0:19:00.30,csapp,,0,0,0,,and in the ideal case it goes n times faster 
Dialogue: 0,0:19:00.94,0:19:02.28,csapp,,0,0,0,,and we're not quite hitting that
Dialogue: 0,0:19:05.90,0:19:12.04,csapp,,0,0,0,, well but here's you've already learned that it's generally bad to be accumulating into a memory 
Dialogue: 0,0:19:12.58,0:19:15.50,csapp,,0,0,0,,and so why not do the thing we learned before 
Dialogue: 0,0:19:15.90,0:19:20.50,csapp,,0,0,0,,which is you accumulate in a register and you only update the memory when you're done with that 
Dialogue: 0,0:19:21.30,0:19:24.76,csapp,,0,0,0,,so let's just do that and I'll call that the local version 
Dialogue: 0,0:19:25.20,0:19:28.60,csapp,,0,0,0,,I'll just increment a sum which is a local variable 
Dialogue: 0,0:19:29.06,0:19:30.46,csapp,,0,0,0,,and only when I'm done 
Dialogue: 0,0:19:31.24,0:19:35.30,csapp,,0,0,0,,then I'll store it in the global array 
Dialogue: 0,0:19:37.14,0:19:40.52,csapp,,0,0,0,,okay so it's functionally equivalent to the one we just showed
Dialogue: 0,0:19:40.52,0:19:46.18,csapp,,0,0,0,,we're just moving instead of accumulating in a global array we're accumulating in a register 
Dialogue: 0,0:19:47.30,0:19:49.92,csapp,,0,0,0,,and here you see a pretty big performance improvement
Dialogue: 0,0:19:51.18,0:20:00.04,csapp,,0,0,0,,so blue is what we showed with the the global array red or orange is  what's this local variable
Dialogue: 0,0:20:00.76,0:20:04.56,csapp,,0,0,0,, and so you see it's actually interesting we're getting a performance improvement 
Dialogue: 0,0:20:05.38,0:20:11.12,csapp,,0,0,0,,as well although it bottoms out at eight 
Dialogue: 0,0:20:11.28,0:20:13.89,csapp,,0,0,0,,and it actually gets worse when you go to sixteen 
Dialogue: 0,0:20:14.44,0:20:17.38,csapp,,0,0,0,,and this is showing that hyper-threading isn't really helping here 
Dialogue: 0,0:20:17.40,0:20:21.98,csapp,,0,0,0,,because basically the the single thread is just accumulating 
Dialogue: 0,0:20:22.68,0:20:27.22,csapp,,0,0,0,,as fast as it can and adding to a register 
Dialogue: 0,0:20:27.64,0:20:32.54,csapp,,0,0,0,,and so it's making pretty good use of what functional units it uses and putting multiple threads
Dialogue: 0,0:20:33.02,0:20:34.50,csapp,,0,0,0,,sharing it isn't really helping 
Dialogue: 0,0:20:34.96,0:20:39.30,csapp,,0,0,0,,at least not on the shark machines 
Dialogue: 0,0:20:39.48,0:20:42.24,csapp,,0,0,0,,this actually might be different on different machines
Dialogue: 0,0:20:43.48,0:20:49.34,csapp,,0,0,0,,and actually if you recall from the performance optimization we found that if you're just doing a bunch of additions
Dialogue: 0,0:20:49.96,0:20:54.36,csapp,,0,0,0,,you can make use as associativity and get more accumulation in parallel 
Dialogue: 0,0:20:54.86,0:20:59.66,csapp,,0,0,0,,so you could actually speed up this program just the single threaded version of this program pretty well 
Dialogue: 0,0:21:00.24,0:21:04.66,csapp,,0,0,0,,but anyways it shows it okay this is starting to look like 
Dialogue: 0,0:21:05.14,0:21:09.76,csapp,,0,0,0,,your your a your single threaded performance is pretty good and B 
Dialogue: 0,0:21:10.08,0:21:13.44,csapp,,0,0,0,,you're getting some useful speed-up out of parallelism
Dialogue: 0,0:21:14.48,0:21:19.36,csapp,,0,0,0,, but as I said this is like the easiest example in the world to parallelize so if you can't do it here
Dialogue: 0,0:21:19.90,0:21:24.52,csapp,,0,0,0,,then then life is pretty hopeless as far as multi-threading 
Dialogue: 0,0:21:25.80,0:21:29.70,csapp,,0,0,0,,so let's talk about as I mentioned this idea of speed-up 
Dialogue: 0,0:21:30.70,0:21:35.14,csapp,,0,0,0,,so speed-up is just defined to be the time for a single-threaded program 
Dialogue: 0,0:21:35.82,0:21:40.86,csapp,,0,0,0,,divided by the time for for P threads running
Dialogue: 0,0:21:41.47,0:21:45.66,csapp,,0,0,0,, or actually will use it P cores instead of P threads question 
Dialogue: 0,0:21:45.72,0:21:51.64,csapp,,0,0,0,,[student speaking]
Dialogue: 0,0:21:51.64,0:21:57.72,csapp,,0,0,0,,yes generally the scheduler has some kind of go balancing built into it 
Dialogue: 0,0:21:58.54,0:22:05.14,csapp,,0,0,0,,and it will tend to especially in a case like this where the threads are sort of grabbing and running
Dialogue: 0,0:22:05.72,0:22:10.96,csapp,,0,0,0,,making they will generally get spread across the the cores 
Dialogue: 0,0:22:11.92,0:22:14.60,csapp,,0,0,0,,so that's a pretty the Linux scheduler is pretty good at
Dialogue: 0,0:22:15.24,0:22:19.32,csapp,,0,0,0,,that when you have more threads than there are  cores 
Dialogue: 0,0:22:19.98,0:22:27.56,csapp,,0,0,0,,then then it basically starts scheduling them  in some cyclic order 
Dialogue: 0,0:22:27.58,0:22:29.84,csapp,,0,0,0,,and you you won't you'll 
Dialogue: 0,0:22:30.68,0:22:33.86,csapp,,0,0,0,,at best you will not get any advantage 
Dialogue: 0,0:22:34.00,0:22:39.00,csapp,,0,0,0,,and in a worst case you actually start slowing down from having more threads than are
Dialogue: 0,0:22:39.94,0:22:40.80,csapp,,0,0,0,,there good question 
Dialogue: 0,0:22:41.24,0:22:43.00,csapp,,0,0,0,,so there's really two versions of speed-up 
Dialogue: 0,0:22:43.02,0:22:48.06,csapp,,0,0,0,, one is if I take my multi-threaded routine and run it with one thread 
Dialogue: 0,0:22:48.72,0:22:51.54,csapp,,0,0,0,,and then I met do it with P threads or cores 
Dialogue: 0,0:22:52.16,0:22:53.18,csapp,,0,0,0,,I can get a speed-up
Dialogue: 0,0:22:53.32,0:22:59.24,csapp,,0,0,0,, but actually the truer thing is if I take the best-known sequential algorithm for performing this task 
Dialogue: 0,0:23:00.18,0:23:02.12,csapp,,0,0,0,,with the best implementation of that
Dialogue: 0,0:23:02.74,0:23:04.82,csapp,,0,0,0,,and then I compare it against my parallel one 
Dialogue: 0,0:23:04.84,0:23:07.10,csapp,,0,0,0,,and so that's referred to as absolute speed-up
Dialogue: 0,0:23:08.50,0:23:15.46,csapp,,0,0,0,, which is the the best measures you know you give both sides the opportunity to do the best implementation that 
Dialogue: 0,0:23:15.86,0:23:17.14,csapp,,0,0,0,,they can and then you compare it 
Dialogue: 0,0:23:18.08,0:23:20.54,csapp,,0,0,0,,and then what's referred to as the efficiency is 
Dialogue: 0,0:23:20.98,0:23:26.00,csapp,,0,0,0,,how close to the speed-up get to the ideal speed-up which is if I'm running on P cores
Dialogue: 0,0:23:26.80,0:23:28.18,csapp,,0,0,0,,I should be P times faster 
Dialogue: 0,0:23:28.88,0:23:30.14,csapp,,0,0,0,,and you'll see that we're 
Dialogue: 0,0:23:30.78,0:23:35.44,csapp,,0,0,0,,you know the question of hyper-threading versus not we're sort of here we're saying no you don't 
Dialogue: 0,0:23:36.38,0:23:39.42,csapp,,0,0,0,,we're not trying to gain from hyper-threading 
Dialogue: 0,0:23:40.08,0:23:43.32,csapp,,0,0,0,,you can play this game various ways and you can argue back and forth 
Dialogue: 0,0:23:44.44,0:23:46.54,csapp,,0,0,0,,whether hyper-threading should count 
Dialogue: 0,0:23:46.64,0:23:53.02,csapp,,0,0,0,,so for P is P the total number of possible threads or the total number of cores 
Dialogue: 0,0:23:53.14,0:23:58.40,csapp,,0,0,0,,it's really a something to argue back and forth about 
Dialogue: 0,0:24:00.24,0:24:06.84,csapp,,0,0,0,, so the point is the efficiency though is is measured as how much do we do relative to ideal 
Dialogue: 0,0:24:08.26,0:24:13.16,csapp,,0,0,0,, and so this is what you get for this code the local version of P some 
Dialogue: 0,0:24:13.86,0:24:19.22,csapp,,0,0,0,, you'll see that our efficiency numbers are   somewhere in the hi somebody range 
Dialogue: 0,0:24:21.08,0:24:22.26,csapp,,0,0,0,,which is good but not great 
Dialogue: 0,0:24:22.98,0:24:26.32,csapp,,0,0,0,,it's pretty good actually if you can get 75 percent efficiency 
Dialogue: 0,0:24:26.80,0:24:31.62,csapp,,0,0,0,,you're doing better than most but again that's because this should have been the world's easiest program to parallelize 
Dialogue: 0,0:24:33.94,0:24:40.84,csapp,,0,0,0,,so and the best speed-up we're getting is a factor of six out of eight course so 
Dialogue: 0,0:24:41.32,0:24:45.58,csapp,,0,0,0,,again that's pretty good but this really should be something you can do well 
Dialogue: 0,0:24:47.74,0:24:52.96,csapp,,0,0,0,,so that just gives you a flavor of what parallel computing can be 
Dialogue: 0,0:24:54.08,0:24:59.44,csapp,,0,0,0,,so now it's sort of back off and talk some general principles just like the speed-up
Dialogue: 0,0:25:00.80,0:25:04.80,csapp,,0,0,0,,there's a fella named Jean nom Dahl who coincidentally just died a few weeks ago
Dialogue: 0,0:25:04.84,0:25:06.28,csapp,,0,0,0,,he might have seen it in the news 
Dialogue: 0,0:25:06.72,0:25:11.42,csapp,,0,0,0,, he was one of the original pioneers at IBM in their mainframe computers 
Dialogue: 0,0:25:12.36,0:25:16.48,csapp,,0,0,0,,then then at some point he in the 60s
Dialogue: 0,0:25:16.88,0:25:19.48,csapp,,0,0,0,, he started his own company called um dal computers 
Dialogue: 0,0:25:19.48,0:25:24.58,csapp,,0,0,0,, and they were like they were the the cool company in mainframe computers
Dialogue: 0,0:25:24.58,0:25:27.36,csapp,,0,0,0,, if that could ever be considered cool right 
Dialogue: 0,0:25:28.28,0:25:33.04,csapp,,0,0,0,,and he built a competitor's to IBM that absolutely drove them crazy
Dialogue: 0,0:25:33.06,0:25:38.79,csapp,,0,0,0,, because they had a virtual monopoly they actually were subject to antitrust suit
Dialogue: 0,0:25:38.88,0:25:45.80,csapp,,0,0,0,,so um Dahl was a sort of the the rebel who broke away from the mother company and started a competitor 
Dialogue: 0,0:25:47.74,0:25:51.44,csapp,,0,0,0,,and he made this very simple observation that's called Alma's dolls law 
Dialogue: 0,0:25:51.44,0:25:56.80,csapp,,0,0,0,,which is basically junior high level algebra to think of this 
Dialogue: 0,0:25:56.96,0:25:59.20,csapp,,0,0,0,,but it's actually a fairly perceptive point 
Dialogue: 0,0:26:00.02,0:26:03.74,csapp,,0,0,0,,about what's the possible benefit of speeding up something 
Dialogue: 0,0:26:04.14,0:26:05.72,csapp,,0,0,0,,and this is discussed in the book
Dialogue: 0,0:26:06.16,0:26:09.78,csapp,,0,0,0,, you know this isn't just for computers it's any process that you want to speed up 
Dialogue: 0,0:26:10.40,0:26:16.22,csapp,,0,0,0,,and it's a very simple observation which is suppose there's some fraction of a system 
Dialogue: 0,0:26:16.22,0:26:19.90,csapp,,0,0,0,,that I can make go faster and I'll call that fraction P 
Dialogue: 0,0:26:20.44,0:26:26.72,csapp,,0,0,0,,P is some number between zero and one point zero right  100% 0%
Dialogue: 0,0:26:27.96,0:26:31.46,csapp,,0,0,0,,and let's suppose we take that part that we're going to make run faster
Dialogue: 0,0:26:31.86,0:26:34.24,csapp,,0,0,0,,and improve its performance by a factor okay 
Dialogue: 0,0:26:37.08,0:26:42.52,csapp,,0,0,0,,then we can just very simply talk about what will be the benefit of that performance 
Dialogue: 0,0:26:42.70,0:26:44.04,csapp,,0,0,0,,so we'll call it T sub K 
Dialogue: 0,0:26:44.76,0:26:52.00,csapp,,0,0,0,,and what it says is the fraction P  of the time will be reduced by K 
Dialogue: 0,0:26:52.98,0:26:59.30,csapp,,0,0,0,,but the fraction that you can't change a one minus P will remain at its old time
Dialogue: 0,0:27:00.07,0:27:03.56,csapp,,0,0,0,, and that's um dolls law that's it that's the whole thing 
Dialogue: 0,0:27:03.98,0:27:05.46,csapp,,0,0,0,,and one interesting measure
Dialogue: 0,0:27:05.94,0:27:11.00,csapp,,0,0,0,,is what if K were infinity what if we had unbounded resources to speed things up 
Dialogue: 0,0:27:11.86,0:27:18.92,csapp,,0,0,0,,and what the observation is the best feed up you'll get is a 1-p
Dialogue: 0,0:27:20.44,0:27:24.88,csapp,,0,0,0,, and so just think it this way if you have 10% of it that you can't change 
Dialogue: 0,0:27:26.02,0:27:29.56,csapp,,0,0,0,,the other 90% you make infinitely fast
Dialogue: 0,0:27:31.24,0:27:33.76,csapp,,0,0,0,,then your performance improvement will be a factor of 10
Dialogue: 0,0:27:34.16,0:27:37.32,csapp,,0,0,0,, that's really all it's saying right pretty straightforward idea
Dialogue: 0,0:27:38.56,0:27:41.10,csapp,,0,0,0,,so this has sort of direct implications in 
Dialogue: 0,0:27:41.40,0:27:48.00,csapp,,0,0,0,,so the example is suppose that we can improve the performance of some system of 90% of it 
Dialogue: 0,0:27:49.04,0:27:53.22,csapp,,0,0,0,,and we can speed up by factor 9 and that number is chosen to make the numbers work out 
Dialogue: 0,0:27:53.72,0:27:57.22,csapp,,0,0,0,,then we'll get it best at 2x performance improvement
Dialogue: 0,0:27:57.66,0:28:02.64,csapp,,0,0,0,,basically what it says is the part of the system that you can't speed up will become your bottleneck 
Dialogue: 0,0:28:04.02,0:28:06.08,csapp,,0,0,0,,and that's just the way it is 
Dialogue: 0,0:28:06.24,0:28:09.58,csapp,,0,0,0,,so the implications for this repair while programming are fairly obvious
Dialogue: 0,0:28:09.74,0:28:14.00,csapp,,0,0,0,,  that if we can take our application and chop off some fraction of it 
Dialogue: 0,0:28:14.60,0:28:18.18,csapp,,0,0,0,,and make it run K times faster by running it on K cores 
Dialogue: 0,0:28:19.74,0:28:28.12,csapp,,0,0,0,,then the part of it that's still running sequentially will come to will limit the ultimate performance
Dialogue: 0,0:28:30.18,0:28:33.62,csapp,,0,0,0,, we can get so that's not really an issue for this summation problem
Dialogue: 0,0:28:33.62,0:28:37.56,csapp,,0,0,0,, because it really does divide into as many independent tasks 
Dialogue: 0,0:28:37.76,0:28:40.34,csapp,,0,0,0,,as as you have numbers and
Dialogue: 0,0:28:40.88,0:28:42.60,csapp,,0,0,0,, as you can see you can make them run 
Dialogue: 0,0:28:42.60,0:28:46.64,csapp,,0,0,0,,but many other applications do some part of it that I can't really make know parallel 
Dialogue: 0,0:28:49.42,0:28:54.10,csapp,,0,0,0,,so just as an example and just for the sake of this class 
Dialogue: 0,0:28:54.28,0:28:59.70,csapp,,0,0,0,,you know an example of a little bit more involved a problem in parallel programming and multi-threading
Dialogue: 0,0:29:00.10,0:29:02.44,csapp,,0,0,0,, let's think about sorting a bunch of numbers 
Dialogue: 0,0:29:02.52,0:29:05.84,csapp,,0,0,0,,so we have n numbers and we want to sort them
Dialogue: 0,0:29:08.86,0:29:13.56,csapp,,0,0,0,, and we have some number of threads
Dialogue: 0,0:29:13.56,0:29:16.52,csapp,,0,0,0,, that we can do this with is there way we can speed this up 
Dialogue: 0,0:29:17.30,0:29:20.26,csapp,,0,0,0,,and you think about it's not that clear how you do it
Dialogue: 0,0:29:20.26,0:29:23.28,csapp,,0,0,0,, there's actually a vast literature in parallel sorting
Dialogue: 0,0:29:23.76,0:29:29.24,csapp,,0,0,0,, and those you've taken or won't take the class 210 will be exposed to a lot of this
Dialogue: 0,0:29:29.76,0:29:32.80,csapp,,0,0,0,, but I'm just going to do a very simple version which is quicksort 
Dialogue: 0,0:29:34.16,0:29:36.68,csapp,,0,0,0,,so quicksort is for example the 
Dialogue: 0,0:29:40.78,0:29:50.72,csapp,,0,0,0,,the C library program Q sort quicksort it was invented in the early 1960s or 1950s by a guy named Tony Hoare
Dialogue: 0,0:29:51.58,0:29:55.12,csapp,,0,0,0,,who also founded a lot of the fundamental logic of program 
Dialogue: 0,0:29:55.24,0:30:00.96,csapp,,0,0,0,,so he's like an amazing person still alive today lives in Cambridge England 
Dialogue: 0,0:30:01.56,0:30:07.16,csapp,,0,0,0,,but if you ever have a chance to go to a talk by him do so he's amazing person
Dialogue: 0,0:30:08.14,0:30:13.82,csapp,,0,0,0,, anyways the idea quicksort is very simple and this is sort of the basic sorting algorithm 
Dialogue: 0,0:30:13.84,0:30:18.24,csapp,,0,0,0,,you grab some element from the array that you're trying to sort
Dialogue: 0,0:30:18.72,0:30:19.68,csapp,,0,0,0,, that's called the pivot
Dialogue: 0,0:30:20.42,0:30:27.68,csapp,,0,0,0,, and then you split the data so that you look at the elements that are either greater or less than the pivot 
Dialogue: 0,0:30:28.10,0:30:32.02,csapp,,0,0,0,,and potentially also equal let's just assume all the elements are unique here 
Dialogue: 0,0:30:32.56,0:30:36.94,csapp,,0,0,0,,so you just split it into two piles one is the lesson once a greater 
Dialogue: 0,0:30:37.42,0:30:41.12,csapp,,0,0,0,,now you creatively you recursively you sort those two piles 
Dialogue: 0,0:30:42.50,0:30:46.42,csapp,,0,0,0,,by the same method and when it's all done you end up with everything sorted
Dialogue: 0,0:30:46.86,0:30:51.00,csapp,,0,0,0,, one nice thing about it is it can be done in place meaning if you have an array of data 
Dialogue: 0,0:30:51.44,0:30:57.36,csapp,,0,0,0,, you can do this all just by swapping elements around and not have to use any extra space
Dialogue: 0,0:30:57.50,0:30:59.44,csapp,,0,0,0,, which you would for example with merge sort 
Dialogue: 0,0:31:00.14,0:31:05.28,csapp,,0,0,0,,so this is a fairly simple algorithm and just to visualize it then
Dialogue: 0,0:31:06.06,0:31:09.62,csapp,,0,0,0,, you have some block of data X array and you want to sort it
Dialogue: 0,0:31:09.70,0:31:13.70,csapp,,0,0,0,, so you pick an element called the pivot and there's various strategies for doing that 
Dialogue: 0,0:31:14.88,0:31:19.42,csapp,,0,0,0,,and now you just subdivide X into three parts 
Dialogue: 0,0:31:19.56,0:31:24.52,csapp,,0,0,0,,L the left hand R the right hand meaning less and greater than P 
Dialogue: 0,0:31:25.06,0:31:26.82,csapp,,0,0,0,,and then you place P in the middle 
Dialogue: 0,0:31:27.78,0:31:33.48,csapp,,0,0,0,,and then you recursively when you're doing this for in a sequential code 
Dialogue: 0,0:31:33.48,0:31:39.03,csapp,,0,0,0,,you'll pick one of these two usually leftmost or rightmost whatever doesn't really matter 
Dialogue: 0,0:31:39.40,0:31:46.57,csapp,,0,0,0,,and you'll recursively you apply the same method to to the left side and
Dialogue: 0,0:31:46.86,0:31:51.96,csapp,,0,0,0,,and ultimately after enough recursions you get to the point where L has been sorted
Dialogue: 0,0:31:52.24,0:31:55.22,csapp,,0,0,0,,and that's shown in this kind of a swishy color thing 
Dialogue: 0,0:31:55.76,0:31:57.04,csapp,,0,0,0,,and call that L
Dialogue: 0,0:31:57.98,0:31:59.30,csapp,,0,0,0,,and same with 
Dialogue: 0,0:32:03.86,0:32:07.16,csapp,,0,0,0,,you'll do the same thing now with the right hand side 
Dialogue: 0,0:32:07.62,0:32:10.14,csapp,,0,0,0,,and when you're done this is usually done in place 
Dialogue: 0,0:32:10.28,0:32:16.52,csapp,,0,0,0,,so you just the L part works on one array part of the array and the are part on another 
Dialogue: 0,0:32:16.94,0:32:18.38,csapp,,0,0,0,,and when you're done they're in sorted order 
Dialogue: 0,0:32:20.26,0:32:23.18,csapp,,0,0,0,,the very simple sort and generally has very good performance 
Dialogue: 0,0:32:24.64,0:32:30.54,csapp,,0,0,0,,so this is what the code for it looks like which is usually you have is a special case 
Dialogue: 0,0:32:30.54,0:32:31.94,csapp,,0,0,0,,if there's only one or two elements 
Dialogue: 0,0:32:33.04,0:32:35.28,csapp,,0,0,0,,and then you do this partitioning 
Dialogue: 0,0:32:35.28,0:32:41.50,csapp,,0,0,0,,so this routine of splitting it between the left and the right hand part is handled by a function called partition 
Dialogue: 0,0:32:42.36,0:32:46.22,csapp,,0,0,0,,and then if there's more than one element in the left side
Dialogue: 0,0:32:46.22,0:32:51.92,csapp,,0,0,0,,you sort that and if there's more than one element in the right hand side you sort that 
Dialogue: 0,0:32:53.68,0:32:58.66,csapp,,0,0,0,,and then when all these recursions are done then the array is sorted 
Dialogue: 0,0:32:59.30,0:33:04.04,csapp,,0,0,0,,so pretty typical code and we won't go in the trickiest part writing the code is 
Dialogue: 0,0:33:04.12,0:33:06.96,csapp,,0,0,0,,how do you make this partitioning go fast 
Dialogue: 0,0:33:07.70,0:33:10.18,csapp,,0,0,0,,but I won't go into that just imagine it happens 
Dialogue: 0,0:33:11.74,0:33:15.90,csapp,,0,0,0,,so this algorithm actually has a natural version of parallelism 
Dialogue: 0,0:33:16.28,0:33:20.58,csapp,,0,0,0,,which is in my sequential version I was sorting both first the left 
Dialogue: 0,0:33:20.72,0:33:24.98,csapp,,0,0,0,,and then the left of the left and the left of the left of the left and kind of working my way 
Dialogue: 0,0:33:25.40,0:33:27.50,csapp,,0,0,0,,until I got that whole array sorted 
Dialogue: 0,0:33:27.78,0:33:30.44,csapp,,0,0,0,,and then I was coming back and I was working on the right 
Dialogue: 0,0:33:30.44,0:33:33.82,csapp,,0,0,0,,and then the left part of the right and the left to the left to the right and blah blah blah
Dialogue: 0,0:33:34.44,0:33:37.54,csapp,,0,0,0,,and doing these recursions because the way the codes written 
Dialogue: 0,0:33:40.18,0:33:44.50,csapp,,0,0,0,,right I am doing the full sort of the left-hand part 
Dialogue: 0,0:33:45.16,0:33:49.60,csapp,,0,0,0,,and only after that is sorted then I'm doing the complete sort of the right-hand part
Dialogue: 0,0:33:49.78,0:33:56.94,csapp,,0,0,0,,so the point is it's it's an algorithm where I'm working just on one part of the array of the time
Dialogue: 0,0:33:58.10,0:34:01.86,csapp,,0,0,0,, but there's a very natural recursion of parallelism here that says 
Dialogue: 0,0:34:02.30,0:34:05.84,csapp,,0,0,0,,okay I've got two parts they each need to be sorted
Dialogue: 0,0:34:06.28,0:34:10.76,csapp,,0,0,0,,let me just fire off two threads and let them do with that 
Dialogue: 0,0:34:12.30,0:34:16.54,csapp,,0,0,0,,and that's the so it's what you call divide and conquer parallelism
Dialogue: 0,0:34:16.60,0:34:20.38,csapp,,0,0,0,,it's a very natural kind of parallelism it shows up in this code 
Dialogue: 0,0:34:23.40,0:34:26.68,csapp,,0,0,0,,so basically I'll do the same thing as before all at
Dialogue: 0,0:34:26.90,0:34:31.44,csapp,,0,0,0,,the top level will be a purely sequential process of partitioning 
Dialogue: 0,0:34:32.60,0:34:38.50,csapp,,0,0,0,,and but then assuming a partition comes up with some non-trivial split 
Dialogue: 0,0:34:39.08,0:34:44.28,csapp,,0,0,0,,then I will recursively begin fork off to threads
Dialogue: 0,0:34:44.94,0:34:47.40,csapp,,0,0,0,,each of which will be responsible for the other and
Dialogue: 0,0:34:47.40,0:34:51.44,csapp,,0,0,0,, so we'll sort of look like I'm working on the two parts in parallel
Dialogue: 0,0:34:52.26,0:34:54.68,csapp,,0,0,0,,and eventually both sides will end up
Dialogue: 0,0:34:55.18,0:34:59.08,csapp,,0,0,0,,now this picture isn't really quite true and that it looked makes it look like
Dialogue: 0,0:34:59.52,0:35:05.44,csapp,,0,0,0,,they're all kind of synchronized together and I'm doing a you know kaboom down like this 
Dialogue: 0,0:35:05.66,0:35:09.02,csapp,,0,0,0,,in a strict way but in fact they're not it's very asynchronous
Dialogue: 0,0:35:09.66,0:35:12.50,csapp,,0,0,0,,the left part is one thread the right is another 
Dialogue: 0,0:35:13.06,0:35:19.30,csapp,,0,0,0,,they just go at their own pace and at the end I'm just going to wait for it all to complete 
Dialogue: 0,0:35:19.74,0:35:23.70,csapp,,0,0,0,,but there's no strict temporal ordering on how that all occurs
Dialogue: 0,0:35:28.28,0:35:33.60,csapp,,0,0,0,,so the way I'll write this in the code is available on the course website 
Dialogue: 0,0:35:33.60,0:35:35.36,csapp,,0,0,0,,I'm only going to show you a glimpse of it
Dialogue: 0,0:35:36.02,0:35:38.40,csapp,,0,0,0,,it's a non-trivial amount of code it takes to do it 
Dialogue: 0,0:35:38.88,0:35:43.36,csapp,,0,0,0,, but basically what I'm going to do is have a bunch of a pool of threads 
Dialogue: 0,0:35:43.98,0:35:48.86,csapp,,0,0,0,,that are ready to work and that's a pretty typical way you write threaded code 
Dialogue: 0,0:35:49.60,0:35:56.82,csapp,,0,0,0,,because actually the the initiation of a thread is a non-trivial amount of computation 
Dialogue: 0,0:35:56.88,0:35:58.16,csapp,,0,0,0,,so usually what you do is 
Dialogue: 0,0:35:58.56,0:36:01.18,csapp,,0,0,0,,you say I've got this many cores 
Dialogue: 0,0:36:01.68,0:36:05.76,csapp,,0,0,0,,I'm going to create a set of that many threads 
Dialogue: 0,0:36:06.58,0:36:10.95,csapp,,0,0,0,,and they will each work by sharing a task queue
Dialogue: 0,0:36:11.52,0:36:19.34,csapp,,0,0,0,,so some agent that is a forking off work for the different threads to do 
Dialogue: 0,0:36:19.70,0:36:25.38,csapp,,0,0,0,,they will do the work assigned to them when that's complete they'll come back and say ok I'm ready for something new
Dialogue: 0,0:36:25.68,0:36:27.02,csapp,,0,0,0,, and it will give them something new 
Dialogue: 0,0:36:27.16,0:36:34.54,csapp,,0,0,0,,so I've there's a little bit of code a very rudimentary code there of creating this task model and task scheduling
Dialogue: 0,0:36:35.52,0:36:38.08,csapp,,0,0,0,, so the basic rule will be any given task 
Dialogue: 0,0:36:39.02,0:36:46.68,csapp,,0,0,0,, any given thread then at any given time has been assigned sub sub range of this array  to be a sorting 
Dialogue: 0,0:36:47.28,0:36:55.66,csapp,,0,0,0,,and it will be specified by the base meaning the starting point of this particular range
Dialogue: 0,0:36:56.26,0:37:01.26,csapp,,0,0,0,,and then the number of elements that it's a told it's sort 
Dialogue: 0,0:37:03.74,0:37:05.70,csapp,,0,0,0,,and one other thing I'll do is 
Dialogue: 0,0:37:06.04,0:37:10.32,csapp,,0,0,0,,once I get down to this array being small enough 
Dialogue: 0,0:37:10.82,0:37:12.90,csapp,,0,0,0,,I'll just sort it sequentially 
Dialogue: 0,0:37:13.60,0:37:18.86,csapp,,0,0,0,,and and we'll see how big that block is or not is actually performance 
Dialogue: 0,0:37:19.00,0:37:20.96,csapp,,0,0,0,,parameter that you can use for tuning the program 
Dialogue: 0,0:37:21.82,0:37:25.32,csapp,,0,0,0,,so the point is that you don't want to take this down too far 
Dialogue: 0,0:37:25.34,0:37:29.60,csapp,,0,0,0,,because the the sort of overhead of threads is enough that 
Dialogue: 0,0:37:29.98,0:37:33.00,csapp,,0,0,0,,when you get to fine-grained you're actually going to start losing performance
Dialogue: 0,0:37:35.38,0:37:39.48,csapp,,0,0,0,,so assume that it's bigger than that I've been given some block there 
Dialogue: 0,0:37:40.04,0:37:43.78,csapp,,0,0,0,,what I'll do then is I'll run the partition step
Dialogue: 0,0:37:44.12,0:37:50.50,csapp,,0,0,0,, this thread will run it a partition step just using the exact function  I showed you or didn't show you 
Dialogue: 0,0:37:52.74,0:38:00.00,csapp,,0,0,0,,and then as long as and then I will us create and add to the task queue two new tasks 
Dialogue: 0,0:38:00.84,0:38:05.06,csapp,,0,0,0,,a one de for the left part and one for the right part 
Dialogue: 0,0:38:05.78,0:38:13.50,csapp,,0,0,0,,and then the scheduler that will assign two threads to handle those two parts 
Dialogue: 0,0:38:13.68,0:38:16.52,csapp,,0,0,0,,and so that code will that's exactly how the code is going to work
Dialogue: 0,0:38:16.66,0:38:21.78,csapp,,0,0,0,,it's going to keep reusing the same threads over and over again
Dialogue: 0,0:38:22.56,0:38:27.02,csapp,,0,0,0,, to but at any given time they'll be given a range of places 
Dialogue: 0,0:38:27.86,0:38:30.88,csapp,,0,0,0,,what typically will happen is they'll run their partitioning step 
Dialogue: 0,0:38:32.42,0:38:38.50,csapp,,0,0,0,,and then say okay I've done my job now assigned to new to new threads to do this 
Dialogue: 0,0:38:38.68,0:38:42.40,csapp,,0,0,0,,and that's the general scheme of it or they'll say this is a small enough block 
Dialogue: 0,0:38:43.26,0:38:45.72,csapp,,0,0,0,,I'm just going to sort the darn thing 
Dialogue: 0,0:38:45.76,0:38:48.16,csapp,,0,0,0,,okay so that's really all the code does it's online 
Dialogue: 0,0:38:48.68,0:38:54.94,csapp,,0,0,0,,if you're interested in this kind of stuff it's um I think it's pretty well written code because I wrote
Dialogue: 0,0:38:57.62,0:39:00.52,csapp,,0,0,0,,it so this is sort of that the 
Dialogue: 0,0:39:01.50,0:39:04.66,csapp,,0,0,0,,somewhat simplified version of the code say
Dialogue: 0,0:39:05.20,0:39:09.42,csapp,,0,0,0,,initialize my task queue scheduling system 
Dialogue: 0,0:39:10.88,0:39:20.30,csapp,,0,0,0,,create global variables of describing the beginning and end of this array to be sorted create a new task queue 
Dialogue: 0,0:39:20.82,0:39:24.14,csapp,,0,0,0,,and then this is the main function the TQ sort helper
Dialogue: 0,0:39:25.00,0:39:31.68,csapp,,0,0,0,, is given some range of  of addresses and a pointer to the task queue
Dialogue: 0,0:39:31.86,0:39:34.72,csapp,,0,0,0,, that's used to manage these tasks 
Dialogue: 0,0:39:35.64,0:39:36.82,csapp,,0,0,0,,and then uh
Dialogue: 0,0:39:40.38,0:39:45.44,csapp,,0,0,0,,when it's all day we'll just wait till all the tasks have completed this is at the top level
Dialogue: 0,0:39:46.04,0:39:49.70,csapp,,0,0,0,,this isn't part of any recursion this is the top level code 
Dialogue: 0,0:39:50.08,0:39:51.84,csapp,,0,0,0,,and then it will free up the data structures 
Dialogue: 0,0:39:54.24,0:39:58.06,csapp,,0,0,0,,and then this is the the part of it that actually does the real work
Dialogue: 0,0:39:59.00,0:40:04.82,csapp,,0,0,0,, it will say so now the TQ sort helper is the part
Dialogue: 0,0:40:04.94,0:40:11.62,csapp,,0,0,0,, that's a scientists to sort some particulars poises from
Dialogue: 0,0:40:12.36,0:40:17.16,csapp,,0,0,0,, the the starting address the base and some number of elements 
Dialogue: 0,0:40:19.06,0:40:22.12,csapp,,0,0,0,,and so this is what each task will do 
Dialogue: 0,0:40:22.46,0:40:26.68,csapp,,0,0,0,,and it says okay if this is a small enough block of elements 
Dialogue: 0,0:40:27.16,0:40:30.42,csapp,,0,0,0,,I'm just going to call my serial quicksort to do it 
Dialogue: 0,0:40:31.66,0:40:34.90,csapp,,0,0,0,,otherwise I'm going to
Dialogue: 0,0:40:45.44,0:40:47.24,csapp,,0,0,0,,oh it's a little bit Messier this okay
Dialogue: 0,0:40:47.86,0:40:53.56,csapp,,0,0,0,,now otherwise it's going to spawn a task to do the sorting 
Dialogue: 0,0:40:54.04,0:40:54.80,csapp,,0,0,0,,let's see 
Dialogue: 0,0:40:55.32,0:40:57.90,csapp,,0,0,0,,I was a little mixed up this is a high level 
Dialogue: 0,0:40:57.90,0:41:04.22,csapp,,0,0,0,, so the actual splitting occurs in this thing which is where it 
Dialogue: 0,0:41:06.42,0:41:08.28,csapp,,0,0,0,, so this is the actual thread routine 
Dialogue: 0,0:41:09.68,0:41:14.20,csapp,,0,0,0,,and what it's saying is run the partition here
Dialogue: 0,0:41:15.36,0:41:20.98,csapp,,0,0,0,, and then call this TQ sort helper which you just saw on the left and the right parts
Dialogue: 0,0:41:25.00,0:41:31.36,csapp,,0,0,0,,of it so just to review then the actual spawning of a task is done by this helper routine 
Dialogue: 0,0:41:32.10,0:41:37.02,csapp,,0,0,0,,but then that what it calls is the the thread routine is 
Dialogue: 0,0:41:38.18,0:41:40.70,csapp,,0,0,0,,what does the work here and what it will do is
Dialogue: 0,0:41:41.00,0:41:43.16,csapp,,0,0,0,, it will do the partitioning within that thread 
Dialogue: 0,0:41:43.60,0:41:52.06,csapp,,0,0,0,,and then it will just throw back and add to the task queue two calls to this helper 
Dialogue: 0,0:41:53.82,0:42:00.16,csapp,,0,0,0,,but as that kind of so between these two routines you can see it's doing this idea of a divide and conquer parallelism 
Dialogue: 0,0:42:02.00,0:42:06.12,csapp,,0,0,0,,so this is a performance running on the shark machines 
Dialogue: 0,0:42:07.42,0:42:18.24,csapp,,0,0,0,,and this is a fairly straightforward I'm just taking some number of random value two to the thirty-seventh
Dialogue: 0,0:42:22.52,0:42:23.42,csapp,,0,0,0,,that can't be right
Dialogue: 0,0:42:25.56,0:42:29.56,csapp,,0,0,0,,finish this is numbers not to the thirty-seventh right you agree with me
Dialogue: 0,0:42:31.04,0:42:34.28,csapp,,0,0,0,,to the 37th is 128 billion roughly 
Dialogue: 0,0:42:34.52,0:42:37.46,csapp,,0,0,0,,so this number is not right I'll have to check it out
Dialogue: 0,0:42:39.40,0:42:43.94,csapp,,0,0,0,,and now what this x axis 
Dialogue: 0,0:42:44.00,0:42:47.20,csapp,,0,0,0,,so the y axis just denotes how long does it take to complete 
Dialogue: 0,0:42:47.82,0:42:52.26,csapp,,0,0,0,,by the way one thing if you're used to measuring performance based on CPU time 
Dialogue: 0,0:42:52.92,0:42:57.52,csapp,,0,0,0,,that's not useful when you're talking a parallel computing you really want to talk a elapsed time 
Dialogue: 0,0:42:57.56,0:43:04.52,csapp,,0,0,0,,the time that you'd get from looking at a clock  and measuring it and dealing with whatever inefficiency occurs
Dialogue: 0,0:43:04.98,0:43:08.66,csapp,,0,0,0,,  there so these are actually the elapsed run time of the entire program
Dialogue: 0,0:43:11.04,0:43:14.78,csapp,,0,0,0,, and you'll see that it varies according to this thing called the serial fraction 
Dialogue: 0,0:43:15.54,0:43:24.64,csapp,,0,0,0,,the serial fraction is just at what point do I slide between a serial quicksort  or keep dividing 
Dialogue: 0,0:43:24.80,0:43:26.80,csapp,,0,0,0,,so how big does the array need to be 
Dialogue: 0,0:43:27.20,0:43:30.22,csapp,,0,0,0,,as a fraction expressed as a fraction of the original array 
Dialogue: 0,0:43:31.52,0:43:36.04,csapp,,0,0,0,,before I I will go into recursion 
Dialogue: 0,0:43:36.14,0:43:40.06,csapp,,0,0,0,,if I were actually to write this a real application I wouldn't do it based on a fraction
Dialogue: 0,0:43:40.14,0:43:47.00,csapp,,0,0,0,, I'd do it based on a block size to say  anything smaller than a thousand elements or some number like that 
Dialogue: 0,0:43:47.18,0:43:51.08,csapp,,0,0,0,,but this code just happens to be expressed this way 
Dialogue: 0,0:43:51.32,0:43:54.64,csapp,,0,0,0,,but the thing to notice that's interesting is you'll see here 
Dialogue: 0,0:43:57.08,0:44:01.14,csapp,,0,0,0,,if the fraction is 1 it basically says I won't split this at all 
Dialogue: 0,0:44:01.14,0:44:04.10,csapp,,0,0,0,,I'm just going to call a sequential quicksort 
Dialogue: 0,0:44:04.24,0:44:07.00,csapp,,0,0,0,,so this is a purely sequential version of it 
Dialogue: 0,0:44:07.56,0:44:10.80,csapp,,0,0,0,,and what it shows is if I once I start - 
Dialogue: 0,0:44:11.48,0:44:15.43,csapp,,0,0,0,,I'll be willing to sort of split this up and do parallelism 
Dialogue: 0,0:44:15.92,0:44:18.92,csapp,,0,0,0,,I start making run faster and faster and faster 
Dialogue: 0,0:44:20.14,0:44:22.24,csapp,,0,0,0,,I'll and then I get into this trough
Dialogue: 0,0:44:22.94,0:44:27.76,csapp,,0,0,0,, and now if I start going finer and finer grain then I'm running into the problem 
Dialogue: 0,0:44:27.76,0:44:34.12,csapp,,0,0,0,,where the thread overhead is more than the advantage I'm getting by doing the parallelism 
Dialogue: 0,0:44:34.26,0:44:41.54,csapp,,0,0,0,,and I'm faster to run that block bigger sort just use a sequential algorithm rather than parallel 
Dialogue: 0,0:44:43.96,0:44:47.40,csapp,,0,0,0,,but the good news here is this is a pretty long trough here 
Dialogue: 0,0:44:47.64,0:44:50.72,csapp,,0,0,0,,so it means is if you're trying to tune this program
Dialogue: 0,0:44:51.74,0:44:57.40,csapp,,0,0,0,,it's not that hard you're not going to pay a huge penalty if you don't know a parameter exactly 
Dialogue: 0,0:44:58.04,0:45:04.26,csapp,,0,0,0,,so as long as this is a huge range right 30 from a 32 to 4096
Dialogue: 0,0:45:04.28,0:45:15.34,csapp,,0,0,0,,it's a factor of of a lot to the fifth and 2 to the 12th is 2 to the 7th fact 128 
Dialogue: 0,0:45:16.12,0:45:19.04,csapp,,0,0,0,,see how I do my arithmetic and powers at 2 
Dialogue: 0,0:45:19.42,0:45:27.15,csapp,,0,0,0,,anyways it's roughly at you know 128 so several orders of magnitude decimal orders of magnitude over which
Dialogue: 0,0:45:27.70,0:45:29.38,csapp,,0,0,0,,you get pretty comparable performance 
Dialogue: 0,0:45:29.54,0:45:33.88,csapp,,0,0,0,,so that means from a performance tuning point of view it's not that hard to do
Dialogue: 0,0:45:34.62,0:45:37.78,csapp,,0,0,0,,and you also see we're getting a pretty decent speed up 
Dialogue: 0,0:45:38.72,0:45:41.74,csapp,,0,0,0,, on our eight core to a hyper-threaded machine 
Dialogue: 0,0:45:41.80,0:45:44.80,csapp,,0,0,0,,we're getting up basically out 7x performance 
Dialogue: 0,0:45:45.62,0:45:50.60,csapp,,0,0,0,,and hyper-threading really isn't helping us at all is part of the lesson here 
Dialogue: 0,0:45:51.24,0:45:53.86,csapp,,0,0,0,,but if you just think of it as eight cores then that's pretty good
Dialogue: 0,0:45:59.28,0:46:05.78,csapp,,0,0,0,,so there is an obvious place here where there's  a Nam Dolls law issue going on
Dialogue: 0,0:46:05.80,0:46:08.26,csapp,,0,0,0,,if you think at that first top-level split 
Dialogue: 0,0:46:09.32,0:46:14.66,csapp,,0,0,0,,the first call to partition is being done over the entire Ray by a serial sequential process
Dialogue: 0,0:46:15.60,0:46:16.50,csapp,,0,0,0,, right that 
Dialogue: 0,0:46:16.82,0:46:24.38,csapp,,0,0,0,,so at the very least that is not going parallel at all there's going exactly one thread is doing the initial partition 
Dialogue: 0,0:46:25.72,0:46:30.64,csapp,,0,0,0,,and then that splits into two so at most you have two threads worth of parallelism 
Dialogue: 0,0:46:31.24,0:46:33.38,csapp,,0,0,0,,and then the next level down at most four 
Dialogue: 0,0:46:33.42,0:46:37.48,csapp,,0,0,0,,and so you really don't you have to get several levels of recursion down 
Dialogue: 0,0:46:38.00,0:46:41.42,csapp,,0,0,0,,before you're really running on all the course that you have available 
Dialogue: 0,0:46:41.70,0:46:45.06,csapp,,0,0,0,,so you'd think that that's limiting your speed up and
Dialogue: 0,0:46:45.06,0:46:49.22,csapp,,0,0,0,, it does and that's part of the reason why our best performance is 
Dialogue: 0,0:46:50.98,0:46:53.32,csapp,,0,0,0,,a factor of seven and not a factor of eight or more 
Dialogue: 0,0:46:59.10,0:47:09.30,csapp,,0,0,0,,so there's quite a bit of work as I mentioned on how to speed up performance including how to make quicksort go faster 
Dialogue: 0,0:47:12.00,0:47:15.62,csapp,,0,0,0,,so there's a vast body of literature on parallel sort
Dialogue: 0,0:47:19.92,0:47:24.88,csapp,,0,0,0,,so one thing I tried was to say okay well let's try and do this partitioning
Dialogue: 0,0:47:27.94,0:47:32.70,csapp,,0,0,0,, step they least the top couple levels let's try and do a parallel version of that and 
Dialogue: 0,0:47:33.34,0:47:36.16,csapp,,0,0,0,,so the idea is you pick one pivot element 
Dialogue: 0,0:47:36.38,0:47:39.24,csapp,,0,0,0,,but now you fire in this example for threads 
Dialogue: 0,0:47:40.42,0:47:47.18,csapp,,0,0,0,,and each of those four threads runs a partition step on on one-fourth of the range 
Dialogue: 0,0:47:48.26,0:47:51.12,csapp,,0,0,0,,and it don't generate their own versions of left and right 
Dialogue: 0,0:47:52.26,0:47:59.12,csapp,,0,0,0,,and then you globally figure out how many are in each of these sub ranges
Dialogue: 0,0:48:00.46,0:48:07.00,csapp,,0,0,0,,and then you tell each thread okay now you copy your part of it over to the relevant section of the array 
Dialogue: 0,0:48:09.60,0:48:13.78,csapp,,0,0,0,,but the good news so there's some amount of synchronization that goes on there 
Dialogue: 0,0:48:14.48,0:48:18.14,csapp,,0,0,0,, but you can imagine that this partitioning step 
Dialogue: 0,0:48:18.44,0:48:23.78,csapp,,0,0,0,, once when you're running it is completely independent of across the different threads 
Dialogue: 0,0:48:23.82,0:48:26.52,csapp,,0,0,0,,so it's getting a almost ideal speed-up 
Dialogue: 0,0:48:27.62,0:48:32.90,csapp,,0,0,0,,so I implemented this and tried and I couldn't make it run faster than the original code 
Dialogue: 0,0:48:33.72,0:48:39.68,csapp,,0,0,0,,and I think the the problem with this was  the copying the cost of copying data 
Dialogue: 0,0:48:41.96,0:48:47.80,csapp,,0,0,0,, here was even though it's being done by multiple threads 
Dialogue: 0,0:48:48.30,0:48:51.24,csapp,,0,0,0,,and getting pretty good performance out of the memory system
Dialogue: 0,0:48:51.26,0:48:56.12,csapp,,0,0,0,,because you're doing sequential you know all the cache issues are pretty good here 
Dialogue: 0,0:48:56.80,0:48:58.94,csapp,,0,0,0,,but that's just enough extra work
Dialogue: 0,0:48:59.52,0:49:02.54,csapp,,0,0,0,,that has to be done for this parallel code that doesn't have to be done
Dialogue: 0,0:49:02.86,0:49:04.88,csapp,,0,0,0,, the sequential code is totally in place 
Dialogue: 0,0:49:05.82,0:49:08.96,csapp,,0,0,0,,meaning not using any additional storage not copying 
Dialogue: 0,0:49:09.34,0:49:13.12,csapp,,0,0,0,,and so that's just enough of a penalty on the parallel part 
Dialogue: 0,0:49:13.56,0:49:16.04,csapp,,0,0,0,,that it didn't really improve performance at all
Dialogue: 0,0:49:16.56,0:49:20.54,csapp,,0,0,0,,so that code is shown as part of the code on the course website
Dialogue: 0,0:49:20.68,0:49:25.70,csapp,,0,0,0,, but like I said I I banged on it quite a bit and trying to tune it 
Dialogue: 0,0:49:25.82,0:49:27.90,csapp,,0,0,0,,and squeak it in various ways and could never make it 
Dialogue: 0,0:49:28.46,0:49:31.54,csapp,,0,0,0,,so I got better overall performance out of this program
Dialogue: 0,0:49:33.34,0:49:38.70,csapp,,0,0,0,,and so that's again a lesson and that's one of the unfortunate lessons is you can spend a lot of time 
Dialogue: 0,0:49:40.16,0:49:43.64,csapp,,0,0,0,,trying to make a program run faster  and get absolutely nowhere 
Dialogue: 0,0:49:44.72,0:49:47.34,csapp,,0,0,0,,and it's frustrating because you put in a lot of work 
Dialogue: 0,0:49:47.90,0:49:52.72,csapp,,0,0,0,,and you know it's a pretty cool idea and you'd love to publish a paper about it or tell your friends about it 
Dialogue: 0,0:49:53.02,0:49:55.64,csapp,,0,0,0,,and it just goes nowhere and it just sits there 
Dialogue: 0,0:49:55.98,0:50:02.20,csapp,,0,0,0,,there's nothing unfortunately there's not an accumulated repository of the bad ideas of computer science 
Dialogue: 0,0:50:02.30,0:50:03.70,csapp,,0,0,0,,don't waste your time trying this 
Dialogue: 0,0:50:04.30,0:50:06.30,csapp,,0,0,0,,that people can talk about so
Dialogue: 0,0:50:06.88,0:50:09.18,csapp,,0,0,0,, this is just a lesson to learn 
Dialogue: 0,0:50:11.90,0:50:15.34,csapp,,0,0,0,,so anyways that was my experience with that 
Dialogue: 0,0:50:15.64,0:50:21.78,csapp,,0,0,0,, again other people have spent a lot more time this is one of the most common applications
Dialogue: 0,0:50:22.10,0:50:24.52,csapp,,0,0,0,, that people try to do parallel programming for 
Dialogue: 0,0:50:26.68,0:50:30.78,csapp,,0,0,0,,so some of the lessons from this is you need a good strategy
Dialogue: 0,0:50:30.80,0:50:33.42,csapp,,0,0,0,, for how you're going to get parallelism out of your application 
Dialogue: 0,0:50:34.10,0:50:38.94,csapp,,0,0,0,,and I showed you two basic versions one is partitioned into K parts
Dialogue: 0,0:50:39.00,0:50:42.26,csapp,,0,0,0,, they're more or less completely independent of each other 
Dialogue: 0,0:50:42.78,0:50:46.78,csapp,,0,0,0,,or something like a divide and conquer strategy where you can keep splitting it 
Dialogue: 0,0:50:46.90,0:50:50.92,csapp,,0,0,0,,but the two splits that you create out of that can go concurrently
Dialogue: 0,0:50:51.78,0:50:54.98,csapp,,0,0,0,,these other different types of parallelism - 
Dialogue: 0,0:50:55.60,0:51:00.06,csapp,,0,0,0,,in general you want to make the inner loops you can't have any synchronization primitives 
Dialogue: 0,0:51:00.60,0:51:02.56,csapp,,0,0,0,,in there it will just run too slow 
Dialogue: 0,0:51:03.28,0:51:07.90,csapp,,0,0,0,,um dolls law as I mentioned is always sort of lurking in the background of 
Dialogue: 0,0:51:08.00,0:51:13.36,csapp,,0,0,0,,if you can always feed up a part of your program then the other part will become the bottleneck 
Dialogue: 0,0:51:14.26,0:51:17.81,csapp,,0,0,0,,but the other thing is like I said you can do it
Dialogue: 0,0:51:17.81,0:51:22.50,csapp,,0,0,0,,you've got the tools you've learned with with P threads 
Dialogue: 0,0:51:22.92,0:51:27.22,csapp,,0,0,0,,and your knowledge of programming and your understanding of cache memories and things like that
Dialogue: 0,0:51:27.74,0:51:32.80,csapp,,0,0,0,, you've got the tools you need to be an effective programmer of this kind of thing 
Dialogue: 0,0:51:33.94,0:51:41.36,csapp,,0,0,0,,but you have to and there's nothing that beats sort of trial and error and testing and tuning experimenting 
Dialogue: 0,0:51:41.78,0:51:44.16,csapp,,0,0,0,,if there's some parameters that need to be set
Dialogue: 0,0:51:44.32,0:51:48.28,csapp,,0,0,0,, then you want to run experiments will sweep through the parameters to try and figure out 
Dialogue: 0,0:51:48.88,0:51:50.14,csapp,,0,0,0,,what the setting should be 
Dialogue: 0,0:51:52.08,0:51:54.76,csapp,,0,0,0,,so that's sort of a little bit about parallel programming
Dialogue: 0,0:51:55.16,0:52:01.50,csapp,,0,0,0,, let me just finish this lecture with a little bit of sort of classic issues about concurrency 
Dialogue: 0,0:52:01.62,0:52:05.32,csapp,,0,0,0,,that that are critical when you're dealing with
Dialogue: 0,0:52:06.50,0:52:11.14,csapp,,0,0,0,, these systems based on what you call a shared memory model of computation 
Dialogue: 0,0:52:11.76,0:52:18.00,csapp,,0,0,0,,so multi-core is an example of conceptually multi-threaded computation remember
Dialogue: 0,0:52:18.02,0:52:21.38,csapp,,0,0,0,,  you're you're working within a single virtual address space 
Dialogue: 0,0:52:22.48,0:52:24.68,csapp,,0,0,0,,and you have private stacks 
Dialogue: 0,0:52:25.18,0:52:30.26,csapp,,0,0,0,,but the more global the heap memory is completely shared across threads 
Dialogue: 0,0:52:31.04,0:52:35.06,csapp,,0,0,0,, and so that's what you call the shared memory programming model 
Dialogue: 0,0:52:35.18,0:52:36.88,csapp,,0,0,0,,and that's what we've really been looking at this course 
Dialogue: 0,0:52:38.76,0:52:42.97,csapp,,0,0,0,,so there's a certain interesting question about called memory consistency models
Dialogue: 0,0:52:43.94,0:52:46.58,csapp,,0,0,0,,and here I'll illustrate it with a very simple example
Dialogue: 0,0:52:47.12,0:52:50.78,csapp,,0,0,0,, imagine we have two global variables a and B
Dialogue: 0,0:52:51.02,0:52:52.22,csapp,,0,0,0,, and we have two different threads 
Dialogue: 0,0:52:53.08,0:52:58.66,csapp,,0,0,0,,and so the first thread is going to write meaning assign a value to a 
Dialogue: 0,0:52:59.12,0:53:02.02,csapp,,0,0,0,,and it's going to read meaning print the value of B 
Dialogue: 0,0:53:02.72,0:53:08.84,csapp,,0,0,0,,and the other thread is going to do the opposite it's going to write assigned a value to B and print the value of a 
Dialogue: 0,0:53:09.40,0:53:14.20,csapp,,0,0,0,,and so now the question is what are the possible outputs for this program 
Dialogue: 0,0:53:15.52,0:53:21.12,csapp,,0,0,0,,and so there's a model that sort of the accepted standard called sequential consistency
Dialogue: 0,0:53:21.64,0:53:24.22,csapp,,0,0,0,,which means that these events can occur
Dialogue: 0,0:53:24.68,0:53:33.56,csapp,,0,0,0,,well that these that within a single thread things have to occur in the sequential order of that thread 
Dialogue: 0,0:53:34.52,0:53:40.98,csapp,,0,0,0,,but across threads whether write a a write B occurs first is completely arbitrary
Dialogue: 0,0:53:41.30,0:53:43.80,csapp,,0,0,0,, and similarly whether writing of B occurs
Dialogue: 0,0:53:44.56,0:53:52.20,csapp,,0,0,0,,between these two actions or before is all too arbitrary 
Dialogue: 0,0:53:52.28,0:53:56.68,csapp,,0,0,0,,so what what it means is you can take two different threads
Dialogue: 0,0:53:56.70,0:54:01.56,csapp,,0,0,0,,and you can interleave there their events  in anyway 
Dialogue: 0,0:54:02.26,0:54:06.90,csapp,,0,0,0,,but you should be able to pull out of that interleaving
Dialogue: 0,0:54:07.44,0:54:10.98,csapp,,0,0,0,,the sequential order of either of both of the two threads 
Dialogue: 0,0:54:12.48,0:54:16.54,csapp,,0,0,0,,so when you do that you end up you can enumerate in the example like this 
Dialogue: 0,0:54:17.00,0:54:19.26,csapp,,0,0,0,,all the possibilities you can say well look it 
Dialogue: 0,0:54:20.12,0:54:22.48,csapp,,0,0,0,,first is either going to be right a or right B
Dialogue: 0,0:54:23.40,0:54:24.60,csapp,,0,0,0,,let's pick right a 
Dialogue: 0,0:54:25.42,0:54:30.04,csapp,,0,0,0,,so now the next event will be either a read of B or write of B
Dialogue: 0,0:54:31.86,0:54:36.10,csapp,,0,0,0,,and then if I if I do write a write read B 
Dialogue: 0,0:54:36.78,0:54:38.42,csapp,,0,0,0,,then I've completed this thread
Dialogue: 0,0:54:39.04,0:54:44.44,csapp,,0,0,0,, and so now the only possibility is to write to B and read a and so forth 
Dialogue: 0,0:54:44.52,0:54:48.74,csapp,,0,0,0,,you work out all the possible things you get six different event orderings 
Dialogue: 0,0:54:49.52,0:54:55.46,csapp,,0,0,0,,and then what will be printed is well first of all whether you print beer before a 
Dialogue: 0,0:54:56.02,0:54:59.42,csapp,,0,0,0,,would depend on the relative ordering of those two threads 
Dialogue: 0,0:54:59.42,0:55:02.34,csapp,,0,0,0,,so that's shown I'm showing the B value in blue
Dialogue: 0,0:55:03.06,0:55:05.52,csapp,,0,0,0,, and the red value in in red the 
Dialogue: 0,0:55:06.00,0:55:07.54,csapp,,0,0,0,,I'm sorry the a value in red
Dialogue: 0,0:55:09.02,0:55:10.46,csapp,,0,0,0,,and you'll get these different
Dialogue: 0,0:55:11.28,0:55:14.88,csapp,,0,0,0,,possibilities these are all the six possible outputs of this program 
Dialogue: 0,0:55:15.54,0:55:22.88,csapp,,0,0,0,,but you'll see that there are two two other outputs one could imagine that won't arise 
Dialogue: 0,0:55:23.78,0:55:30.48,csapp,,0,0,0,,one is to print 101 in other words to have them both print 
Dialogue: 0,0:55:31.64,0:55:34.28,csapp,,0,0,0,,the original values of these two variables 
Dialogue: 0,0:55:34.88,0:55:36.62,csapp,,0,0,0,,and that's impossible because 
Dialogue: 0,0:55:37.74,0:55:39.86,csapp,,0,0,0,,I have to have done at least one right 
Dialogue: 0,0:55:42.26,0:55:45.84,csapp,,0,0,0,,before I can reach either of these two print statements right 
Dialogue: 0,0:55:46.28,0:55:50.44,csapp,,0,0,0,,so it's not possible for these to still be in their original values of
Dialogue: 0,0:55:51.70,0:55:53.62,csapp,,0,0,0,,when I hit these print statements 
Dialogue: 0,0:55:56.14,0:55:59.10,csapp,,0,0,0,,and whichever order I hit these two 
Dialogue: 0,0:55:59.12,0:56:01.06,csapp,,0,0,0,,so those two are impossible 
Dialogue: 0,0:56:01.36,0:56:04.48,csapp,,0,0,0,,so that's the idea of sequential consistency that there's 
Dialogue: 0,0:56:05.16,0:56:10.96,csapp,,0,0,0,,some very large number but of possible outputs of a program 
Dialogue: 0,0:56:11.42,0:56:14.40,csapp,,0,0,0,,but in any case they can't violate
Dialogue: 0,0:56:14.86,0:56:17.26,csapp,,0,0,0,, the ordering implied by the individual threads 
Dialogue: 0,0:56:19.44,0:56:23.36,csapp,,0,0,0,,so you'd say okay that seems like pretty obvious thing 
Dialogue: 0,0:56:24.66,0:56:29.16,csapp,,0,0,0,,but actually if you think from a hardware perspective it's not that trivial to make that happen 
Dialogue: 0,0:56:30.04,0:56:36.04,csapp,,0,0,0,,so let me just throw a show you a scenario of multi-core Hardware 
Dialogue: 0,0:56:36.58,0:56:38.70,csapp,,0,0,0,,that would violate sequential consistency 
Dialogue: 0,0:56:39.72,0:56:42.98,csapp,,0,0,0,,assume that each of our threads has its own private cache
Dialogue: 0,0:56:44.68,0:56:49.74,csapp,,0,0,0,,and so if I execute this statement 
Dialogue: 0,0:56:50.14,0:56:53.94,csapp,,0,0,0,,what I'll do is I will grab a copy of a from
Dialogue: 0,0:56:54.98,0:56:57.26,csapp,,0,0,0,,the main memory and bring it into my cache 
Dialogue: 0,0:56:57.82,0:56:59.92,csapp,,0,0,0,,and I will assign this new value to it 
Dialogue: 0,0:57:00.96,0:57:08.20,csapp,,0,0,0,,and similarly a thread to will grab a copy of its of B and and update that 
Dialogue: 0,0:57:09.68,0:57:13.98,csapp,,0,0,0,,and now if I do my two print statements
Dialogue: 0,0:57:14.54,0:57:18.02,csapp,,0,0,0,, if thread two picks up the value from the memory
Dialogue: 0,0:57:18.54,0:57:24.10,csapp,,0,0,0,, not knowing that thread one as a modified copy of that value  then it would naturally print one
Dialogue: 0,0:57:24.96,0:57:30.44,csapp,,0,0,0,,and similarly if if thread one picked up a copy of B from main memory it would print 100 
Dialogue: 0,0:57:30.54,0:57:36.90,csapp,,0,0,0,,so we'd see exactly this unallowable execution and the reason is
Dialogue: 0,0:57:37.48,0:57:42.36,csapp,,0,0,0,,because each of these threads have their own private copies of these variables
Dialogue: 0,0:57:42.84,0:57:45.40,csapp,,0,0,0,,and they're not properly synchronized
Dialogue: 0,0:57:46.18,0:57:50.90,csapp,,0,0,0,,but you could see in a hardware scenario it would be easy to build this hardware and make that mistake 
Dialogue: 0,0:57:52.30,0:57:55.92,csapp,,0,0,0,,so how does it work in a in a multi-core processor well
Dialogue: 0,0:57:56.46,0:57:59.26,csapp,,0,0,0,,they have a trick they call it Snoopy caches 
Dialogue: 0,0:58:00.34,0:58:05.72,csapp,,0,0,0,,and it's a little bit like the readers writers of synchronization that
Dialogue: 0,0:58:05.72,0:58:07.94,csapp,,0,0,0,,you're working on for your proxies 
Dialogue: 0,0:58:08.56,0:58:13.28,csapp,,0,0,0,,that you want to make it so that if everyone's just reading some shared value
Dialogue: 0,0:58:13.64,0:58:19.44,csapp,,0,0,0,, they should be able to get copies into their own caches  to optimize the performance of it 
Dialogue: 0,0:58:20.26,0:58:24.96,csapp,,0,0,0,,but if one of them wants to write to it it needs to get an exclusive copy of it
Dialogue: 0,0:58:25.62,0:58:27.58,csapp,,0,0,0,,and lock out any other thread 
Dialogue: 0,0:58:28.06,0:58:35.46,csapp,,0,0,0,,from accessing that either to read it or to write it from long enough to make the update 
Dialogue: 0,0:58:36.42,0:58:37.56,csapp,,0,0,0,,and so 
Dialogue: 0,0:58:40.12,0:58:48.16,csapp,,0,0,0,,they have a protocol where they tag actually and these tags are at the level of cache lines typically
Dialogue: 0,0:58:48.78,0:58:55.28,csapp,,0,0,0,, so the tagged cache line in main memory with its state and the typical state would be invalid 
Dialogue: 0,0:58:56.12,0:58:59.52,csapp,,0,0,0,,it's shared oryx its exclusive 
Dialogue: 0,0:59:00.14,0:59:06.74,csapp,,0,0,0,,so shared means that there can be a copies of it but they can only be read-only copies 
Dialogue: 0,0:59:07.34,0:59:14.08,csapp,,0,0,0,,and exclusive meaning that it's exclusively available to a single thread 
Dialogue: 0,0:59:15.84,0:59:19.36,csapp,,0,0,0,,so this is built into them the hardware of a multi-core processor
Dialogue: 0,0:59:20.10,0:59:30.18,csapp,,0,0,0,,so what will happen that is in order to do a write to a thread one will acquire an exclusive copy of this element 
Dialogue: 0,0:59:30.18,0:59:34.88,csapp,,0,0,0,,and that actually tagging happens down here at the main memory and in the cache both
Dialogue: 0,0:59:37.86,0:59:44.22,csapp,,0,0,0,,oh and similarly if if thread two wants a to write to B 
Dialogue: 0,0:59:44.52,0:59:46.72,csapp,,0,0,0,,it must get an exclusive copy of that 
Dialogue: 0,0:59:48.44,0:59:52.88,csapp,,0,0,0,,and then when the read occurs what happens is actually this cache
Dialogue: 0,0:59:53.96,0:59:59.60,csapp,,0,0,0,, miss will send out a signal on a bus a shared communication medium saying
Dialogue: 0,1:00:00.10,1:00:01.34,csapp,,0,0,0,,I want to read a 
Dialogue: 0,1:00:02.50,1:00:06.32,csapp,,0,0,0,,and instead of the main memory responding to it actually it will
Dialogue: 0,1:00:07.84,1:00:11.54,csapp,,0,0,0,,that result will be supplied by the other cache 
Dialogue: 0,1:00:12.42,1:00:16.54,csapp,,0,0,0,,and it will convert the state of this element to being a shared element
Dialogue: 0,1:00:18.74,1:00:23.20,csapp,,0,0,0,, locally but you'll see that the main memory element isn't updated yet it
Dialogue: 0,1:00:23.20,1:00:26.30,csapp,,0,0,0,,goes through the whole right-back protocol you've already seen 
Dialogue: 0,1:00:26.82,1:00:29.50,csapp,,0,0,0,,and sometimes it will update that there's different implementations 
Dialogue: 0,1:00:30.16,1:00:31.88,csapp,,0,0,0,,but this is why it's called a Snoopy cache is 
Dialogue: 0,1:00:32.04,1:00:38.76,csapp,,0,0,0,,that it basically thread two is is peeking into or getting it access to information
Dialogue: 0,1:00:38.82,1:00:40.70,csapp,,0,0,0,, that's available in thread ones cache 
Dialogue: 0,1:00:43.10,1:00:48.96,csapp,,0,0,0,,and so now thread two will correctly get a copy of a 
Dialogue: 0,1:00:49.46,1:00:50.98,csapp,,0,0,0,,that's in this shared state 
Dialogue: 0,1:00:51.62,1:00:57.30,csapp,,0,0,0,,and the same goes would be it will snoop over and thread two will one will get a readable copy
Dialogue: 0,1:00:57.88,1:00:59.92,csapp,,0,0,0,, these are now all marked as shared State 
Dialogue: 0,1:01:00.52,1:01:04.68,csapp,,0,0,0,,and so if if either of them want to write 
Dialogue: 0,1:01:04.68,1:01:08.74,csapp,,0,0,0,,they'd have to now basically get exclusive access to it 
Dialogue: 0,1:01:09.06,1:01:11.10,csapp,,0,0,0,,and that would have to then disable 
Dialogue: 0,1:01:11.82,1:01:16.04,csapp,,0,0,0,,the copy and the other in the other location 
Dialogue: 0,1:01:16.16,1:01:20.48,csapp,,0,0,0,,so you can imagine this protocol being non-trivial actually to get right and to implement 
Dialogue: 0,1:01:20.90,1:01:24.80,csapp,,0,0,0,,and it gets way more complicated than this with all the variations on it 
Dialogue: 0,1:01:26.10,1:01:31.10,csapp,,0,0,0,,so but it's become the norm in in multi-core hardware design 
Dialogue: 0,1:01:31.86,1:01:36.36,csapp,,0,0,0,,but it's actually part of the factor that limits  the core count on a processor 
Dialogue: 0,1:01:37.08,1:01:43.32,csapp,,0,0,0,,because just the hardware involved in keeping the consistency across the caches is non-trivial
Dialogue: 0,1:01:43.32,1:01:49.12,csapp,,0,0,0,, it has to work very fast we're talking at the cash rate access speeds 
Dialogue: 0,1:01:49.88,1:01:52.30,csapp,,0,0,0,,so there's not a lot of time involved in there 
Dialogue: 0,1:01:52.42,1:01:54.96,csapp,,0,0,0,,so actually implementing this stuff making it run 
Dialogue: 0,1:01:55.42,1:02:01.70,csapp,,0,0,0,,making it scale across say eight cores 10 cores 16 cores is not a not a trivial thing
Dialogue: 0,1:02:02.20,1:02:07.72,csapp,,0,0,0,, but that that goes on in the background and so you can for most systems nowadays you can assume that
Dialogue: 0,1:02:09.88,1:02:15.84,csapp,,0,0,0,,there's some memory consistency model that you can program to that's supported by the hardware of the system 
Dialogue: 0,1:02:17.30,1:02:24.22,csapp,,0,0,0,,and that this serial serializability that's referred to as sort of the the easiest to understand
Dialogue: 0,1:02:24.34,1:02:26.28,csapp,,0,0,0,, there's others at a little bit more nuanced 
Dialogue: 0,1:02:29.04,1:02:33.40,csapp,,0,0,0,,well guess that fell off the bottom here and doesn't seem right
Dialogue: 0,1:02:42.30,1:02:42.98,csapp,,0,0,0,,that's it 
Dialogue: 0,1:02:43.88,1:02:46.80,csapp,,0,0,0,,okay so just to wrap that up then
Dialogue: 0,1:02:48.56,1:02:50.82,csapp,,0,0,0,, it gives you a flavor of and 
Dialogue: 0,1:02:52.10,1:02:57.62,csapp,,0,0,0,,you can see that getting programs to run fast through multi-threading is not not easy 
Dialogue: 0,1:02:58.16,1:03:02.98,csapp,,0,0,0,,you often have to rewrite your application you have to think about the algorithm you have to worry about debugging
Dialogue: 0,1:03:03.52,1:03:06.52,csapp,,0,0,0,, it as you've already discovered at both the 
Dialogue: 0,1:03:07.28,1:03:12.28,csapp,,0,0,0,,the shell web and the proxy lab that concurrency where you can't predict the order of events 
Dialogue: 0,1:03:12.88,1:03:16.30,csapp,,0,0,0,,makes it much more difficult to debug code 
Dialogue: 0,1:03:16.80,1:03:18.62,csapp,,0,0,0,,so all these factors come in 
Dialogue: 0,1:03:20.18,1:03:24.30,csapp,,0,0,0,,and you have to have some understanding of the underlying mechanisms that are used 
Dialogue: 0,1:03:24.98,1:03:27.02,csapp,,0,0,0,,and what their performance implications are
Dialogue: 0,1:03:27.66,1:03:30.12,csapp,,0,0,0,,so in particular let me just observe here that 
Dialogue: 0,1:03:32.58,1:03:34.98,csapp,,0,0,0,,if I'm like doing synchronization 
Dialogue: 0,1:03:37.36,1:03:43.06,csapp,,0,0,0,,across threads like you saw that original one where they are fighting over this global 
Dialogue: 0,1:03:43.58,1:03:44.98,csapp,,0,0,0,,variable P 'some whatever it was called 
Dialogue: 0,1:03:46.12,1:03:48.24,csapp,,0,0,0,,you can imagine these the caches
Dialogue: 0,1:03:48.66,1:03:57.48,csapp,,0,0,0,, in this battle with each other to try and get exclusive access to this single memory of value
Dialogue: 0,1:03:58.34,1:04:04.04,csapp,,0,0,0,,and because each one is running as fast as it possibly can
Dialogue: 0,1:04:04.20,1:04:11.20,csapp,,0,0,0,, but each one requires getting exclusive copy writing to it  and releasing it 
Dialogue: 0,1:04:11.32,1:04:15.98,csapp,,0,0,0,,so that locking mechanism is flying back and forth between these caches 
Dialogue: 0,1:04:16.68,1:04:18.72,csapp,,0,0,0,,and it's really not very fast so
Dialogue: 0,1:04:19.16,1:04:22.52,csapp,,0,0,0,,that's the kind of thing is why 
Dialogue: 0,1:04:24.02,1:04:31.10,csapp,,0,0,0,,and also you know as an application programmer you're making calls semaphore call
Dialogue: 0,1:04:31.20,1:04:35.50,csapp,,0,0,0,,bounces you up into the OS kernel which is a cost involved 
Dialogue: 0,1:04:36.20,1:04:43.04,csapp,,0,0,0,,so this thing has all the bad all the things that make programs not run the way you really like them to 
Dialogue: 0,1:04:43.52,1:04:48.48,csapp,,0,0,0,,so that's one of the challenges in parallel programming is how do you actually 
Dialogue: 0,1:04:49.16,1:04:51.52,csapp,,0,0,0,,make use of the parallelism that's there 
Dialogue: 0,1:04:51.78,1:04:57.24,csapp,,0,0,0,,without getting bogged down by the cost of the various mechanisms of control 
Dialogue: 0,1:04:58.86,1:05:03.54,csapp,,0,0,0,,oh so anyways this is part of what you have to appreciate and understand as a programmer
Dialogue: 0,1:05:04.00,1:05:11.52,csapp,,0,0,0,, is how these things work at a level deep enough that you'll have some sense of what makes programs run faster or slower
Dialogue: 0,1:05:11.52,1:05:13.32,csapp,,0,0,0,,where the mistakes could want 
Dialogue: 0,1:05:14.98,1:05:18.62,csapp,,0,0,0,,so that's just a little little flavor of a much bigger topic 
Dialogue: 0,1:05:19.48,1:05:20.48,csapp,,0,0,0,,so that's it for today
